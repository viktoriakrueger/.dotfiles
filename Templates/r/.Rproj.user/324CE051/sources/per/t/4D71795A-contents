#ein notizbuch für r
########################################allgemeines####


?(Funktionsname) # hilfe für befehl
args(Funktionsname) # werden alle verfügbaren Argumente angezeigt
rm() #löscht ein Objekt 
rm(list=c("OBJEKT1","OBJEKT2")) #löscht liste
rm(list=ls()) #löscht alles

#den print in der konsole hochschrauben
options(max.print=1000000)


#pakete

# instalieren
install.packages(pkgs="NAME_DES_PAKETS", dependencies = TRUE)
devtools::install_github("rstudio/NAME_DES_PAKETS")




# arbeitsverzeichnis ändern
setwd(dir = "X:/Pfad zu/meinem Ordner")                     $Window$
  setwd(dir = "~/Documents/Pfad zu/meinem Ordners")           $macOS$
  setwd(dir = "/Users/IHR NUTZERNAME/Pfad zu/meinem Ordner")  $macOS$
  
  # dateien im arbeitsverzeichnis anzeigen
  list.files()

# tetxtdatei im arbeitsverzeichnis erstellen
file.create("NAME_DER_DATEI.txt")  

#tetxtdatei im arbeitsverzeichnis löschen
file.remove("xxxTEST.txt")


# Uhrzeit

# aktuelle Systemzeit
systime <- Sys.time()  


# Datum

# aktuelles Datum

date <- Sys.Date()  # aktuelles Datum

date  # Anzeige des Objekts mit dem heutigen Datum

date.1 <- as.Date(x = "2000-01-01")  # Start des Jahrtausends

as.numeric(date) - as.numeric(date.1)  # Anzahl Tage seit dem 1. Januar 2000

as.Date(x = 1000, origin=Sys.Date())  # Datum heute in 1000 Tagen






install.packages(pkgs="NAME_DES_PAKETS", dependencies = TRUE)




#snippets###


  



snippet tool
  summarytools::freq(${1}$ ${2})

snippet psy
  psych::describe(as.numeric(${1}$ ${2}))



snippet save
  save(DF, file = "DF.RData")

snippet load
  load("DF.RData")

snippet box
  DF %>% 
  select(V1, VX) %>%
  drop_na() %>% 
  ggplot() +
  aes(x=VX, y=(as.numeric(V1, fill=VX, group=VX))) +
  geom_boxplot() +
  scale_fill_viridis(discrete = TRUE, alpha=0.6) +
  geom_jitter(color="black", size=0.4, alpha=0.9) +
  theme(legend.position="none", plot.title = element_text(size=11)) +
  ggtitle("TITEL") +
  xlab("BEZEICHNUNG_X_ACHSE") +
  ylab("BEZEICHNUNG_y_ACHSE")




# functions #####
  # calc age
  
  calc_age <- function(x, y = Sys.Date()) {
    
    
    period <- lubridate::as.period(lubridate::interval(x, y),
                                   unit = "year")
    
    period$year
  } 
  
  
  
  
  impute_mean <- function(x){
    in_na <- is.na(x)
    x[in_na] <- mean(x[!in_na])
    as.numeric(x)
  }
#berechnungen | funktionen | formeln####

# berechnungen
DF$X <- (+ ; - ; * ; / ; ** ; ) 1234


#funktion1
mein_Mittelwert <- function(mein_Argument)
{ mean(mein_Argument, na.rm=TRUE) }

mean(x=c(1,2,NA,4))  # Ergebnis ist NA

mein_Mittelwert(mein_Argument=c(1,2,NA,4))


# funktion2
meine_Rechnung <- function(x, y){
 x + 2*y 
}

meine_Rechnung(x=2, y=3)



# formel erstellen

FORMEL <- 1234

# update formel
FORMEL <- update(FORMEL, 1234)


#loop erstellen

#break

count <- 1

repeat
{
  
  WAS_SOLL_GLOOPT_WERDEN
  
  print(count)
  count <- count+1
  if(count >= X)
  {
    break
  }
}


# survey redressment ####
# nachgewichtung # gewichtung


library(srvyr)



##############markdown####

rmarkdown::render("Untiteld.rmd", params = list())

#### tabellen ####

## vorher ein objekt erstellen, welches die tabelle beinhaltet

## so kann man im text bezug auf die tabelle nehmen und sie wird automatisch ins tabellenverzeichnis aufgenommen

#    pdf/latex      in der tabelle \ref{tab:LABEL} sieht man ...
#    html           \@ref(tab:NAME_DES_CHUNKS)




## PDF ####


```{r echo=FALSE, message=FALSE, warning=FALSE}
library(dbplyr)
knitr::kable(DF, caption="\\label{tab:LABEL}CAPTION") %>% 
  kableExtra::kable_styling(latex_options = "striped", position = "center")
```


## HTML ####


```{r echo=FALSE, message=FALSE, warning=FALSE}
library(dbplyr)
knitr::kable(DF, caption="\\label{tab:LABEL}CAPTION") %>% 
  kableExtra::kable_material(c("striped"))
```


#### abbildungen ####

### so kann man im text bezug auf die abbildung nehmen und sie wird automatisch ins abbildungsverzeichnis aufgenommen

#       in der Abbildung \ref{fig:LABEL} sieht man ...

## plots ####


```{r echo=FALSE, message=FALSE, warning=FALSE, fig.cap="\\label{fig:LABEL}CAPTION",  fig.align="center"}
plot(DF)
```

## images ####

```{r, out.width="0.3\\linewidth", include=TRUE, fig.align="center", fig.cap="\\label{fig:LABEL}CAPTION", echo=FALSE, message=FALSE, warning=FALSE}
knitr::include_graphics("/Users/alexchaichan/PFAD.png")
```

#_####

#_####
#_####
#_####
########## send email ####

# google ####
# library(mailR) 
# mailR::send.mail(from = "aleksejchaichan@gmail.com",
#                  to = c("aleksejchaichan@posteo.de"),
#                  subject = "hi",
#                  body = "hello world!",
#                  smtp = list(host.name = "smtp.gmail.com", port = 465, 
#                              user.name = "aleksejchaichan@gmail.com", passwd = "g(oogl(m4yjz31dQ)e)", 
#                              ssl=TRUE),
#                  authenticate = TRUE,
#                  send = TRUE)
# 
# 

# microsoft ####
# library(Microsoft365R) ###

# create environment for outlok account
outl <- Microsoft365R::get_personal_outlook()
#outlb <- get_business_outlook()

# list the most recent emails in your Inbox
outl$list_emails()


# get some common folders
inbox <- outl$get_inbox()
drafts <- outl$get_drafts()
sent <- outl$get_sent_items()
deleted <- outl$get_deleted_items()

# get a specific folder by name
folder <- outl$get_folder("myfolder")


# alternatively, use method chaining to achieve the same thing
em <- outl$
  create_email()$
  set_subject("Hello")$                                  #betreff
  set_body("Hello from R")$                              #
  set_recipients(to="aleksejchaichan@posteo.de")         #empfänger

em <- em$add_attachment()
em$send()

#_####
#_####
#_####
########################regular expression ####
# regular expression  werte ändern in einer variable regex # aus , wird .
mutate_at("V1", str_replace, ",", ".")
#_####

#_####
#_####
#_####
####################### step by step ######################
# read data ####
# tidy data ####
# werte neu vergeben ####

# hier wird eine neue varible erstellt die informationen werden aus der variable v1 genommen wenn "contains" übereinstimmt

DF <- DF %>%
  mutate("NEW_V" = case_when(grepl("contains", V1) ~ "NEW_VALUE",  
                         grepl("contains" "contains", V1) ~ "NEW_VALUE"))
# eda ########

#intro

DataExplorer::introduce(DF)
DataExplorer::plot_intro(DF)


# alle wichtigen zahlen

DF %>% 
  dplyr::select(()) %>% 
  psych::describe() %>% 
  round(2) %>% 
  knitr::kable(caption="CAPTION") %>% 
  kableExtra::kable_material(c("striped"))


### univariat

# histo

DF %>% 
  dplyr::select(()) %>% 
  DataExplorer::plot_histogram(nrow = 2, ncol = 2)

# qq-plot

DF %>% 
  dplyr::select(()) %>% 
  DataExplorer::plot_qq(nrow = 2, ncol = 2)


### bivariat


# boxplot

DF %>%
  dplyr::select(()) %>% 
  DataExplorer::plot_boxplot(by= "CATEGORIAL_VAR",  ncol = 2)


## corrplots

DF %>% 
  dplyr::select(()) %>% 
  DataExplorer::plot_correlation(cor_args = list( 'use' = 'complete.obs'))


# only continous variables
DF %>% 
  dplyr::select(()) %>% 
  DataExplorer::plot_correlation(type = 'c',cor_args = list( 'use' = 'complete.obs'))


#barplots
# nimmt automatisch categoriale variables
DF %>%
  DataExplorer::plot_bar(maxcat = 20, parallel = TRUE)

#_####

#_####
#_####
#_####
#_####
########################### data wrangling ####



######## data table ####
library(data.table)


#read data

dt <- fread("https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data", header=FALSE)


# set column names

names(dt) <- c("age", "workclass", "fnlwgt", "education", "education_num", "marital_status", "occupation", "relationship", "race", "sex", "capital_gain", "capital_loss","hours_per_week", "native_country", "label")



#To compute the average age of all “Tech-support” workers:

dt[occupation == 'Tech-support', mean(age)]


dt %>% 
  filter(occupation %in% c("Tech-support")) %>% 
  pull(age) %>% 
  mean(na.rm=T)



#To aggregate age by occupation for all male workers:

dt[sex == 'Male', (sum(age)), by = occupation]


#To conditionally modify values in a column

dt[occupation == 'Tech-support', age := age + 5]
# and the ifelse function does just as nicely:
dt[, age := ifelse(occupation == 'Tech-support', age + 5, age)]



# Renaming columns

setnames(dt, 'occupation', 'job')


# applying a function to several columns

dt[, lapply(.SD, function(x) x*1000), .SDcols = c("capital_gain", "capital_loss")]
######## tidy data ####


###Initial Spot Checks
##First things to check after loading new data:

# Did the last rows/columns from the original file make it in?
#  May need to use different package or manually specify range


#Are the column names in good shape?
#  Modify a col_names= argument or fix with rename()


#Are there "decorative" blank rows or columns to remove?
#  filter() or select() out those rows/columns


#How are missing values represented: NA, " " (blank), . (period), 999?
#  Use mutate() with ifelse() to fix these (perhaps en masse with looping)


#Are there character data (e.g. ZIP codes with leading zeroes) being incorrectly represented as numeric or vice versa?
#  Modify col_types= argument, or use mutate() and as.numeric()



#1. jede variable in eigener spalte (column)
#2. jede beobachtung (obs / observation) in eigener zeile (row)
#3. jede beobachtung in eigener tabelle 

#den df in der konsole anzeigen lassen
tibble::as_tibble(DF)

### columns to rows ####

# hier werden die spalten zu zeilen 

stocks <- data.frame(
  time = as.Date('2009-01-01') + 0:9,
  X = rnorm(10, 0, 1),
  Y = rnorm(10, 0, 2),
  Z = rnorm(10, 0, 4)
)


stocks <- stocks %>% gather(stock, price, -time)


stocks %>% spread(stock, price)
stocks %>% spread(time, price)


### gather####

# wide format - > long format 

#  die spalten brauchen neue namen und welche spalten werden angesprochen, die geordnet werden sollen

tidyr::pivot_longer(DF, "NAMESPALTE", "NAMESPALTE", 1:99)


#example
library(EDAWR)

tibble::as_tibble(cases)

tidyr::gather(cases, "year", "n", 2:4)




### spread ####

# der datensatz wird so arrangiert, dass die variablen (alten spalten) ersetzt werden durch die kategorien der variablen

tidyr::pivot_wider(DF, V1, V2)

#example 

library(EDAWR)

tibble::as_tibble(pollution)

tidyr::spread(pollution, size, amount)

tidyr::pivot_wider(pollution, names_from=size, values_from =  amount)





## seperate ####

# man kann variablen zerlegen z.b. das datum

tidyr::separate(DF, V1, c("v1", "v2", "v3"), sep = "-") # seperator nicht vergessen



#example

library(EDAWR)

tidyr::separate(storms, date, c("year", "month", "day"), sep = "-")







## unite ####

# man kann die variablen  zusammenführen z.b. das datum

tidyr::unite(DF, "V1", v1, v2, v3, sep ="-") # seperator nicht vergessen





#example

library(EDAWR)

storms2 <- tidyr::separate(storms, date, c("year", "month", "day"), sep = "-")

tidyr::unite(storms2, "date", year, month, day, sep ="-")









# (alt) wide-format ---> long-format (t-test)####
DF_LONG <- tidyr::gather(DF, TIME, VALUE, V1:VN) #V1 ist die variable für den ersten messzeitpunkt. VN für den letzten messzeitpunkt
DF_LONG$TIME <- as.factor(DF_LONG$TIME) #time noch als faktor codieren


######## data manipulation ####

#1. extract existing variables    select()
#2. extract existin observations  filter()
#3. derive new variables (from existing variables)    mutate()
#4. change the unit of analysis    summarise()

# dplyr::select() ####

dplyr::select(DF, -V1    V1:VN) # - nimmt alle bis auf V1 # (von : einschließlich bis)

dplyr::select(DF, contains("")) #Select columns whose name contains a character string
dplyr::select(DF, ends_with("")) #Select columns whose name ends with a character string.
dplyr::select(DF, everything()) #Select every column.
dplyr::select(DF, matches(".t.")) #Select columns whose name matches a regular expression.
dplyr::select(DF, num_range("x", 1:5)) #Select columns named x1, x2, x3, x4, x5.
dplyr::select(DF, one_of(c("V1", "V2"))) #Select columns whose names are in a group of names.
dplyr::select(DF, starts_with("V1")) #Select columns whose name starts with a character string.
dplyr::select(DF, V1:VN) #Select all columns between V1 and VN (inclusive).
dplyr::select(DF, -V1) #Select all columns except V1


# dply:filter() ####

dplyr::filter(DF, V1 < > == <= >= != 1234, V2 %in%  c("obs1", "obs2", "obs3")) # %in% gibt die fälle in der variable an in der gesucht werden soll

# < kleiner als
# > größer als
# == gleich
# <= kleiner gleich
# >= größer gleich
# != ungleich
# %in% Gruppenzugehörigkeit
# is.na ist NA
# !is.na ist keinNA
##boolesche Operatoren
# & und
# | oder
# ! nicht
# xor ekakt oder
# any any true
# all all true


# dplyr::mutate() ####

dplyr::mutate(DF, "NEWV1" = V1  + - * /  V2, "NEWV2" = "NEWV1"^2) # neue spalte berechnen und hinzufügen

dplyr::transmute(DF, "NEWV1" = V1  + - * /  V2, "NEWV2" = "NEWV1"^2) # entfernt ursprünglichen spalten

dplyr::mutate_each(DF, funs(...)) 
lead #Werteverschiebung um 1 nach vorne
lag #Werteverschiebung um 1 nach hinten
dense_rank #Rangordnung ohne Lücke.
min_rank #Rangordnung. Kleinerer Rang bei Gleichstand
percent_rank #Rangordnung skaliert auf [0, 1].
row_number #Rangordnung. Erster Wert bekommt den kleineren Rang bei Gleichstand.
ntile #Vektor in n Behälter aufteilen.
between #Sind Werte zwischen a und b?
dplyr::cume_dist #Summenverteilung
dplyr::cumall #Kumulativ alle („all“)
dplyr::cumany #Kumulativ irgendeines („any“)
dplyr::cummean #Kumulativer Mittelwert („mean“)
cumsum #Kumulative Summe
cummax #Kumulatives Maximum
cummin #Kumulatives Minimum
cumprod #Kumulatives Produkt
pmax #Elementweises Maximum
pmin #Elementweises Minimum


# dplyr::summarise() ####

#Daten in eine einzelne Zeile zusammenfassen.
dplyr::summarise(DF, "mean" = mean(V1))

DF %>% 
  dplyr::summarise("median" = median(V1), "variance" = var(V1), "n"=n())

#Zusammenfassungs-Funktion auf jede Spalte anwenden.
dplyr::summarise_each(DF, funs(mean))

first #Erster Wert eines Vektors.
last #Letzter Wert eines Vektors.
nth #n-ter Wert eines Vektors.
n #Anzahl der Werte eines Vektors.
n_distinct #Anzahl der unterschiedlichen Werte eines Vektors.
IQR #Interquartilsabstand eines Vektors.
min #Minimalwert eines Vektors.
max #Maximalwert eines Vektors.
mean #Arithmetisches Mittel eines Vektors.
median #Median eines Vektors.
var #Varianz eines Vektors.
sd #Standardabweichung eines Vektors.




# dplyr:: arrange() ####

dplyr::arrange(DF, V1) #low to high

#_####
# rownames_to_column ####
rownames_to_column("var")
#_####
#_####
#_####
#_####
#_####
#_####
#_####
#######datensatz einlesen####
# csv, xls, etc.####

  # R.Data
save(DF, file = "DF.RData")

load("DF.RData")



  # spss
  
DF <- foreign::read.spss(file = "NAME_DES_DATENSATZES_IM_WORKINGDIRECTORY .sav", use.value.labels = TRUE, to.data.frame = TRUE) 

DF <- haven::read_sav(file = "NAME_DES_DATENSATZES_IM_WORKINGDIRECTORY.sav")


  # csv
  
DF <- read.table("NAME_DES_DATENSATZES .csv", header=TRUE, stringsAsFactors=FALSE, sep=",", na.strings="NA", dec=".", strip.white=TRUE)

read_csv("NAME_DES_DATENSATZES .csv")



# objekt speichern als 
  
# csv.
  write.csv(DF, "NAME_WIE_DATEI_HEISSEN_SOLL.csv")
  
# als xlsx  
  openxlsx::write.xlsx(x=DF, file="NAME_DER_DATEI.xlsx", sheetName="SHEET_NAME")


# google drive 

  googledrive::drive_download("https://docs.google.com/", overwrite = T)

  

# Download Files
drive_download(
  "Excel Python or R Data",
  path = "/users/nickbautista/documents/data/Excel Python or R Data.csv",
  overwrite = TRUE)
 
# Read File into R
data <- read_csv("/users/nickbautista/documents/data/Excel Python or R Data.csv")
 
data$CPD <- data$Spend / data$Downloads
 
# Return File to Google Drive
write_csv(data, "/users/nickbautista/documents/data/Excel Python or R Data v2.csv")
 
drive_upload("/users/nickbautista/documents/data/Excel Python or R Data v2.csv",
             path = "Blog/Excel Python or R Data v2.csv",
             type = "spreadsheet")
  


# googlesheets4 ####
https://www.tidyverse.org/blog/2020/05/googlesheets4-0-2-0/
  
  
###json####
  library(rjson)
  library(dplyr)
  library(purrr)
  
  
  df <- as.data.frame(fromJSON(file = "~/Kundenprojekte/WEISS/04_Dauerlauf_HBM_Rohdaten/04_Dauerlauf_HBM_Rohdaten/export_json/Messung_2020_08_03_10_03_10.json"))
  
  
  messungen <- list.files("~/Kundenprojekte/WEISS/04_Dauerlauf_HBM_Rohdaten/04_Dauerlauf_HBM_Rohdaten/export_json", full.names = TRUE)
  
  mess_daten <- map_df(messungen, ~as.data.frame(fromJSON(file = .x)))
  
  which(!stringr::str_detect(messungen, ".json"))
  
  mess_df <- mess_daten %>% 
    as_tibble() %>% 
    janitor::clean_names()
  
  
  write_csv(mess_df, "~/Kundenprojekte/WEISS/aggregated_data.csv")
  
  raw_data <- read.table("~/Kundenprojekte/WEISS/04_Dauerlauf_HBM_Rohdaten/04_Dauerlauf_HBM_Rohdaten/Messung_2020_08_03_10_03_10.ASC", sep = "\t",
                         dec = ",", row.names = NULL, na.strings = "****")
  
  clean_names <- names(raw_data)[-1]
  
  raw_df <- raw_data %>% 
    select(-ncol(raw_data)) %>% 
    as_tibble() %>% 
    setNames(clean_names) %>% 
    mutate(across(1, ~as.numeric(str_replace(.x, ",", ".")))) %>% 
    janitor::clean_names()
  
  write_csv(raw_df, "~/Kundenprojekte/WEISS/raw_data.csv")
  
### xls to csv ####
  DF <- readxl::read_excel("NAME.xlsx")
  readr::write_csv(DF, file="NAME.csv")
#datensatz selber erzeugen + werte vergeben ####
  
  df1 <- data.frame("", "", "") #"" anzahl an spalten
  
  df1 <- rbind(df1, list("test3", "test2", "test1")) #1. zeile
  df1 <- rbind(df1, list("1", "2", "3")) #2. zeile
  ...
  df1 <- df1[-c(1), ] #1. leere zeile löschen
  colnames(df1) <- c("NAME1", "NAME2", "NAME3") #spalennamen vergeben 
  
  
  #oder #(((besser)))
  
  df1 <- data.frame("NAME_DER_SPALTE_1" = c('A', 'B', 'C', 'D'), "NAME_DER_SPALTE_2" = c(1, 2, 3, 4), "NAME_DER_SPALTE_3" = c(4, 5, 6, 7))
  
  
  
  #die ausprägungen der variablen werden als character gespeichert; ändern:
  
  as.factor()
  as.character()
  as.numeric()
  
  
  df1$NAME_DER_SPALTE_1 <- as.numeric(df1$NAME_DER_SPALTE_1) #als numerisch 
  df1$NAME_DER_SPALTE_1 <- as.factor(df1$NAME_DER_SPALTE_1) #als factor
  
  head(df1)
  
#oder # besser!!  
  
  
  df1 %>%
    mutate(NAME_DER_SPALTE_2 = as.character(NAME_DER_SPALTE_2)) %>% 
  as_tibble()
  
  
  
  
  
  
#####subsets erstellen####
  

  
  # sample dataset
SAMPLE <- dplyr::sample_n(DF, N) #für N die größe des samples angeben
  
   # subset
SUBSET <- dplyr::select(DF, ...)
SUBSET <- subset(DF, select=c(V1, V2, V3, ...))

    # subset mit genau der ausprägung (name des levels (z.b. "stimme eher zu" | wert)

SUBSET <- dplyr::filter(DF, V1 ...)
SUBSET <- subset(DF, DF$V1 == "LABEL" | 1234)

  # es findet ein listenweiser ausschluss von allen fälle statt, die NA's aufweisen
DF <- DF[complete.cases(DF),]
















#sichtung der daten####








############datensatzes

  # neue variable mit einer fortlaufenden zeilennummer # id number # identity # identität

DF %>% 
  mutate(id  = row_number())

DF$id <- 1:nrow(DF)
#für N die fallzahl eintragen

  # auflistung aller variablen im datensatz
names(DF)  

  # typen (strukur) der variablen im datensatz
str(DF)  

  # deskriptive Zusammenfassung aller Variablen im Datensatz
summary(DF)

  # fehlende werte anzeigen lassen  #NA counts
sapply(DF, function(x)(sum(is.na(x))))




##########variablen



# häufigkeitstabelle
summarytools::freq(DF$V1, cum = T, total = T) 


# verteilung
psych::describe(as.numeric(DF$V1))  

summarytools::descr(DF$V1) 

Hmisc::describe(DF$V1) 

# wertebreich variable
range(DF$V1, na.rm = T)



























#datensatz veraendern####

    ## löschen

  # variablen löschen
DF$V1 = NULL

  #rows löschen
DF <- DF[-c(ROW_NUMMER), ] # die nummer der zeile eintragen

  #alle fälle mit NA's einer variable löschen
DF <- subset(DF, !(is.na(V1)))

  # alle wertelables entfernen # remove all labels
sjlabelled::remove_all_labels()


  # wertebereich veringern
DF$V1 <- DF$V1 - 1234



    ############# struktur verändern

  # als faktor
DF$V1 _fac <- as.factor(DF$V1)

DF$V1 _fac <- factor(DF$V1, level=c(1, 2, 3, ...), labels=c("NAME_DES_LABELS","NAME_DES_LABELS","NAME_DES_LABELS"), ordered=TRUE)

  # als geordneter factor
DF$V1 _fac <- factor(DF$V1, ordered = TRUE, levels = c("LEVEL1", "LEVEL2", "LEVEL3", "...")) 



  # als numerisch
      ####achtung!!: wenn man einen faktor direkt numerisch macht, dann beginnt die erste kategorie auch bei 
          ###z.B alter: die erste kategorie 16 (jahre) beginnt bei 1 
                ##deshalb erst  as.character umwandeln und dann as.numeric
  DF$V1 _num <- as.character(DF$V1)
  DF$V1 _num <- as.numeric(DF$V1)

  # einen dataframe komplet numerisch machen
  DF <- data.frame(lapply(DF, function(x) as.numeric(x)))
  
      # falls der obere befehl porbleme verursacht
      DF <- data.frame(lapply(DF, function(x) as.numeric(as.character(x))))
  


    ## missing values

  # fehlende werte als NA's markieren für ganzen datensatz vergeben 
DF[DF== -9] = NA #fehlenden wert ergänzen 

  # fehlende werte aus variablen als NA markieren und variable überschreiben
DF$V1[DF$V1== -9] = NA #fehlenden wert ergänzen 




    ## neus Objekt


  # erstellen eines neuen objekts #"" steht für jede spalte #weiter oben ist eien detailierte beschreibung
DF <- data.frame("", "", ...) 



  # 2 dataframes miteinander mergen
DF <- cbind(DF1, DF2)

  # bestehendes objekt um liste erweitern
DF <- rbind(DF, list(V_1, V_2, ...))



    ## umbenennen  # rename


# collumn umbenennen
DF <- dplyr::rename(DF,  "Geschlecht" = "V161")

    # spalten namen verändern  (mehrere auf einmal)
    colnames(DF) <- c("NAME1", "NAME2", "NAME3", ...)



  # zeilen namen verändern$
rownames(DF) <- c("NAME1", "NAME2", "NAME3", ...)


  # levels / labels von faktoren umbenennen
DF$V1 <- plyr::revalue(DF$V1, c("ALT"="NEUE", "ALT"="NEU"))














#_####
#labels vergeben####

  # info über variable
  
expss::info(DF$V1)


  # für den ganzen datensatz
DF = expss::apply_labels(DF,
                                             V1 = "NAME_DES_LABELS_1",
                                             V2 = "NAME_DES_LABELS_2",
                                             V2 = c("LABEL_1" = 0, "LABEL_2" = 1),
)


  # für einzellne variablens
expss::var_lab(DF$V1) = "NAME_DES_LABELS"
expss::val_lab(DF$V1) = expss::num_lab("
            -1 WERTELABEL
             0 WERTELABEL   
             1 WERTELABEL  
")

  # labes löschen$
    ## über werte
expss::val_lab(DF$V1) = val_lab(DF$V1) %d% WERT
    ## \oder über name
expss::val_lab(DF$V1) = val_lab(DF$V1) %n_d% "LABEL"




#bsp: label####
#label

df = expss::apply_labels(df,
                         V161 = "ERFOLGSBED.,BRD: KLASSENZUGEHOERIGKEIT",
                         V161 = c("STIMME GAR NICHT ZU" =1 , "STIMME EHER NICHT ZU" =2, "STIMME EHER ZU" =3, "STIMME VOLL ZU" =4),
                         V162 = "ERFOLGSBED.,BRD: ELTERNHAUS, SCHICHT",
                         V162 = c("STIMME GAR NICHT ZU" =1 , "STIMME EHER NICHT ZU" =2, "STIMME EHER ZU" =3, "STIMME VOLL ZU" =4), V163 = "ERFOLGSBED.,BRD: BILDUNG,NICHT HERKUNFT",
                         V163 = c("STIMME GAR NICHT ZU" =1 , "STIMME EHER NICHT ZU" =2, "STIMME EHER ZU" =3, "STIMME VOLL ZU" =4), V164 = "ERFOLGSBED.,BRD: KONJUNKTUR,SOZIALLEIST.",
                         V164 = c("STIMME GAR NICHT ZU" =1 , "STIMME EHER NICHT ZU" =2, "STIMME EHER ZU" =3, "STIMME VOLL ZU" =4), V165 = "GUTES GELD FUER JEDEN,AUCH OHNE LEISTUNG",
                         V165 = c("STIMME GAR NICHT ZU" =1 , "STIMME EHER NICHT ZU" =2, "STIMME EHER ZU" =3, "STIMME VOLL ZU" =4), V166 = "EINKOMMENSDIFFERENZ ERHOEHT MOTIVATION",
                         V166 = c("STIMME GAR NICHT ZU" =1 , "STIMME EHER NICHT ZU" =2, "STIMME EHER ZU" =3, "STIMME VOLL ZU" =4), V167 = "RANGUNTERSCHIEDE SIND AKZEPTABEL",
                         V167 = c("STIMME GAR NICHT ZU" =1 , "STIMME EHER NICHT ZU" =2, "STIMME EHER ZU" =3, "STIMME VOLL ZU" =4), V168 = "SOZIALE UNTERSCHIEDE SIND GERECHT",
                         V168 = c("STIMME GAR NICHT ZU" =1 , "STIMME EHER NICHT ZU" =2, "STIMME EHER ZU" =3, "STIMME VOLL ZU" =4)
)














#_####
#recode####


# recode # %>% # pipe


DF %>% 
  mutate_at(vars(0:999),
            ~as.numeric(recode(.,
            "1"= 4, "2"=3, "3"=2, "4"=1)))





  # recode
  
DF$V1 _re <- car::recode(DF$V1, "1=_; 2=_; 3=_; 4=_; ...", levels =c("1"="NAME_LABEL", "2"="NAME_LABEL", "3"="NAME_LABEL", "4"="NAME_LABEL"))  

  # num (von-bis) #für VON:BIS zahlen eintragen #für NAME__WERT den namen der ausprägung oder wert der ausprägung eintragen 
DF$V1 _re <- car::recode(DF$V1, "VON:BIS = 'NAME__WERT'; VON:BIS = 'NAME__WERT'; VON:BIS = 'NAME__WERT'; else = 'NAME__WERT'")


  # dummy-recode mit psych::
dummies <- psych::dummy.code(DF$V1)
colnames(DF) <- c("NAME", "NAME", "NAME", ...)
DF <- cbind(DF, dummies)


  # größe | kleiner - numerische variable neu definieren
DF$V1[DF$V1 <= WERT] <- 1
DF$V1[DF$V1 > WERT & DF$V1 <= WERT] <- 2
DF$V1[DF$V1 > WERT] <- 3





#bsp: recode####



  # abitur recodieren 
  #schulabschluss recode - X025A_01 - Highest educational level attained - Respondent -> abitur: 1=ja, 0=nein
evs$abitur <- as.numeric(evs$X025A_01)
evs$abitur <- car::recode(evs$abitur, "0=0; 1=0; 2=0; 3=1; 4=0; 5=0; 6=1; 7=1; 8=1; 9=1")
evs$abitur <- as.factor(x=evs$abitur)
evs$abitur <- plyr::revalue(evs$abitur, c("1"="_ja", "0"="_nein"))
evs$abitur_num <- (evs$abitur)


  # LinksMitteRechts recode -  E033 - Self positioning in political scale -> 1;2;3;4=links, 5;6=mitte, 7;8;9;1;10=rechts
  
evs$lmr <- as.numeric(evs$E033)
evs$lmr_links <- car::recode(evs$lmr, "1:4=1; 5:6=0; 7:10=0")
evs$lmr_mitte <- car::recode(evs$lmr, "1:4=0; 5:6=1; 7:10=0")
evs$lmr_rechts <- car::recode(evs$lmr, "1:4=0; 5:6=0; 7:10=1")




  # beispiel alter jung; mittel; alt
  
  # alter recode X002 - Year of birth -> jung=1, mittel=2, alt=3

summarytools:freq(DF$V1) #V1 geburtsjahr

DF$alter <- as.numeric(DF$VARIABLE_GEBURTS_JAHR)

DF$alter[evs$alter <= 26] <- 3
DF$alter[evs$alter > 26 & evs$alter <= 40] <- 2
DF$alter[evs$alter > 40 ] <- 1

  # dummie codeieren des alters

DF$alter_jung <- car::recode(evs$alter, "1=1; 2=0; 3=0")
DF$alter_mitte <- car::recode(evs$alter, "1=0; 2=1; 3=0")
DF$alter_alt <- car::recode(evs$alter, "1=0; 2=0; 3=1")

DF$alter <- as.factor(x=evs$alter)
DF$alter <- plyr::revalue(evs$alter, c("1"="_jung", "2"="_mittel", "3"="alt"))



  # beispiel recode und labels an die richtige stelle gebracht
  evs$E069_12 <- car::recode(evs$E069_12, "1=4;2=3;3=2;4=1", levels =c("1"="None at all", "2"="Not very much", "3"="Quite a lot", "4"="A great deal"))      #E069_12


  # dicho #dichotomisieren # dichotomisiert 
evs$E069_12_dicho <- as.numeric(evs$E069_12)
evs$E069_12_dicho <- car::recode(evs$E069_12_dicho, "1=1; 2=1; 3=0; 4=0")
evs$E069_12_dicho <- as.factor(x=evs$E069_12_dicho)
evs$E069_12_dicho <- plyr::revalue(evs$E069_12_dicho, c("1"="vertrauen", "0"="kein vertrauen"))

#bsp: recode gruppen für subsets erstellen####

#subsets für das geschlecht
male <- subset(DF, DF$VX == "MANN")
#VX variable für das geschleht
female <- subset(DF, DF$VX == "FRAU")


#subsets für das alter
summarytools::freq(DF$V1)
DF$V59gruppe <- as.numeric(DF$V59)
DF$V59gruppe <- car::recode(DF$V59gruppe, "1=1; 2=2; 3=3; 4=4; 5=5; 6=5", levels =c("1"="18-29 JAHRE", "2"="30-44 JAHRE", "3"="45-59 JAHRE", "4"="60-74 JAHRE", "5" = "ab 75"))  
summarytools::freq(DF$V59gruppe)
sehr_jung <- subset(DF, DF$V59gruppe == 1)
jung <- subset(DF, DF$V59gruppe == 2)
mittel <- subset(DF, DF$V59gruppe == 3)
älter <- subset(DF, DF$V59gruppe == 4)
alt <- subset(DF, DF$V59gruppe == 5)

#subset für das einkommen nach: sehr wenig, wenig, mittel, viel
DF$V556gruppe <- as.numeric(DF$V556)
DF$V556gruppe <- car::recode(DF$V556gruppe, "1=1; 2=1; 3=1; 4=1; 5=1; 6=1; 7=1 ;8=1 ; 9=2 ; 10=2 ;11=2 ; 12=2 ; 13 =2 ; 14=2 ; 15= 3; 16=3 ; 17=3 ; 18=3 ; 19=3 ; 20=4 ; 21=4 ; 22=4")  
summarytools::freq(DF$V556gruppe)
sehr_wenig <- subset(DF, DF$V556gruppe == 1)
wenig <- subset(DF, DF$V556gruppe == 2)
mittel <- subset(DF, DF$V556gruppe == 3)
viel <- subset(DF, DF$V556gruppe == 4)

#subset für gehaltsklasse
DF$Gehaltsklasse <- as.character(DF$V473)
DF$Gehaltsklasse <- as.numeric(DF$Gehaltsklasse)
DF$Gehaltsklasse <- car::recode(DF$Gehaltsklasse, "0:899 = '1'; 900:1299 = '2'; 1300:1499 = '3'; 1500:1999 = '3' ; 2000:2599 = '4' ; 2600:3599 = '5' ; 3600:4999 = '6' ; 5000:18000 = '7'")
DF <- subset(DF, !(is.na(Gehaltsklasse)))


#für den schulabschluss
summarytools::freq(DF$V60)
DF$V60neu <- as.numeric(DF$V60)
summarytools::freq(DF$V60neu)
ohne_abschl <- subset(DF, DF$V60neu == 1)
volks_haupt <- subset(DF, DF$V60neu == 2)
mittlere_reife <- subset(DF, DF$V60neu == 3)
fachhoch <- subset(DF, DF$V60neu == 4)
hochschul <- subset(DF, DF$V60neu == 5)
anderer <- subset(DF, DF$V60neu == 6)
noch_schule <- subset(DF, DF$V60neu == 7)

# factors ####

all %>%
  mutate(educ_4kat = factor(
    levels = c(
      "Hauptschulabschluss",
      "Mittlere Reife",
      "Fachhochschulreife",
      "Abitur"
    ),
    ifelse(
      educ == 2,
      "Hauptschulabschluss",
      ifelse(
        educ == 3,
        "Mittlere Reife",
        ifelse(educ == 4, "Fachhochschulreife",
               ifelse(educ == 5, "Abitur", NA))
      )
    )
  )) %>%
  pull(educ_4kat) %>%
  table()
pull(age_cat) %>%
  str()

#_####
#_####
#_####
#_####
#_####
#_####
#_####
#################fehlende werte####


## missing values

# fehlende werte als NA's markieren für ganzen datensatz vergeben 
DF[DF== -9] = NA #fehlenden wert ergänzen 

# fehlende werte aus variablen als NA markieren und variable überschreiben
DF$V1[DF$V1== -9] = NA #fehlenden wert ergänzen 

#der code erstellt einen neuen dataframe mit allen variablen und missings
IMPUT <- sapply(DF, function(x)(sum(is.na(x))))
IMPUT <- data.frame(IMPUT)





###### fehlende werte löschen
#der befehl lässt im df nurnoch fälle mit gültigen werten
DF <- DF[complete.cases(DF),]

##### für jeden analyse vollständige fälle verwenden

#der befehl löscht alle fälle, die in der variable fehlende werte haben
DF <- subset(DF, !(is.na(V1)))

######imputation ####
    #https://www.analyticsvidhya.com/blog/2016/03/tutorial-powerful-packages-imputing-missing-values/
  

#ein subset erstellen mit allen benötigten variablen #numeric machen
IMPUT <- subset(DF, select=c(V1:Vk))
IMPUT <- data.frame(lapply(IMPUT, function(x) as.numeric(x)))


#### MICE ####
#(Multivariate Imputation via Chained Equations)
#annahme, dass missings (MAR) missing at random sind

#linear regression is used to predict continuous missing values. 
#Logistic regression is used for categorical missing values

#Precisely, the methods used by this package are:

#PMM (Predictive Mean Matching)  – For numeric variables
#logreg(Logistic Regression) – For Binary Variables( with 2 levels)
#polyreg(Bayesian polytomous regression) – For Factor Variables (>= 2 levels)
#Proportional odds model (ordered, >= 2 levels)



#eine datenmatrix und einen plott ausgeben lassen in dem infos über fehlende werte und ihre häufigkeiten enthalten sind

aggrPlot <- VIM::aggr(IMPUT, col=c('navyblue','yellow'),
          numbers=TRUE, sortVars=TRUE,
          labels=names(DF), cex.axis=.7,
          gap=3, ylab=c("Missing data","Pattern"))

summary(aggrPlot)


#der befehl imputiert die werte 
IMPUT <- mice::mice(IMPUT, m=5, maxit = 50, method =c("", "", "", ""), seed = 500) #method = "", "", "" #für jede variable kann man eine andere methode verwenden
#m  – Refers to 5 imputed data sets
#maxit – Refers to no. of iterations taken to impute missing values
#method – Refers to method used in imputation. we used predictive mean matching.
summary(IMPUT)

#zeigt für jeden fall die werte an, die für die variable imputiert wurden
IMPUT$imp$V1

#es wird der 2 datensatz ausgewählt und in einem df abgespeichert
COMPLETE <- mice::complete(IMPUT,2)


fit <- with(data = IMPUT, exp = lm(as.numeric(AV) ~ as.numeric(UV1) + as.numeric(UV2) + as.numeric(UV3) + as.numeric(UV4)))
summary(fit)

mice::pool(fit)



#### Amelia ####


#It makes the following assumptions:
  
  #All variables in a data set have Multivariate Normal Distribution (MVN). It uses means and covariances     to summarize data.
  #Missing data is random in nature (Missing at Random)

#The only thing that you need to be careful about is classifying variables. It has 3 parameters:
  
# 'idvars' – keep all ID variables and other variables which you don’t want to impute
# 'noms' – keep nominal variables here
# 'ords' – keep ordinal variables here

#specify columns and run amelia
AMELIA <- Amelia::amelia(IMPUT, m=5, parallel = "multicore", ords = 1:99) #ords wenn es ordinale vriablen sind die zahl gibt die stelle an #noms wenn es nominale variablen gibt #diese variablen werden dann in die analyse nicht einbezogen

#access imputed outputs
AMELIA$imputations[[1]]
AMELIA$imputations[[2]]
AMELIA$imputations[[3]]
AMELIA$imputations[[4]]
AMELIA$imputations[[5]]

#check a particular column in a data set, use the following commands
AMELIA$imputations[[5]]$V1


#### missForest ####

#non parametric imputation method

#impute missing values, using all parameters as default values
FOREST <- missForest::missForest(IMPUT)


#check imputed values
FOREST$ximp

#check imputation error
FOREST$OOBerror
#NRMSE is normalized mean squared error. It is used to represent error derived from imputing continuous values. 
#PFC (proportion of falsely classified) is used to represent error derived from imputing categorical values.








#### Hmisc ####


# impute() function simply imputes missing value using user defined statistical method (mean, max, mean). It’s default is median. 
# aregImpute() allows mean imputation using additive regression, bootstrapping, and predictive mean matching.


#Here are some important highlights of this package:
  
# It assumes linearity in the variables being predicted.
# Fisher’s optimum scoring method is used for predicting categorical variables.
      # https://en.wikipedia.org/wiki/Scoring_algorithm

# impute with (mean, min, max, median 'random') value
IMPUT$imputed_ <- with(IMPUT, Hmisc::impute(E114, mean)) #mean, min, max, median, 'random'

#using argImpute # argImpute() automatically identifies the variable type and treats them accordingly.
impute_arg <- Hmisc::aregImpute(~ V1 + V2 + V3 + V4 + V5, data = IMPUT, n.impute = 5)

#check imputed V1
impute_arg$imputed$V1

#### mi (Multiple imputation with diagnostics) ####

#predictive mean matching (pmm)

#Below are some unique characteristics of this package:
  
#It allows graphical diagnostics of imputation models and convergence of imputation process.
#It uses bayesian version of regression models to handle issue of separation.
#Imputation model specification is similar to regression output in R
#It automatically detects irregularities in data such as high collinearity among variables.
#Also, it adds noise to imputation process to solve the problem of additive constraints.





#_####
#_####
#_####
#_####
#_####
#_####
######################################Strukturierende verfahren####
###Skalen und Itemanalyse####

#reliabilität von skalen bestimmen####

          #### vollständige itemanalyse
  
  # ein subset erstellen mit allen variablen die analysiert werden sollen
ITEM <- subset(DF, select=c(V_1, V_2, V_3, V_4, ...))

#eine schnelle variante
sjPlot::sjt.itemanalysis(ITEM) 

#die händische berechnung

  # cronbachs alpha berechnen - man bekommt eine warnung, dass einige items inverse codiert sind
psych::alpha(ITEM, check.keys = T) 

############# inverse codierte items recodieren car::recode und in ITEM speichern
ITEM$V1 _re <- car::recode(DF$V1, "1=_; 2=_; 3=_; 4=_; ...")#, levels =c("1"="NAME_LABEL", "2"="NAME_LABEL", "3"="NAME_LABEL", "4"="NAME_LABEL"))  

  # inverse codierte items in einem neuen objekt selektieren 
ITEM _re <- (c("V_5_re", "V_6_re", "V_7_re", "V_8_re", ...))

  # neues alpha berechnen 
itemanalyse <- psych::alpha(ITEM, keys = "ITEM_re")


  # alle werte werden berechnet und als tabelle ausgegeben

cronbachs_alpha <- itemanalyse$total$raw_alpha
alpha_minus_item <- itemanalyse$alpha.drop$raw_alpha
trennschaerfe <- itemanalyse$item.stats$r.drop
schwierigkeit <- itemanalyse$item.stats$mean/7
itemanalyse_tabelle <- data.frame(cronbachs_alpha, alpha_minus_item, trennschaerfe, schwierigkeit)
itemanalyse_tabelle <- round(itemanalyse_tabelle, digits = 2)
rownames(itemanalyse_tabelle) <- colnames(alpha)



    ## skalen aus items bilden (recodierte items verwenden!!! wenn es welche gibt)

  # mittelwerte  
DF$X _mean <- rowMeans(subset(ITEM, select = c(V_1_re, V_2, V_3, V_4 ...)), na.rm = T)
  # summenscore 
DF$X _sum <- rowSums(subset(ITEM, select = c(V_1_re, V_2, V_3, V_4_re ...)), na.rm = T)

#_####
###faktorenanalyse (explorative)####


####notes rotation von varimax erst zu letzt ändern weil man sonst die unahänigkeit der variablen untereinander aufgibt
    


      ### vorbereitung

#datensatz erstellen und variable auswählen
FAKTOR <- subset(DF, select=c(V_1, V_2, V_3, ...))


  # datenauswahl nur mit vollständigen fällen (c=complete)
FAKTOR <- FAKTOR[complete.cases(FAKTOR),]

       # evtl. skalieren
      FAKTOR <- scale(FAKTOR)


##Voraussetzung
#die Fallzahl darf nicht zu gering sein
#Items müssen untereinander korrelieren -->Bartlett Test
#KMO Werte prüfen


  # deskriptive analyse - datencheck (keine ausreiser)
psych::describe(FAKTOR)

  # bartlett test
#Er prüft im Rahmen der Faktorenanalyse, ob die Items untereinander korrelieren. Kann die Nullhypothese nicht abgelehnt werden (p-Wert <=0) , sollte die Faktorenanalyse nicht durchgeführt werden.
psych::cortest.bartlett(FAKTOR)

  # kmo und msa - alle werte müssen über 0.5 sein (wenn nicht -> ausschluss, außer es ist sehr wichtig)
#KMO gibt an, in welchem Umfang die einzelnen Ausgangsvariablen in der Faktorenanalyse zusammengehören. Man bestimmt den gemeinsamen Varianzanteil, den alle Variablen miteinander teilen. Den setzt man mit dem gemeinsamen Varianzanteil aller Variablen miteinander, zuzüglich der Summe der quadrierten Partialkorrelationskoeffizienten, in Beziehung.

psych::KMO(FAKTOR)

      ## anzahl faktoren bestimmen
  # mit varimax-rotation und ml-faktorenanalyse
psych::nfactors(FAKTOR, rotate = "varimax", fm="mle")
    ## velicer map -> gibt an wie viele faktoren da sein könnten
  

                                    # oder


  # anzahl faktoren mit parallel analyse bestimmen #EMPFOHLEN
ev <- eigen(cor(FAKTOR))
ap <- nFactors::parallel(subject=nrow(FAKTOR), var=ncol(FAKTOR), rep=100, cent=0.05)
nS <- nFactors::nScree(x=ev$values, aparallel = ap$eigen$qevpea)

nFactors::plotnScree(nS)





##maximum-likelihood-faktorenanalyse (ML)####
  #anzahl faktoren?
  #welche rotation?
  
ML_FAKTOR <- factanal(FAKTOR, ANZAHL, rotation = "varimax") #ANZAHL an faktoren eingeben
print(ML_FAKTOR, digits = 2, cutoff= 0.3)


  # kommunalitäten
    # wie viel varianz der einzelnen items wird durch die faktoren erklärt
1-ML_FAKTOR$uniquenesses

  # plot
psy::scree.plot(ML_FAKTOR$correlation)

##hauptachsenanalyse (PAF)####

# https://clauswilke.com/blog/2020/09/07/pca-tidyverse-style/



  #https://m-clark.github.io/posts/2020-04-10-psych-explained/
#wir haben eine überlegung was dahinter für ein latentes konstrukt sein könnte

#psych macht hier etwas falsch: gibt literatur dazu. viki kann das erklären
  
Anzahl_Faktoren = X #hier die Anzahl der Faktoren eintragen

PAF_FAKTOR <- psych::fa(FAKTOR, X, rotation = "varimax") #minimum residual = MR
print(PAF_FAKTOR)


  # faktorladungen ausgeben
#Faktorladung besagt, wie eng ein Item mit dem Faktor zusammenhängt. Quadrierte Ladung ist der gemeinsame Varianzanteil zwischen Faktor und Item.
PAF_FAKTOR$loadings


  # kommunalitäten
PAF_FAKTOR$communality

  # plot
plot(PAF_FAKTOR$values, type = "b")
abline(h=1)


##hauptkomponentenanalyse (PCA)####
#komplette Varianz erklären

PCA_FAKTOR <- psych::principal(FAKTOR, 2, rotate = "varimax")
PCA_FAKTOR

  # faktorladungen ausgeben
#Faktorladung besagt, wie eng ein Item mit dem Faktor zusammenhängt. Quadrierte Ladung ist der gemeinsame Varianzanteil zwischen Faktor und Item.
PCA_FAKTOR$loadings


  # kommunalitäten
PCA_FAKTOR$communality

  # plot
plot(PCA_FAKTOR$values, type = "b")
abline(h=1)




##konfirmatorische faktorenanalyse (CFA)####


### Lade die notwendigen Packages
library(corpcor)
library(GPArotation)
library(psych)
library(lavaan)


#### 1.Inspektion der Kovarianzmatrix als Rechengrundlage der CFA ###

# 1a. Auswahl der relevanten Variablen f?r die Faktorenanalyse
auswahl <- subset(DF, select = esws01i:lsws28i) #die benötigten variblen auswählen

#fehlende werte ausschließen
auswahl <- auswahl[complete.cases(auswahl),]



# 1b. Inspektion der Kovarianzmatrix als Rechengrundlage der CFA
matrix <- cov(auswahl, use ="pairwise.complete.obs")
round(matrix, 2)



#### 2. Einfaktorielles Konfirmatorisches Faktorenmodell ###

cfa_1fa <- cfa('Gen =~ esws01i + esws02i + esws03 + lsws08 + lsws09i + esws16i + esws17i + esws18i + esws19 + lsws26 + lsws27 + lsws28i',data = auswahl)

summary(cfa_1fa, fit = TRUE, standardized = TRUE)

modindices(cfa_1fa)



#### 3. Zweifaktorielles Konfirmatorisches Faktorenmodell mit korrelierten Faktoren ###

cfa_2fa <- cfa('ESWS =~ esws01i + esws02i + esws03 + esws16i + esws17i + esws18i + esws19
                LSWS =~ lsws08 + lsws09i  + lsws26 + lsws27 + lsws28i',
               data = auswahl)

summary(cfa_2fa, fit = TRUE, standardized = TRUE)

modindices(cfa_2fa)



#### 4. Zweifaktorielles Konfirmatorisches Faktorenmodell mit unkorrelierten Faktoren ###

cfa_2fa_unkorr <- cfa('ESWS =~ esws01i + esws02i + esws03 + esws16i + esws17i + esws18i + esws19
                       LSWS =~ lsws08 + lsws09i  + lsws26 + lsws27 + lsws28i
                       ESWS ~~ 0*LSWS',
                      data = auswahl)

summary(cfa_2fa_unkorr, fit = TRUE, standardized = TRUE)

modindices(cfa_2fa_unkorr)
##exploratorische faktorenanalyse (EFA)####
### Lade die notwendigen Packages
library(corpcor)
library(GPArotation)
library(psych)
library(lavaan)


#### Exploratorische Faktorenanalyse ###

#### 1.Inspektion der Korrelationsmatrix als Rechengrundlage der Faktorenanalyse ###

### 1a. Auswahl der relevanten Variablen f?r die Faktorenanalyse
auswahl <- subset(dat, select = esws01i:lsws28i)


#fehlende werte ausschließen
auswahl <- auswahl[complete.cases(auswahl),]

# evtl. standardisieren
auswahl<- scale(auswahl)



### 1b. Inspektion der Korrelationsmatrix als Rechengrundlage der EFA

matrix <- cor(auswahl, use ="pairwise.complete.obs")
round(matrix, 2)


#### 2. Sind die Voraussetzungen der Faktorenanalyse erf?llt? ###

# Bartlett-Test
cortest.bartlett(auswahl)

# KMO-Test
KMO(auswahl)

#### 3. ?berlegung, welche Extraktionsmethode verwendet werden soll (PCA oder PFA?) ###


################# 3.1 Hauptkomponentenanalyse ###

#### 3.1 a) Faktorenextraktion ###

# Wir starten mit einem Faktorenmodell, dass so viele Hauptkomponenten wie Items umfasst 
# Rotation spielt an dieser Stelle noch KEINE Rolle

pca_full <- psych::principal(auswahl, nfactors=12, rotate="none")
pca_full # inspiziere Kommunalit?t und Uniqueness, inspiziere Eigenwerteverlauf


# Inspiziere Scree-Plot
plot(pca_full$values, type="b")


# Parallelanalyse
fa.parallel(auswahl, fa="pc") # "pc" steht f?r Hauptkomponentenanalyse


# Nachdem man sich f?r eine bestimmte Anzahl an zu extrahierenden Faktoren entschieden hat, wird das Modell 
# mit dieser Anzahl an Faktoren berechnet
pca_red <- principal(auswahl, nfactors=2, rotate="none")
pca_red # inspiziere Kommunalit?t und Uniqueness, Gesamtvarianzaufkl?rung ("Cumulative Var")


#### 3.1 b) Rotation ###

# Wir beginnen mit einer Orthogonalen Rotation (Varimax)
pca_red_varimax <- principal(auswahl, nfactors=2, rotate="varimax")
print(pca_red_varimax, cut = .1, sort = TRUE)# Output nach Gr??e sortieren, ansonsten wird es un?bersichtlich


# Oblique Rotation (Promax)
pca_red_promax <- principal(auswahl, nfactors=2, rotate="promax")
print(pca_red_promax, cut = .1, sort = TRUE)# Output nach Gr??e sortieren, ansonsten wird es un?bersichtlich

#standardm??ig wird nur die Mustermatrix bei Obliquer Rotation ausgegeben
# F?r besonders gewiefte: so berechnet man die Strukturmatrix
structure <- pca_red_promax$loadings %*% pca_red_promax$Phi
print(structure, digits=2, cut=.1)


################# 3.2 Hauptachsenanalyse ###

#### 3.2 a) Faktorenextraktion ###

# Wir starten mit dem Eigenwertverlauf aller m?glichen Faktoren 
# Rotation spielt an dieser Stelle noch KEINE Rolle
pfa_full <- fa.parallel(auswahl, fa="fa") # "fa" entspricht einer Hauptachsenanalyse 
# pr?fe hier Screeplot
# pr?fe hier Ergebnis der Parallelanalyse

pfa_full# inspiziere Eigenwerteverlauf



# Nachdem man sich f?r eine bestimmte Anzahl an zu extrahierenden Faktoren entschieden hat, 
# wird das Modell mit dieser Anzahl an Faktoren berechnet
pfa_red <- fa(auswahl, 2, fm="pa", rotate="none")
print(pfa_red, digits=2, cut=.1, sort=TRUE)# inspiziere Kommunalit?t und Uniqueness, Gesamtvarianzaufkl?rung ("Cumulative Var")


#### 3.2 b) Rotation ###

#  Orthogonalen Rotation (Varimax)
pfa_red_varimax <- fa(auswahl, 2, fm="pa", rotate="varimax")
print(pfa_red_varimax, digits=2, cut=.1, sort=TRUE)


# Oblique Rotation (Promax)
pfa_red_promax <- fa(auswahl, 2, fm="pa", rotate="promax")
print(pfa_red_promax, digits=2, cut=.1, sort=TRUE)





#_####
#_####

###clusternanalyse####

    ## vorbereitung
  # keine fehlenden werte!
  # gleiche skalen -> standardisieren 
  
  # ein subset ziehen mit allen benötigten variablen
CLUSTER <- subset(DF, select=c(V1, V2, V3, ...))
CLUSTER <- data.matrix(CLUSTER)
CLUSTER <- as.data.frame(CLUSTER)

  # fehlende werte ausschließen
CLUSTER <- CLUSTER[complete.cases(CLUSTER),]

  #skalen z-standardisieren#
CLUSTER <- cbind(scale(CLUSTER))


##hierachische clusteranalyse####

DF <- df %>% 
  slice(1:N) %>% 
  select(featuers) %>% #müssen alle numerisch sein
  data.matrix() %>%  #so und dann
  as.data.frame() %>% #. werden die alle numerisch gemacht
  #filter(complete.cases(.)) %>% 
  scale() %>% 
  as.data.frame()

hclust <- cluster %>% 
  dist(method = "maximum") %>% 
  hclust(method = "ward.D")


plot(hclust, hang=-1, cex = 0.7)
rect.hclust(hclust, k = 5, border = "red")


test <- agg_kunden %>% 
  slice(1:1000) %>% 
  mutate(cluster = cutree(hclust, k = 5))



test %>% 
  group_by(cluster) %>%
  summarise(mean_age =mean(age, na.rm=T))
  
  # distanzen bilden
H_CLUSTER <- dist(CLUSTER, method = "_____") #"euclidean" #"maximum" #"manhattan" #"canberra" #"binary" #"minkowski"


  # clustern
H_CLUSTER <- hclust(H_CLUSTER, method = "_____") #"ward.D2" #"ward.D" #"single" #"complete"  #"average" #"mcquitty" #"median" #"centroid"

  # dendogram
plot(H_CLUSTER, hang=-1, cex = 0.7)
# cluster in dendogram markieren
rect.hclust(H_CLUSTER, k = ANZAHl, border = "red") #ANZAHl der cluster eintragen

  # X - Cluster Lösung
CLUSTER$clus_1 <- cutree(H_CLUSTER, k = 2) #ANZAHl der cluster eintragen



  # cluster mit eigenschaften beschreiben
psych::describeBy(CLUSTER, group = CLUSTER$clus_1)
table(CLUSTER$clus_1)




#k-means clusteranalyse####


  # anzahl cluster festlegen
    # fehlerquadratsummer für verschiedene clusterlösung berechnen 
#für N anzahl der fälle eintragen 
K_CLUSTER <- (nrow(CLUSTER[, 1:N])-1)*sum(apply(CLUSTER[, 1:N],2, var))
for (i in 2:15){
  K_CLUSTER[i] <- sum(kmeans(CLUSTER[, 1:N], centers=i)$withinss)
}
                        
  # fehlerquadratsumme gegen anzahl cluster plotten
plot(1:15, K_CLUSTER, type="b", xlab="Anzahl Cluster", ylab="Fehlerquadratsumme")
                        
  # cluster-lösung mit 25 zufälligen initialen clusterzentren
K_CLUSTER <- kmeans(CLUSTER[, 1:N], 3, nstart=25) #für N die anzahl an fällen eintragen
                        
  # clusterzentren
aggregate(CLUSTER, by=list(K_CLUSTER$cluster), FUN = mean)
                        
  # clusterzentren zugehörigkeit speichern
CLUSTER$clus_2 <- K_CLUSTER$cluster
                        
                        
  # cluster mit eigenschaften beschreiben
psych::describeBy(CLUSTER[, 1:14], group = CLUSTER$clus_2)
table(CLUSTER$clus_2)
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        

#_####
#_####
#_####
#_####
#_####
#_####
#_####
#_####
##########################################visualisierung####
#########plots####
#_####
#######box_plot####


#datensatz aufbereiten für ggplot2 
boxplot_DF <- subset(DF, select=c(VY, VX )) #benötigten variablen aussuchen #VX nur wenn es mehrere Gruppen gibt 
boxplot_DF <- subset(boxplot_DF, !(is.na(VY))) #NA's löschen
boxplot_DF <- subset(boxplot_DF, !(is.na(VX))) #NA's löschen


###box_plot-normal####


DF %>% 
  select(V1, VX) %>%
  drop_na() %>% 
  ggplot() +
  aes(x=VX, y=(as.numeric(V1, fill=VX, group=VX))) +
  stat_boxplot(geom="errorbar", width=.5) +
  geom_boxplot(fill = "white", outlier.color = "black", outlier.size = 2, outlier.shape = 1, notch = F) +
  #scale_x_discrete() + # comment in / out (bei mehreren Gruppen)
  theme_classic() +
  #coord_flip() +
  ggtitle("TITEL") +
  xlab("BEZEICHNUNG_X_ACHSE") +
  ylab("BEZEICHNUNG_y_ACHSE") +
  theme(plot.title = element_text(hjust = 0.5))







#benötigte pakete um den plot darzustellen
library(ggplot2)

ggplot(data=boxplot_DF, aes(x=VX, y=(as.numeric(VY, fill=VX, group=VX)))) + #x, fill & group löschen wenn es nur eine variable gibt 
  stat_boxplot(geom="errorbar", width=.5) +
  geom_boxplot(fill = "white", outlier.color = "black", outlier.size = 2, outlier.shape = 1, notch = T) +
  #scale_x_discrete() + # comment in / out (bei mehreren Gruppen)
  theme_classic() +
  #coord_flip() +
  ggtitle("TITEL") +
  xlab("BEZEICHNUNG_X_ACHSE") +
  ylab("BEZEICHNUNG_y_ACHSE") +
  theme(plot.title = element_text(hjust = 0.5))

###box_plot+jitter####



DF %>% 
  select(V1, VX) %>%
  drop_na() %>% 
  ggplot() +
  aes(x=VX, y=(as.numeric(V1, fill=VX, group=VX))) +
  geom_boxplot() +
  scale_fill_viridis(discrete = TRUE, alpha=0.6) +
  geom_jitter(color="black", size=0.4, alpha=0.9) +
  theme(
    legend.position="none",
    plot.title = element_text(size=11)
  ) +
  ggtitle("TITEL") +
  xlab("BEZEICHNUNG_X_ACHSE") +
  ylab("BEZEICHNUNG_y_ACHSE")





#benötigte pakete um den plot dazustellen
library(ggplot2)
library(hrbrthemes)
library(viridis)


# plot
ggplot(data=boxplot_DF, aes(x="", y=(as.numeric(VY)))) +
  geom_boxplot() +
  scale_fill_viridis(discrete = TRUE, alpha=0.6) +
  geom_jitter(color="black", size=0.4, alpha=0.9) +
  theme_ipsum() +
  theme(
    legend.position="none",
    plot.title = element_text(size=11)
  ) +
  ggtitle("TITEL") +
  xlab("BEZEICHNUNG_X_ACHSE") +
  ylab("BEZEICHNUNG_y_ACHSE")

###box_plot+jitter für gruppen####

#benötigte pakete um den plot dazustellen
library(ggplot2)
library(hrbrthemes)
library(viridis)

#sieht nur gut aus wenn die variable wirklch metrisch ist und es nicht zu viele fälle sind 

#so muss der datensatz bzw. die variablen aussehen
data.frame(
  VX=c( rep("A",10), rep("B",10), rep("B",10), rep("C",20), rep('D', 10)  ),
  VY=c( rnorm(10, 10, 5), rnorm(10, 13, 1), rnorm(10, 18, 1), rnorm(20, 25, 4), rnorm(10, 12, 1) )
)



#der plot stellt die Y variable dar und die verteilung der gruppenvariable innerhalb des boxplots dar
  ggplot(data=boxplot_DF, aes(x=VX, y=(as.numeric(VY, fill=VX)))) +
  geom_boxplot() +
  scale_fill_viridis(discrete = TRUE, alpha=0.6) +
  geom_jitter(color="black", size=0.4, alpha=0.9) +
  theme_ipsum() +
  theme(
    legend.position="none",
    plot.title = element_text(size=11)
  ) +
  ggtitle("TITEL") +
  xlab("BEZEICHNUNG_X_ACHSE") +
  ylab("BEZEICHNUNG_y_ACHSE")
  
  
  
  
  
#pie_plot####

#i.d.r nicht mehr als 7 kategorien








pie(table(DF$V1), labels = beschriftung1, main = "TITEL", col = c("red", "blue", "pink", "yellow", "green", "Turquoise", "purple"))

prozent1 <- round((table(DF$V1)/sum(table(DF$V1)))*100,2)
beschriftung1 <- c("BESCHRIFTUNG1", "BESCHRIFTUNG2", "BESCHRIFTUNG3", "BESCHRIFTUNG4", "BESCHRIFTUNG5", "BESCHRIFTUNG6", "BESCHRIFTUNG7")
beschriftung1 <- paste(beschriftung1, prozent1, "%", sep = " ")










#pie_plot-ggplot####


DF %>% 
  select(V1) %>%
  table() %>% 
  as.data.frame() %>%
  dplyr::mutate(id=LETTERS[dplyr::row_number()]) %>% 
  rename("TEXT" = 1) %>% 
  ggplot() +
  aes(x="", y=Freq, fill = paste0(id,' : ', TEXT, '(', round(Freq/sum(Freq)*100, 2),'%)')) +
  geom_bar(stat="identity", width=1) +
  geom_text(aes(x=1.4, label = id), position = position_stack(vjust = 0.5)) +
  theme_void() +
  theme_classic() +
  theme(legend.position = "top") +
  coord_polar("y", start=0) +
  #scale_fill_manual(values=palette) +
  theme(axis.line = element_blank()) +
  theme(axis.text = element_blank()) +
  theme(axis.ticks = element_blank()) +
  labs(x = NULL, y = NULL, fill = NULL) +
  labs(title = "TITEL") + 
  labs(subtitle = "SUBTITEL") + 
  labs(caption = 'Quelle: ')

#####aufbereitung der daten

summarytools::freq(DF$VY)

#erstellen einer häufigkeitstabelle für VY
pie_DF <- table(DF$VY)
pie_DF <- as.data.frame(pie_DF)

#der datensatz bekommt eine neue variable mit fortlaufender nummerierung
pie_DF <- pie_DF%>%
  dplyr::mutate(id=LETTERS[dplyr::row_number()])

#random colors
palette <- randomcoloR::distinctColorPalette(10)

ggplot(pie_DF, aes(x="", y=Freq, fill = paste0(id,' : ', Var1, '(', round(Freq/sum(Freq)*100, 2),'%)'))) +
  geom_bar(stat="identity", width=1) +
  geom_text(aes(x=1.4, label = id), position = position_stack(vjust = 0.5)) +
  theme_void() +
  theme_classic() +
  theme(legend.position = "top") +
  coord_polar("y", start=0) +
  scale_fill_manual(values=palette) +
  theme(axis.line = element_blank()) +
  theme(axis.text = element_blank()) +
  theme(axis.ticks = element_blank()) +
  labs(x = NULL, y = NULL, fill = NULL) +
  labs(title = "TITEL") + 
  labs(subtitle = "SUBTITEL") + 
  labs(caption = 'Quelle: ')


#bar_plot####






########2 varibalen

barplot(table(DF$V1, DF$V2),
        col = c("darkblue", "darkred"), xlab = "TITEL_X", ylab = "TITEL_Y", main ="TITEL",
        axis.lty = 1, cex.axis = 1.5, cex.names = 1.5, cex.lab = 1.5, cex.main = 2,
)
legend("topright", c("BESCHRIFTUNG_KÄSTCHEN1", "BESCHRIFTUNG_KÄSTCHEN2"), pch = 15, col = c("darkblue", "darkred"),
       cex = 1.75, bty = "n", y.intersp = 0.3, inset = c(-0.1, -0.05))












#100%-barplot(ggplot)####


DF %>% 
  select(V1) %>%
  table() %>% 
  prop.table(NULL) %>%
  as.data.frame() %>%
  rename("TEXT" = 1) %>% 
  ggplot() +
  aes(x="", y=Freq, fill = TEXT) + 
  geom_bar(stat = "identity", position = "fill") + 
  scale_y_continuous(labels = scales::percent) +
  geom_text(aes(label=paste0(sprintf("%1.1f", Freq*100), "%")), position = position_stack(vjust = 0.5), colour = "white") +
  scale_fill_manual(values=randomcoloR::distinctColorPalette(10)) + #farbe festlegen
  #theme_ipsum() + #theme festlegen
  labs(title = "TITEL") + 
  labs(subtitle = "SUBTITEL") + 
  labs(caption = 'Quelle: ')







summarytools::freq(DF$V1)

#den datensatz erstellen mit den benötigten variablen
bar100_DF <- table(DF$V1) #benötigten variablen tablen
bar100_DF <- prop.table(bar100_DF, NULL) #relativen häufigkeiten erzeugen
bar100_DF <- as.data.frame(bar100_DF) # als objekt anlegen
bar100_DF$rating <- rownames(bar100_DF) #eine neue spalte anlegen "rating"
colnames(bar100_DF)[2] <- "Anteil" #spalt "freq" wird umbenannt auf platz [2] in "Anteil" könnte evtl. wo anders sein
bar100_DF$rating[bar100_DF$rating<=9999999] <- 0 # alle ausprägungen von "rating" 0 machen
names(bar100_DF)[1] <- paste("NAME_DER_SPALTE") #hier ändert man den namen der spalte / variable auf position [1]



ggplot(data=bar100_DF, aes(x=rating, y=Anteil, fill = NAME_DER_SPALTE)) + #NAME_DER_SPALTE von oben einsetzen
  geom_bar(stat = "identity", position = "fill") + 
  scale_y_continuous(labels = scales::percent) +
  geom_text(aes(label=paste0(sprintf("%1.1f", Anteil*100), "%")), position = position_stack(vjust = 0.5), colour = "white") +
  scale_fill_manual(values=palette) +
  theme_ipsum() +
  labs(title = "TITEL") + 
  labs(subtitle = "SUBTITEL") + 
  labs(caption = 'Quelle: ')

#100%_bar_plot_gruppen(ggplot)####



DF %>% 
  select(V1, VX) %>%
  table() %>% 
  prop.table(2) %>%
  as.data.frame() %>%
  rename("TEXT" = 1, "TEXT2" = 2) %>% 
  ggplot() +
  aes(x=TEXT2, y=Freq, fill = TEXT) + 
  geom_bar(stat = "identity", position = "fill") + 
  scale_y_continuous(labels = scales::percent) +
  geom_text(aes(label=paste0(sprintf("%1.1f", Freq*100), "%")), position = position_stack(vjust = 0.5), colour = "white") +
  scale_fill_manual(values=randomcoloR::distinctColorPalette(10)) + #farbe festlegen
  #theme_ipsum() + #theme festlegen
  labs(title = "TITEL") + 
  labs(subtitle = "SUBTITEL") + 
  labs(caption = 'Quelle: ')




summarytools::freq(DF$VY)
summarytools::freq(DF$VX)

#du musst dir einen dataframe erstellen und die daten aufbereiten

bar100_DF <- table(DF$VY, DF$VX) #benötigte variablen tablen VX - gruppenvariable
bar100_DF <- prop.table(bar100_DF, 2) #ich weiß noch nicht genau was die zahl heißen soll??? #auf jeden fall werden die reöativen häufigkeiten erzeugt
barplot(bar100_DF)
bar100_DF <- as.data.frame.matrix(bar100_DF)
bar100_DF$rating <- rownames(bar100_DF) #es wird eine neue variable erstellt "rating"
bar100_DF <- reshape2::melt(bar100_DF, id.vars=c("rating"), value.name = "Anteil") #die rows werden verbunden
names(bar100_DF)[2] <- paste("LEVELS") #hier ändert man den namen der spalte / variable #im plot auch anpassen!
bar100_DF$rating <- factor(bar100_DF$rating, ordered = TRUE, levels = c("LEVEL1", "LEVEL2", "...")) #hier muss man die levels / ausprägungen ordnen




#plotten

ggplot(data=bar100_DF, aes(x="LEVELS", y=Anteil, fill = rating)) + #hier den NAME von oben einsetzen
  geom_bar(stat = "identity", position = "fill") + 
  scale_y_continuous(labels = scales::percent) +
  geom_text(aes(label=paste0(sprintf("%1.1f", Anteil*100), "%")), position = position_stack(vjust = 0.5), colour = "white") +
  scale_fill_manual(values=palette) +
  theme_ipsum() +
  labs(title = "TITEL") + 
  labs(subtitle = "SUBTITEL") + 
  labs(caption = 'Quelle: ')

#likert_skala_plot ####

library(likert)

# Use a provided dataset
data(pisaitems)


pisaitems %>% 
  select(ST24Q01:ST24Q11) %>% 
  mutate(across(where(is.numeric), as.factor)) %>% 
  likert::likert() %>% 
  plot()


pisaitems %>% 
  select(ST24Q01:ST24Q04) %>% 
  sample_n(100) %>% 
  drop_na() %>% 

  pivot_longer(everything()) %>% 

  ggplot() +
  geom_bar(aes(x = value, y=reorder(name ,value), fill=name), position="stack", stat="identity") +
  coord_flip() + 
  scale_y_continuous(labels = scales::percent) +
  ggtitle("At my child's school my child is safe") +
  ylab("Percentage") +
  xlab("School Code") +
  scale_fill_brewer(palette="PRGn") +
  theme(legend.position="bottom") 
  #scale_x_discrete(limits=c("StronglyAgree", "Agree","Disagree","StronglyDisagree"))










#bsp:pyramid-plot(ggplot)####

#benötigte pakete
df %>%
  select(age, sex) %>% 
  table() %>% 
  as.data.frame.matrix() %>% 
  rownames_to_column("Alter") %>% 
  mutate(Frau = as.numeric(Frau),
         Mann = as.numeric(Mann),
         Alter = as.numeric(Alter)) %>% 
  mutate(Alter = ifelse(Alter >= 18 & Alter <= 22, "18-22", 
                 ifelse(Alter >= 23 & Alter <= 27, "23-27",
                 ifelse(Alter >= 28 & Alter <= 32, "28-32",
                 ifelse(Alter >= 33 & Alter <= 37, "33-37",
                 ifelse(Alter >= 38 & Alter <= 42, "38-42",
                 ifelse(Alter >= 43 & Alter <= 47, "43-47",
                 ifelse(Alter >= 48 & Alter <= 52, "48-52",
                 ifelse(Alter >= 53 & Alter <= 57, "53-57",
                 ifelse(Alter >= 58 & Alter <= 62, "58-62",
                 ifelse(Alter >= 63 & Alter <= 67, "63-67",
                 ifelse(Alter >= 68 & Alter <= 72, "68-72",
                 ifelse(Alter >= 73 & Alter <= 77, "73-77",
                 ifelse(Alter >= 78 & Alter <= 82, "78-82", "83 und älter")))))))))))))) %>% 
  group_by(Alter) %>% 
  mutate(Frau = sum(Frau),
         Mann = sum(Mann)) %>% 
  ungroup() %>% 
  distinct(Alter, .keep_all = TRUE) %>% 
  pivot_longer(names_to = 'Geschlecht', values_to = 'Population', cols = 2:3) %>% 
  mutate(PopPerc=case_when(Geschlecht=='Mann'~round(Population/sum(Population)*100,2),
                           TRUE~-round(Population/sum(Population)*100,2)),
         signal=case_when(Geschlecht=='Mann'~1,
                          TRUE~-1)) %>% 
  ggplot()+
  geom_bar(aes(x=Alter,y=PopPerc,fill=Geschlecht),stat='identity')+
  geom_text(aes(x=Alter,y=PopPerc+signal*.3,label=abs(PopPerc)))+
  coord_flip()+
  scale_fill_manual(name='',values=c('darkred','steelblue'))+
  scale_y_continuous(breaks=seq(-10,10,1),
                     labels=function(x){paste(abs(x),'%')})+
  labs(x='',y='Befragte (%)',
       title='Altersverteilung nach Geschlecht',
       subtitle=paste('SUBTITEL'),
       caption='Quelle: ')+
  cowplot::theme_cowplot()+
  theme(axis.text.x=element_text(vjust=.5),
        panel.grid.major.y = element_line(color='lightgray',linetype='dashed'),
        legend.position = 'top',
        legend.justification = 'center')


#wordcloud####

#eine tabelle erstellen und als einen dataframe speichern
df <- table(DF$V1) 
df <- as.data.frame(df)

wordcloud2::wordcloud2(data=df, size=.3)
# year over year ####
categories <- c("Electronics", "Home&Garden", "Automotive", "Toys")

countries <- c("DE", "US", "UK", "FR")


revenue_df <- expand.grid(
  Category = categories, 
  Country = countries, 
  Year = c("2020", "2021"),
  Month = month.name
) %>% 
  as_tibble() %>% 
  mutate(
    GMV = round(100000 * runif(n(), 1, 50)),
    Quantity = round(
      GMV / case_when(
        Category == "Electronics" ~ 200,
        Category == "Home&Garden" ~ 150,
        Category == "Automotive" ~ 1000,
        Category == "Toys" ~ 80
      )
    ),
    Sellers = round(1000 * runif(n(), 1, 10))
  ) %>% 
  filter(Year == "2020" | Month %in% month.name[1:10])

revenue_df %>% 
  write_csv("revenue.csv")


old_loc <- Sys.getlocale("LC_TIME")
Sys.setlocale(category = "LC_TIME", locale = "English")

gmv_df <- read_csv(here::here("revenue.csv")) %>% 
  mutate(date = as.Date(strptime(str_c("01-", Month, "-", Year), format = "%d-%B-%Y")))

Sys.setlocale(category = "LC_TIME", locale = old_loc)




plot_data <- read_csv("revenue.csv") %>% 
  mutate(date = as.Date(strptime(str_c("01-", Month, "-", Year), format = "%d-%B-%Y"))) %>% 
  filter(Category == "Toys", Country == "DE") %>% 
  select(Year, GMV, date) %>% 
  mutate(date = if_else(Year == "2020", date %m+% years(1), date)) %>% 
  pivot_wider(id_cols = date, names_from = Year, values_from = GMV, names_prefix = "GMV_") %>% 
  filter(between(date, max(date) %m-% months(7), max(date))) %>% 
  drop_na() %>% 
  mutate(YoY = GMV_2021 / GMV_2020 - 1)


max_y <- plot_data %>% select(-YoY) %>% pivot_longer(-date, names_to = "Figure") %>% pull(value) %>% max()
min_y <- plot_data %>% select(-YoY) %>% pivot_longer(-date, names_to = "Figure") %>% pull(value) %>% min()

plot_data <- plot_data %>% 
  mutate(YoY = (YoY - min(YoY)) * (max_y - min_y) / diff(range(YoY)) + min_y) %>% 
  pivot_longer(-date, names_to = "Figure") %>% 
  mutate(Figure = ifelse(Figure=="GMV_2020", "2020",
                         ifelse(Figure=="GMV_2021", "2021", Figure)))


ggplot(plot_data %>% filter(Figure != "YoY")) + 
  geom_bar(data = plot_data %>% 
             filter(Figure == "YoY") %>% 
             rename(YoY = value), 
           aes(x = date, y = YoY),
           stat = "identity", alpha = 0.6) +
  geom_point(aes(x = date, y = value, color = as.factor(Figure))) +
  geom_line(aes(x = date, y = value, color = as.factor(Figure))) +
  scale_y_continuous(labels=scales::dollar_format(suffix="€",prefix="")) +
  labs(title= "YoY / Toys") +
  labs(colour = "Year")

#_####

#_####
# plotly ####
DF %>%
  plot_ly(x = ~V1) %>% 
  add_trace(y = ~V2, type = "scatter") %>% 
  layout(
    title = "TITLE",
    xaxis = list(title = "XAXIS"),
    yaxis = list(title = "YAXIS")
  )
#_####
#_####
#_####
#_####
#_####
#_####
#_####
#_####
#tabellen visualisieren mit stargazer::####

# latex output
stargazer::stargazer(OBJECT, type = "latex")

# html save
stargazer::stargazer(OBJECT, "html", out = "stargazer.html")

#_####
#_####
#_####
#_####
#_####
#_####
#_####
#_####
#_####
#_####
#_####
#################################################bivariate statistik####

#                                                      Y
#
#                    |                                 |
#                    |          metrisch               |                 kategorial        
#                    |_________________________________|_____________________________________________
#        metrisch    |                                 |              
#                    |      korrelationsanalyse        |              latente klassenanalyse
#     X              |           X<->Y                 |                     X->Y
#                    |_________________________________|______________________________________________
#                    |                                 |            
#        kategorial  |        varianzanalyse           |              kreuztabellenanalyse
#                    |           X->Y                  |                      X<->Y
















#_####
#################   unterschiede    ####
########### der richtige (test) mittelwertvergleich####
#
#
#                                     mittelwertvergleich
#
# stichproben____________|______________abhängig___________________|____________unabhänig_____________
#                        |                 |                       |                |
# gruppen / messungen____|__2______________|__>2___________________|__2_____________|__>2____________
#                        |                 |                       |                |
# av = metrisch /        |  t-test für     |  einfache anova       |  t-test für    |  einfache anova
#      normalverteilt    |  abh. sp        |  mit                  |  unabh. sp     |  (mehr als 2 uvs
#                        |                 |  messwiederholung     |                |   mehrfaktorielle
#                        |                 |                       |                |   anova)
#                        |                 |                       |                |
# av = nicht metrisch /  |  vorzeichen-    |  friedmann-           |  mann-whitney- |  kruskal-wallis-
#      verteilungsfrei   |  test           |  test                 |  u-test        |  test


##stichproben

#abhängig:    gleiches untersuchungsobjekt zu unterschiedlichen zeitpunkten
#unabhängig:  unterschiedliche untersuchungsobjekte (zeitpunkt egal)


##gruppen / messungen

#2:   paarweiser vergleich
#>2:  paarweiser vergleich nicht möglich (alpha-fehlerkumulierung; fehler 1. art h1 annehmen obwohl h0                                              zutrifft) (bonfferoni korrektur kann abhilfe schaffen)


##verteilung & skallierung der AV

#normalverteilt:             
#                       parametrischer test
#metrisch / intervall:

#verteilungsfrei:
#                       non-parametrischer test
#nicht metrisch: 

#ist eines der beiden kriterien für einen metrischen test nicht erfüllt dann nimmt man einen non-parametrischen test






#######prüfung der av auf normalverteilung####

#wenn man einen metrischen test rechnen will dann müssen die verteilungen für alle gruppen normalverteilt sein
#man zieht sich ein subset für jede gruppe einzeln und überprüft dieses auf normalverteilung

####analytisch

#kolmogorov-smirnov-test
ks.test(DF$AV)
#H0: normalverteilung liegt vor

#shapiro wilk test 
shapiro.test(DF$AV) #achtung ist bei großer sp nicht verlässlich
#(Der Shapiro-Wilk-Test ist ein statistischer Signifikanztest, der die Hypothese überprüft, dass die zugrunde liegende Grundgesamtheit einer Stichprobe normalverteilt ist.)
#H0 nimmt an, dass eine Normalverteilung der Grundgesamtheit vorliegt
#H1 dass keine Normalverteilung gegeben ist.
#Nullhypothesenicht abgelehnt, wenn der p wert größer ist
# >0.05 = H0 gillt
# <0.05 = H1 gillt

#problem mit analytischen tests: 
#kleine SP = power gering
#große SP  = p-wert sinkt automatisch
#(Field 2018 S. 248f.)

####grafisch

DescTools::PlotQQ(DF$V1)


qqnorm(scale(as.numeric(DF$V1)))
qqline(scale(as.numeric(DF$V1)))


#histogram
#histo #histogram empfholen zur prüfung auf normalverteilung
hists(as.numereic(DF$AV))


#####t-test####
#prüfung auf normalverteilung beim t.test


GROUP1 <- DF %>% 
  dplyr::select()

GROUP2 <- DF %>% 
  dplyr::select()

#beim t-test muss du dir ein neues subset erstellen und die gruppen einzeln auf normalverteilung prüfen 
GROUP1 <- subset(DF, DF$VX == 01234 |"LABEL") 
#gruppenvariable VX: Geschlecht 0=männer | 1=frauen bestimmen nach der gefiltert wird, oder LABEL: "MANN" "FRAU"
GROUP2 <- subset(DF, DF$VX == 01234 |"LABEL") 


shapiro.test(GROUP1$AV) #p <0.05 = Normalverteilt #AV ist die abjängige variable die getestet wird durch die gruppen

#visuelle überprüfung auf normalverteilung
hist(GROUP1$AV)

ttest <- subset(DF, select=c(AV, VX))
ttest <- subset(ttest, !(is.na(AV))) #NA's löschen
ttest <- subset(ttest, !(is.na(VX))) #NA's löschen

###t-test für unabhängige SP####



#varianzen prüfen (gleiche / ungleiche varianzen) 
#bei ungleichen varianze (welch-test) #heißt nichts anders als var.equal = F
psych::describeBy(ttest$AV, ttest$VX) #VX gruppenvariable
#gleiche varinzen (sd) var.equal = T, / #ungleiche varianzen (sd) var.equal = F, #zweiseitger test
#unabhängige
t.test(ttest$AV~ttest$VX, var.equal = T/F, alternative = "two.sided") #VX gruppenvariable  #alternative: "greater", "less"


#wie groß ist der unterschied vom t-test
lsr::cohensD(ttest$AV ~ ttest$VX)


#ergebnis einordnen
#### A Power Primer (1992)
###Jacob Cohen
##s. 157 / punkt 1. (cohens D)
#effektstärken

#   small     #medium     #large
#   0.20       0.50        0.80









###t-test für abhängige SP####






#man muss sich an der stelle einen datensatz erzeugen, welcher ausschließlich fälle enthält, aus der gruppe, die man testen möchte #VX ist die gruppenvariable z.b männer-frauen; arm-mittel-reich etc. 
#über wert (1234) z.b. männer:0 frauen:1 oder üver LABEL: "FRAU", "MANN" etc.
GROUP1 <- subset(DF, DF$VX == 01234 |"LABEL") 
GROUP2 <- subset(DF, DF$VX == 01234 |"LABEL") 
GROUP3 <- subset(DF, DF$VX == 01234 |"LABEL") 



t.test(GROUP1$T0, GROUP1$T10, paired = T, alternative = "two.sided") #T0: 1.messzeitpunkt #T10: 2. messzeitpunkt   #alternative: "greater", "less" (das sind einseitige stichproben)
#wie groß ist der unterschied vom t-test
lsr::cohensD(GROUP1$T0, GROUP1$T10, method = "paired")

t.test(GROUP2$T0, GROUP2$T10, paired = T, alternative = "two.sided") #T0: 1.messzeitpunkt #T10: 2. messzeitpunkt   #alternative: "greater", "less" (das sind einseitige stichproben)
#wie groß ist der unterschied vom t-test
lsr::cohensD(GROUP2$T0, GROUP2$T10, method = "paired")

t.test(GROUP3$T0, GROUP3$T10, paired = T, alternative = "two.sided") #T0: 1.messzeitpunkt #T10: 2. messzeitpunkt   #alternative: "greater", "less" (das sind einseitige stichproben)
#wie groß ist der unterschied vom t-test
lsr::cohensD(GROUP3$T0, GROUP3$T10, method = "paired")


#ergebnis einordnen
#### A Power Primer (1992)
###Jacob Cohen
##s. 157 / punkt 1. (cohens D)
#effektstärken

#   small     #medium     #large
#   0.20       0.50        0.80





#########ANOVA####
###voraussetzungen:
#intervallskallierte (metrische) AV
#normalverteilte residuen
#alternativ: normalverteilte AV je gruppe
#test auf normalverteilung     
#varianzhomogenität
#levene-test
#H0: varianzhomogenität/ varianzgleichheit 
#bei kleiner SP geringe teststärke (--> h0 wird seltener verworfen als sie es müsste)
#bei zu großer SP zu "empfindlich" (--> h0 wird zu häufig verworfen)

# anova (einfaktorielle varianzanalyse) ist ein mittelwertvergleich 
#eignet sich für > 2 gruppen
# es wird getestet ob ein signifikanter unterschied der AV zwischen gruppen auftritt (bei einer UV - einfaktoriell)
# H0 : kein unterschied -> ziel = verwerfung von H0

#manchmal gibt es probleme mit fehlenden werten wenn man die effektstärke berechnen will
#der folgende befehl löscht alle fälle in denen die variable NA's hat
ANOVA <- subset(DF, select=c(V1, VX))
ANOVA <- subset(ANOVA, !(is.na(V1)))
ANOVA <- subset(ANOVA, !(is.na(VX)))

###einfaktoriell (one-way) ####


#modell definieren

aov_model <- aov(ANOVA$AV ~ ANOVA$VX) #AV abhängige varible #XV gruppenvariable



###prüfung auf normalverteilnug

hist(rstandard(aov_model)) #muss in etwa normalverteilt sein
plot(aov_model, 2) #müssen in etwa auf der linie liegen


###levene-test auf homogenität

#desskriptiver test
psych::describeBy(ANOVA$AV, ANOVA$VX) #die sd muss sich stark unterscheiden # sd ist auch ein deskriptiver test auf gruppenunterschiede

#analytischer test
car::leveneTest(ANOVA$AV, ANOVA$VX) #p-wert <0.05 = H0 (Varianzhomogenität) verwerfen!



####varianzanalyse (ANOVA)
summary(aov_model) #p-wert <0.05 = signifikanter unterschied 

pairwise.t.test(ANOVA$AV, ANOVA$VX, p.adjust="bonferroni")


####effektstärke berechnen
#wie groß ist der effekt, den wir mit der anova festgestellt haben?

DescTools::EtaSq(aov_model) #eta quadrat (eta.sq) wert interessiert uns
#eta quadrat in einen f-wert umrechnen
sqrt(eta.sq/(1-eta.sq)) #werte für "eta.sq" einsetzen 

#ergebnis einordnen
#### A Power Primer (1992)
###Jacob Cohen
##s. 157 / punkt 7. (one-way analysis of variance)
#effektstärken

#   small     #medium     #large
#   0.10       0.25        0.40







##mit messwiederholung####

####datensatz erstellen

#man muss sich an der stelle einen datensatz erzeugen, welcher ausschließlich fälle enthält, aus der gruppe, die man testen möchte #VX ist die gruppenvariable z.b männer-frauen; arm-mittel-reich etc. 
#über wert (1234) z.b. männer:0 frauen:1 oder üver LABEL: "FRAU", "MANN" etc.
GROUP1 <- subset(DF, DF$VX == 01234 |"LABEL") 
GROUP2 <- subset(DF, DF$VX == 01234 |"LABEL") 
GROUP3 <- subset(DF, DF$VX == 01234 |"LABEL") 

#evtl. muss man die gruppen auch erst einteilen im kapitel recode und bsp:recode gibt es genügend beispiele


DF_LONG <- subset(GROUP1, select=c(VX, V1, V2, VN, ...)) #VX: gruppenvariable #V1 1. messung V2 2. messung ...
DF_LONG <- data.matrix(DF_LONG)
DF_LONG <- as.data.frame(DF_LONG)
colnames(DF_LONG) <- c("ID", "0", "1" "N", ...) #die gruppenvariable wird in ID umbenannt


#daten müssen für eine messwiederholung aus dem wide-format in das longformat umgewandelt werden

DF_LONG <- tidyr::gather(DF_LONG, TIME, VALUE, "1":"N") #"1" ist die variable für den ersten messzeitpunkt. "N" für den letzten messzeitpunkt
DF_LONG$TIME <- as.numeric(DF_LONG$TIME) #TIME noch numerisch codieren


#modell definieren

aov_model <- aov(DF_LONG$VALUE ~ DF_LONG$TIME + Error(DF_LONG$ID/DF_LONG$TIME)) #die variablen sind
summary(aov_model)


#graph anzeigen lassen um den Verlauf über die Zeitmessungen zu beobachten
aov_plot <- tapply(DF_LONG$VALUE, DF_LONG$TIME, mean, na.rm=T)
plot(aov_plot, type ="o", xlab= "TIME", ylab= "AV")


#für die anderen gruppen wiederholen




###mehrfaktoriell (two-way) ####









##mit messwiederholung####
###mann-whitney-u-test####

#varianzen prüfen (gleiche / ungleiche varianzen)
psych::describeBy(as.numeric(DF$AV), DF$VX)

#mann-withney-test
wilcox.test(as.numeric(AV) ~ VX, data = DF, exact=F, correct = F, conf.int = T)



###chi-quadrat-test####

#prüft zwei merkmale auf unabhängigkeit

#h0 = die beiden merkmale sind unabhängig voneinander
#alternativhypothese = die beiden merkmale sind abhängig voneinander
#voraussetzung: nominal oder ordinal skallierte variablen

XTABLE <- xtabs(~ DF$AV + DF$UV)

chisq.test(DF$VX, DF$VX) #wenn warnung kommt: fisher.test !!
fisher.test(DF$VX, DF$VX) #wahrer p.wert

#erwartete und beobachtete häufigkeiten

N <- sum(XTABLE)  

expected <- outer (rowSums(XTABLE), colSums(XTABLE)) / N 
expected



###kruskall-wallis-test####



#H0: kein unterschied bzw. AV's sind gleich
#Ziel: verwerfung von H0

#Voraussetzung: AV muss ordinal-skalliert sein

kruskal.test(DF$AV ~ DF$VX) #je größer chi-squared desto eher wird das signifikant

#jetzt wissen wir aber noch nicht, zwischen welchen gruppen ein unterschied besteht. das finden wir mit dem pairwise test raus

pairwise.wilcox.test(as.numeric(DF$AV),  DF$VX, paired = F, p.adjust.method = "bonferroni")

psych::describeBy(as.numeric(DF$AV),  DF$VX)


##bsp: kruskal-wallis-test####
#hier wird verglichen, wie sich die verschiedenen gruppen, die nach einkommen gebildet wurden in der beantwortung auf die variable (161 Label: ERFOLGSBED.,BRD: KLASSENZUGEHOERIGKEIT) unterscheiden

#zuerst werden die daten aufbereitet 

#die variable wird recodiert und die gehaltsklassen in gruppen eingeteilt
all$V556neu <- as.numeric(all$V556)
all$V556neu <- car::recode(all$V556neu, "1=1; 2=1; 3=1; 4=1; 5=1; 6=1; 7=1 ;8=1 ; 9=2 ; 10=2 ;11=2 ; 12=2 ; 13 =2 ; 14=2 ; 15= 3; 16=3 ; 17=3 ; 18=3 ; 19=3 ; 20=4 ; 21=4 ; 22=4")  
summarytools::freq(all$V556neu)
sehr_wenig <- subset(all, all$V556neu == 1)
wenig <- subset(all, all$V556neu == 2)
mittel <- subset(all, all$V556neu == 3)
viel <- subset(all, all$V556neu == 4)

#die AV wird numerisch gemacht
all$V161_num <- as.numeric(all$V161)


#hier kann man sich die verteilung in einem histogram ansehen 
hist(as.numeric(sehr_wenig$V161))
hist(as.numeric(wenig$V161))
hist(as.numeric(mittel$V161))
hist(as.numeric(viel$V161))

#und die unterschiede der lagemaße
psych::describeBy(all$V161_num, all$V556neu)


#jetzt rechnen wir den kruskal.test: unsere variable ist ordinal skalliert und verteilungsfrei und wir haben mehr als 2 gruppen

kruskal.test(all$V161 ~ all$V556neu) #je größer chi-squared desto eher wird das signifikant

#jetzt wissen wir aber noch nicht, zwischen welchen gruppen ein unterschied besteht. das finden wir mit dem pairwise test raus

pairwise.wilcox.test(all$V161_num, all$V556neu, paired = F, p.adjust.method = "bonferroni")



###kreuztabelle#####









gmodels::CrossTable(DF$V1, DF$V2, prop.r = T, prop.c = TRUE, prop.t = T, prop.chisq = F)








####korrelationsmatrix####


KORMATRIX <- subset(DF, select=c(V1, V2, ...)) #wenn der data frame zu groß ist -> vorher sample ziehen (kednall korrelation dauert sonst zuuu lange)
KORMATRIXm <- cor(KORMATRIX, use = "complete.obs", method = "") #"pearson" |"kendall" |"spearman"
KORMATRIXp <- corrplot::cor.mtest(KORMATRIXm)

corrplot::corrplot(KORMATRIXm, method="number", type = "upper", col="black")
corrplot::corrplot(KORMATRIXm, p.mat=KORMATRIXp$p, method="number", type = "upper", sig.level = 0.95)
corrplot::corrplot.mixed(KORMATRIXm, upper ="circle", lower="number", lower.col ="black")




#_####
############### zusammenhänge####
###korrelation (pearson)####



#voraussetzungen
#1. metrisch skalierte variablen
#2. keine ausreißer ---> boxplot
#3. linearer zusammenhang ---> streudiagramm 
plot(DF$V1, DF$V2)
#4. bivariate normalverteilung
      #zum vergleich: univariate normalverteilung = die variablen sind in sich normalverteilt
  #kann man nicht so richtig prüfen?? ----> ab einer fallzahl von 30 ist diese meistens gegeben


 
#korrelation teten 
cor(DF$V1, DF$V2)

# signifikanz test #p-wert
cor.test(DF$V1, DF$V2, alternative = "two.sided", conf.level = 0.95) #"pearson" #"kendall" #"spearman"

#plot
plot(DF$V1, DF$V2)








###korrelation (kendal)####

#bei kleinen stichproben 

#https://www.youtube.com/watch?v=ux84UCN21Vk




###korrelation (spearmann)####

#wenn eine variable ordinal und die andere metrisch skalliert ist

#https://www.youtube.com/watch?v=APTTJAGmJ_A




###korrelation (punktbiseriale)####


#eine kategorial dichotome variable und eine metrische

#einfachte pearson korrelation !
#ist die av dichotom dann wird automatish eine punktbiseriale korrelation gerechnet

#https://www.youtube.com/watch?v=dsz0L9KdKyI



#_####
#_####
##### stl timeseries analysis zeitreihen analyse ####
# time serie analysis
#https://www.youtube.com/watch?v=nfImWHs7kPw
#https://github.com/business-science/timetk

library(timetk)


DF %>%
  #unite("V_TIME", DATUMsvariable1, DATUMsvariable2, sep=" ", remove = F) %>%
  #mutate(V_TIME= as.POSIXct(V_TIME, format = "%Y-%m-%d %H:%M:%S")) %>% 
  timetk::plot_stl_diagnostics(
    V_TIME, V1,
    .feature_set = c("observed", "season", "trend", "remainder"),
    .frequency   = "auto",
    .trend       = "auto",
    .interactive = T)

DF %>% 
  select(V_TIME, V1) %>% 
  tsibble::as_tsibble() %>% 
  column_to_rownames(var = "V_TIME") %>%
  unlist() %>% 
  ts(data = as.vector(zoo::coredata(.)),
     frequency = 360) %>% 
  decompose() %>%
  plot()



#### changepoint analysis #####
#change point analyse
# breakpoints #break points
#_####
#_####
#_####
#_####
#_####
#_####
#_####
#_####
#########################################multivariate statistik####
###grundannahmen der regression###
  

#$prämisse$                  $verletzung$                   $konsequenz$
#_________________________________________________________________________________
#linearität         |     nicht linearität      |    verzerrung der schätzwerte
#                   |                           |
#unkorreliertheit   |     Korrelation           |    ineffizienz der schätzer
#von U & X          |                           |
#                   |                           |
#homoskedaszitidäz  |     heteroskedastizität   |    ineffizienz der schätzer
#                   |                           |                                  
#unkorreliertheit   |     autokorrelation       |                   
#der residuen       |     der residuen          |    ineffizienz der schätzer  
#                   |                           |
#normalverteilung   |     residuen sind         |    ungültigkeit der signifikanz-
#der residuen       |     nicht normalverteilt  |    tests bei kleinem n
#                   |                           |
#keine multi-       |     multikolliniarität    |    ineffizienz der schätzer    
#kolliniarität
#
#                                                   AV - Y
#                                        
#                               ohne kontexbezug       |      mit kontextbezug                
#______________________________________________________|_____________________________________________   
#                    |                                 |
#                    |                                 |
#                    |                                 |
#                    |    metrisch        kategorial   |    metrisch        kategorial
#                    |_________________________________|_____________________________________________
#                    |    lineare         logistische  |  
#        metrisch    |    regression      regression   |   
#                    |                                 |
# UV - X             |                                 |     lineare-        logistische-
#                    |                                 |         -mehrebenenanalyse 
#                    |    lineare-        logistische- |   
#        kategorial  |    dummy-          dummy-       |
#                    |    regression      regression   |
#
#
#
#
#
#
#
#
#######lineare regression####

#   https://journal.r-project.org/archive/2017/RJ-2017-046/RJ-2017-046.pdf
#   https://www.gmudatamining.com/lesson-10-r-tutorial.html#Creating_a_Machine_Learning_Workflow

# av = kriteritum |  regressand
# uv = prediktor  |  regressoren

#_______________________________________________________________________________________________

  # lineares model
  
FORMEL <- AV ~ UV1 + UV2 + UV3 + UV4

#        eine variable wird: "entfernt" - | "hinzugefügt" +
FORMEL <- update(FORMEL, . ~ . - UV1)

# code für lineares model
MODEL <- lm(FORMEL, data=DF)
summary(MODEL)

#heteroskedaszitität robuste schätzung

lmtest::coeftest(MODEL, sandwich::vcovHC(MODEL, type=c("HC3")))


  # z-standardisierung um die estimates untereinander zu vergleichen
DF$UV1 _z <- scale(DF$UV1)

FORMELz <- AVz ~ UV1z + UV2z + UV3z + UV4z


LMz <- lm(FORMELz, data=DF)
summary(LMz)



#regressionstbellen in ergebnistabellen wiedergeben

sjPlot::tab_model(
  MODEL
  pred.labels = c("Intercept", "V1)", "V2)", "V3)",
                  "V4"),
  dv.labels = c("TABELLE1", "TABELLE2"),
  string.pred = "Coeffcient",
  string.ci = "Conf. Int (95%)",
  string.p = "P-Value"
)

Hinweise:
  
  Abgetragen sind die nicht standardisierten Regressionskoeffizienten einer OLS- Regression.

Alle Werte sind auf 2 Nachkommastellen gerundet.

x)  Fünfstufige Skala 1 Sehr Gut, 2 Gut, 3 Zufriedenstellend, 4 Weniger gut, 5 Schlecht
y)  Zehnstufige Skala von 1 Links bis 10 Rechts
z)  Offene Abfrage


#######Grundannahmen####



# 1 homoskedastizität

plot(fitted.values(MODEL), residuals(MODEL))
plot(fitted.values(MODEL), rstandard(MODEL))
plot(MODEL)


# 2 Autokorrelation
#Der Durbin-Watson-Test ist ein statistischer Test, mit dem man versucht zu überprüfen, ob eine Autokorrelation 1. Ordnung vorliegt, d. h., ob die Korrelation zwischen zwei aufeinanderfolgenden Residualgrößen bei einer Regressionsanalyse ungleich null ist. Der Test wurde von dem britischen Statistiker James Durbin und dem Australier Geoffrey Watson entwickelt. eignet sich eher für längsschnittdaten (messwiderholung) - nicht querschnittsdaten
car::dwt(MODEL)

# 3 normalverteilung der residuen

hist(rstandard(MODEL))
DescTools::PlotQQ(rstandard(MODEL))


# 4 keine multikolliniarität

car::vif(MODEL)#die werte sollten unter 10 liegen (Wooldridge, J.M. (2009). Introductory econometrics: A modern approach, S. 99)
1/car::vif(MODEL)# die werte sollten über 0.1 liegen alles drunter ist problematisch

# wenn die werte nicht passen dann sollte man sich gedanken machen, ob die varaiblen mit unpassenden werte nicht das gleiche messen





#######Grundannahmen NKB ####


ppver_res = residuals(linreg)
ppver_hat = fitted(linreg)
ppver_hatp=predict(linreg)
res=as.data.frame(cbind(ppver_res, ppver_hat, ppver_hatp))

## Mulitkolinearität
cor.test(pinter,EVS_WVS$E226)
car::vif(linreg)

## Erwartungswerrt = 0
summary(residuals(linreg))
plot(linreg)

## Homoskedastizität
plot(fitted(linreg), residuals(linreg), pch=20, ylab="Residuen",
     xlab="Geschätzte Werte Vertrauen in Parteien", main="Homoskedastizität")
abline(h=0, col="blue", lwd=2)

## keine Autokorrelation
car::dwt(linreg)

## Normalverteilung der Residuen
qqnorm(residuals(linreg))
qqline(residuals(linreg), col="red" , lwd=2)

hist(residuals(linreg))

## Unabhängigkeit von X und U
plot(fitted(linreg), residuals(linreg), pch=20, ylab="Residuen",
     xlab="Geschätzte Werte Vertrauen in Parteien", main="Unabhängigkeit von X und U")
abline(h=0, col="blue", lwd=2)
####regressionsdiagnostik####
        ###daniel wollschläger grundlagen der datenanalyse mit r s.185
  
    ## ausreißer
  
  N=ZAHL_DER_FÄLLE

DIAG <- subset(DF, select=c(AV, UV1, UV2, UV3, UV4))
#DIAG$V <- as.character(DIAG$V)# wenn es faktorvariablen gibt die nicht korrekt numerisch gemacht werden
DIAG <- data.frame(lapply(DIAG, function(x) as.numeric(x)))

Xpred <- subset(DF, select=c(UV1, UV2))
#Xpred$V <- as.character(Xpred$V) wenn es faktoren gibt die nicht korrekt numerisch transofmiert werden
Xpred <- data.frame(lapply(Xpred, function(x) as.numeric(x)))
Xz <- scale(Xpred)                      # z-transformierte
boxplot(Xz, main="Verteilung der Prädiktoren")     # boxplots
summary(Xz)

  # mahalanobis-distanz der beobachtungen zum zentroid der prädiktore
ctrX   <- colMeans(Xpred, na.rm = TRUE)   # zentroid
sX     <- cov(Xpred, use = "complete.obs")        # kovarianzmatrix
mahaSq <- mahalanobis(Xpred, ctrX, sX)  # quadrierte m-distanzen
summary(sqrt(mahaSq))

fitHAS <- lm(AV ~ UV1 + UV2, data=DIAG) # regression
h      <- hatvalues(fitHAS)                 # hebelwerte
h <- as.numeric(h)
hist(h, main="Histogramm der Hebelwerte")   # histogramm
summary(h)

all.equal(mahaSq, (N-1) * (h - (1/N)), check.attributes=FALSE)

inflRes <- influence.measures(fitHAS) #diagnosegrößen
summary(inflRes) #auffällig beobachtungen


cooksDst <- cooks.distance(fitHAS)    # cooks Distanz
plot(fitHAS, 4)                       # spike-plot

  # manuelle berechnung
P   <- X                                          # anzahl prädikatoren
E   <- residuals(fitHAS)                          # anzahl residuen
MSE <- sum(E^2) / (N - (P+1))                     # quad. standardschätzfehler
CD  <- (E^2 / (MSE * (1-h)^2)) * (h / (P+1))      # cooks distanz
all.equal(cooksDst, CD)                           # kontrolle

  ## residuen 
  
Estnd <- rstandard(fitHAS)                # standardisierte residuen
all.equal(Estnd, E / sqrt(MSE * (1-h)))   # manuelle Kontrolle

  #studentisierte residuen und manuelle kontrolle
Estud <- rstudent(fitHAS)
all.equal(Estud, E / (lm.influence(fitHAS)$sigma * sqrt(1-h)))

hist(Estud, main="Histogramm stud. Residuen", freq=FALSE  )   # histogramm
curve(dnorm(x, mean=0, sd=1), col="red", lwd=2, add=TRUE) # dichtefunktion der standardnormalverteilung

qqnorm(Estud, main="Q-Q-Plot stud. Residuen")                 # q-q-plot
qqline(Estud, col="red", lwd=2)                 # referenzgerade für nv


  #shapiro-wilk-test auf normalverteilung der studentisierten residuen
shapiro.test(Estud)


  # studentisierte residuen gegen Vorhersage darstellen
plot(fitted(fitHAS), Estud, pch=20, xlab="Vorhersage", ylab="studentisierte Residuen", main="Spread-Level-Plot")
abline(h=0, col="red", lwd=2)   # referenz fürmModellgültigkeit


lamObj  <- car::powerTransform(fitHAS, family="bcPower")
(lambda <- coef(lamObj))                  # max log-likelihood lambda

  #transformiertes kriterium und manuelle Kontrolle
wTrans <- car::bcPower(weight, coef(lambda))   # Transformation
all.equal(wTrans, ((weight^lambda) - 1) / lambda)   # manuell


  # Multikollinearität
Rx <- cor(Xpred, use = "complete.obs")

car::vif(fitHAS) 

diag(solve(Rx)) # Diagonalelemente der Inversen der Korrelationsmatrix

  # regressionsmodell mit standardisierten prädiktoren
fitScl <- lm(scale(AV) ~ scale(UV1) + scale(UV1), data=DIAG)
kappa(fitScl, exact=TRUE)

X        <- model.matrix(fitScl)            # designmatrix
(eigVals <- eigen(t(X) %*% X)$values)       # eigenwerte von X^t * X

  # konditionsindizes: jeweils wurzel aus eigenwert / (minimum != 0)
sqrt(eigVals / min(eigVals[eigVals >= .Machine$double.eps]))

  # vif: regression mit standardisierten variablen -> kein unterschied
car::vif(fitScl)

  # kappa: regression mit ursprünglichen Variablen -> Unterschied
kappa(lm(KRITERITUM ~ PRÄDIKTOR_1 + PRÄDIKTOR_2, data = DIAG), exact=TRUE)



###Measures of Influence####
  #https://cran.r-project.org/web/packages/olsrr/vignettes/influence_measures.html
  
olsrr::ols_plot_cooksd_chart(MODEL) #cook’s d chart
olsrr::ols_plot_dfbetas(MODEL) #DFBETAs panel
olsrr::ols_plot_dffits(MODEL) #DFFITS plot
olsrr::ols_plot_resid_stud(MODEL) #studentized residual plot
olsrr::ols_plot_resid_stand(MODEL) #standardized residual chart
olsrr::ols_plot_resid_lev(MODEL) #studentized residuals vs leverage plot
olsrr::ols_plot_resid_stud_fit(MODEL) #deleted studentized residual vs fitted values plot
olsrr::ols_plot_hadi(MODEL) #Hadi Plot
olsrr::ols_plot_resid_pot(MODEL) #Potential Residual Plot


###logistische regression####
  
      ## voraussetzung
  
    #dichotome kategoriale variable
  
  # nullmodel
model0 <- glm(AV ~ 1, data=DF, family = binomial())
  
  # testmodel
GLM <- glm(AV ~  UV1 + UV2 + ..., data=DF, family = binomial())
summary(GLM)
#das vorzeichen des `Estimates` gibt die richtung der wahrscheinlichkeit an

  # omnibustest
#mit dem omnibustest bekommen wir mit `chisqp` einen wert, ob wir die ergebnisse des `GLM` überhaupt interpretieren dürfen
modelchi <- GLM$null.deviance - GLM$deviance
chidf <- GLM$df.null - GLM$df.residual
1-pchisq(modelchi, chidf) #der wert gibt uns an ob das erklärungsmodell signifikant besser angepasst ist als das 0 modell
  
  # odds ratios #chancenverhältnisse
exp(cbind(OR = coef(GLM), confint(GLM))) #die zahl so interpretieren wie sie ist (keine kommaverschiebung)
#interpretation: steigt die uv um eine einheit, dann steigt das chancenverhältnis um 1234  die ausprägug  der av anzunehmen (vorzeichen des estimates)
#ist die uv dichotom (0/1) dann wird dieniedrigere kategorie als reffernz kategorie genommen
#tip: man kann das inverse / reziproke bilden: 1/OR dann wird die andere kategorie zur refferenzkategorie

  # gütemaße
    #r^2
n <- length(GLM$residuals)
R2cs <- 1-exp((GLM$deviance-GLM$null.deviance)/n) #cox & snell r^2 (wertebereich 0 bis 0.75)
R2n <- R2cs/(1-exp(-(GLM$null.deviance/n))) #nagelkerkes r^2 
  
  
#bsp: logistische regression####
  # formel für logistische regression$
  
formel_glm <- E069_12_dicho ~ E033_num + erwerbs + abitur

glm

logmod <- glm(formel_glm, data = evs, family = "binomial")
summary(logmod)

  # koeffizienten extrahieren (veränderung der LOGITS)
coef(logmod)

  # koeffizienten umrechnen (ver)
exp(coef(logmod))


  # berechnen der average marginal effects (AME)
mod.ame = margins::margins(logmod)
summary(mod.ame)
plot(mod.ame)

  # log-likelihood
logLik(logmod)

  # pseudo r-squared und likelihood ratio test
rcompanion::nagelkerke(logmod)


  # likelihood ratio test händisch
  
logmod_zero <- glm(E069_12_dicho ~ 1, data=evs, family = "binomial")
summary(logmod_zero)
lmtest::lrtest(logmod_zero, logmod)


    ## modeldiagnostik
  
    #visuelle inspektion auf linearität
  
predict_prob <- predict(logmod, evs, type="response")
predict_logit <- log(predict_prob /(1-predict_prob))
hist(predict_logit)

plot(evs$E033_num, predict_logit)

  # einflussbereiche beobachten
plot(logmod, which = 4, id.n =3)

  # varianzinflationsfaktoren zur diagnose von multikollinearität
      #\> 10 variable entfernen
      #\> 4 vorsicht geboten
      #\< 4 alles in ordnung
car::vif(logmod)

  # heteroskedastizität robuste standardfehler
lmtest::coeftest(logmod, vcov= vcovHC(logmod, type ="HC1"))



#_####

# build linear model tidymodels #### 

# read data

urchins <-
  # Data were assembled for a tutorial 
  # at https://www.flutterbys.com.au/stats/tut/tut7.5a.html
  read_csv("https://tidymodels.org/start/models/urchins.csv") %>% 
  # Change the names to be a little more verbose
  setNames(c("food_regime", "initial_volume", "width")) %>% 
  # Factors are very helpful for modeling, so we convert one column
  mutate(food_regime = factor(food_regime, levels = c("Initial", "Low", "High")))


# plot data
ggplot(urchins,
       aes(x = initial_volume, 
           y = width, 
           group = food_regime, 
           col = food_regime)) + 
  geom_point() + 
  geom_smooth(method = lm, se = FALSE) +
  scale_color_viridis_d(option = "plasma", end = .7)


# create engine
lm_mod <- 
  parsnip::linear_reg() %>% 
  parsnip::set_engine("lm")

# 
lm_fit <- 
  lm_mod %>% 
  parsnip::fit(width ~ initial_volume * food_regime, data = urchins)

parsnip::tidy(lm_fit)



# dot-and-whisker plot of our regressio
parsnip::tidy(lm_fit) %>% 
  dotwhisker::dwplot(dot_args = list(size = 2, color = "black"),
         whisker_args = list(color = "black"),
         vline = geom_vline(xintercept = 0, colour = "grey50", linetype = 2))



# use model to predict

new_points <- expand.grid(initial_volume = 20, 
                          food_regime = c("Initial", "Low", "High"))

mean_pred <- predict(lm_fit, new_data = new_points)


conf_int_pred <- predict(lm_fit, 
                         new_data = new_points, 
                         type = "conf_int")



# Now combine: 
plot_data <- 
  new_points %>% 
  bind_cols(mean_pred) %>% 
  bind_cols(conf_int_pred)

# and plot:
ggplot(plot_data, aes(x = food_regime)) + 
  geom_point(aes(y = .pred)) + 
  geom_errorbar(aes(ymin = .pred_lower, 
                    ymax = .pred_upper),
                width = .2) + 
  labs(y = "urchin size")


# logistic regression model tidymodels ####
# https://www.tidymodels.org/start/case-study/
  
# import librarys
library(tidyverse)
library(tidymodels)

# import data 
data("mlc_churn")



## explorative data analysis ###

# view data
glimpse(mlc_churn)

# count churns
mlc_churn %>% 
  count(churn) %>% 
  mutate(prop = n/sum(n))

# visualization churns
ggplot(data=mlc_churn, aes(x=churn)) +
  geom_histogram(stat =  "count", fill=c("darkred", "steelblue")) +
  labs(title = "Mobile Contract Churn", c="Churn", y="Frequency")




#### spliting data & resampling ###
splits <- initial_split(mlc_churn, strata = churn)


churn_training <- training(splits)
churn_test  <- testing(splits)






#### training set proportions by churns ###
churn_training %>% 
  count(churn) %>% 
  mutate(prop = n/sum(n))


# test set proportions by churns
churn_test  %>% 
  count(churn) %>% 
  mutate(prop = n/sum(n))


# split training into training & validation
val_set <- validation_split(churn_training, strata = churn, prop = 0.80)








#### 1st model ##


# logistic regression model
lr_mod <- 
  logistic_reg(penalty = tune(), mixture = 1) %>%
  set_engine("glmnet")

# create recipe
lr_recipe <- 
  recipe(churn ~ ., data = churn_training) %>% 
  step_dummy(all_nominal(), -all_outcomes()) %>% 
  step_zv(all_predictors()) %>% 
  step_normalize(all_predictors())


# create workflow
lr_workflow <- 
  workflow() %>% 
  add_model(lr_mod) %>% 
  add_recipe(lr_recipe)

# tune model 
lr_reg_grid <- tibble(penalty = 10^seq(-4, -1, length.out = 30))

lr_reg_grid %>% top_n(-5) # lowest penalty values
lr_reg_grid %>% top_n(5)  # highest penalty values

# train model
lr_res <- 
  lr_workflow %>% 
  tune_grid(val_set,
            grid = lr_reg_grid,
            control = control_grid(save_pred = TRUE),
            metrics = metric_set(roc_auc))

# roc curve
lr_res %>% 
  collect_metrics() %>% 
  ggplot(aes(x = penalty, y = mean)) + 
  geom_point() + 
  geom_line() + 
  ylab("Area under the ROC Curve") +
  scale_x_log10(labels = scales::label_number())


# best model
lr_res %>% 
  show_best("roc_auc", n = 15) %>% 
  arrange(penalty)


lr_best <- 
  lr_res %>% 
  collect_metrics() %>% 
  arrange(penalty) %>% 
  slice(24)
lr_best

lr_auc <- 
  lr_res %>% 
  collect_predictions(parameters = lr_best) %>% 
  roc_curve(churn, .pred_yes) %>% 
  mutate(model = "Logistic Regression")

autoplot(lr_auc)

#### random forest model ###

rf_mod <- 
  rand_forest(mtry = tune(), min_n = tune(), trees = 1000) %>% 
  set_engine("ranger", num.threads = 4) %>% 
  set_mode("classification")

#reciept & workflow
rf_recipe <- 
  recipe(churn ~ ., data = churn_training)

rf_workflow <- 
  workflow() %>% 
  add_model(rf_mod) %>% 
  add_recipe(rf_recipe)

rf_res <- 
  rf_workflow %>% 
  tune_grid(val_set,
            grid = 25,
            control = control_grid(save_pred = TRUE),
            metrics = metric_set(roc_auc))


rf_res %>% 
  show_best(metric = "roc_auc")

autoplot(rf_res)


rf_best <- 
  rf_res %>% 
  select_best(metric = "roc_auc")
rf_best

rf_res %>% 
  collect_predictions()


rf_auc <- 
  rf_res %>% 
  collect_predictions(parameters = rf_best) %>% 
  roc_curve(churn, .pred_yes) %>% 
  mutate(model = "Random Forest")


bind_rows(rf_auc, lr_auc) %>% 
  ggplot(aes(x = 1 - specificity, y = sensitivity, col = model)) + 
  geom_path(lwd = 1.5, alpha = 0.8) +
  geom_abline(lty = 3) + 
  coord_equal() + 
  scale_color_viridis_d(option = "plasma", end = .6)

#### last model ###

# the last model
last_rf_mod <- 
  rand_forest(mtry = 8, min_n = 7, trees = 1000) %>% 
  set_engine("ranger", num.threads = 4, importance = "impurity") %>% 
  set_mode("classification")

# the last workflow
last_rf_workflow <- 
  rf_workflow %>% 
  update_model(last_rf_mod)

# the last fit
last_rf_fit <- 
  last_rf_workflow %>% 
  last_fit(splits)


last_rf_fit %>% 
  collect_metrics()


last_rf_fit %>% 
  pluck(".workflow", 1) %>%   
  extract_fit_parsnip() %>% 
  vip::vip(num_features = 20)


last_rf_fit %>% 
  collect_predictions() %>% 
  roc_curve(churn, .pred_yes) %>% 
  autoplot()

# arima ####
# https://cran.rstudio.com/web/packages/sweep/vignettes/SW01_Forecasting_Time_Series_Groups.html

library(tidyverse)
library(tidyquant)
library(timetk)
library(sweep)
library(forecast)

gmv_df %>% 
  mutate(Month = month(date, label = TRUE),
         Year = as.factor(Year)) %>% 
  group_by(Year, Month) %>%
  summarise(total_qty = sum(GMV)) %>% 
  #plot
  ggplot(aes(x = Month, y = total_qty, group = Year)) +
  geom_area(aes(fill = Year, position = "stack")) +
  labs(title = "Quantity Sold: Month Plot", x = "", y = "Sales") +
  scale_y_continuous(labels=scales::dollar_format(suffix="€",prefix="")) +
  tidyquant::theme_tq()





#Forecasting Workflow

#The forecasting workflow involves a few basic steps:

#Step 1: Coerce to a ts object class.
#Step 2: Apply a model (or set of models)
#Step 3: Forecast the models (similar to predict)
#Step 4: Tidy the forecast



nested_data <- gmv_df %>%
  #mutate(order_month = as_date(zoo::as.yearmon(date))) %>%
  group_by(Category, date) %>%
  summarise(total_gmv = sum(GMV)) %>% 
  nest() %>% 
  mutate(data_ts = map(.x       = data, 
                       .f       = timetk::tk_ts, 
                       select   = -date, 
                       start    = 2020,
                       freq     = 12)) %>% 
  mutate(fit_ets = map(data_ts, forecast::ets))

nested_data %>% 
  mutate(tidy = map(fit_ets, sweep::sw_tidy)) %>%
  unnest(tidy) %>%
  spread(key = Category, value = estimate)

nested_data %>% 
  mutate(glance = map(fit_ets, sweep::sw_glance)) %>%
  unnest(glance)

nested_data %>% 
  mutate(augment = map(fit_ets, sweep::sw_augment, timetk_idx = TRUE, rename_index = "date")) %>%
  unnest(augment) %>% 
  ggplot(aes(x = date, y = .resid, group = Category)) +
  geom_hline(yintercept = 0, color = "grey40") +
  geom_line(color = palette_light()[[2]]) +
  geom_smooth(method = "loess") +
  labs(title = "Solds By Category") +
  tidyquant::theme_tq() +
  facet_wrap(~ Category, scale = "free_y", ncol = 3) +
  scale_x_date(date_labels = "%Y")

nested_data %>%
  mutate(decomp = map(fit_ets, sweep::sw_tidy_decomp, timetk_idx = TRUE, rename_index = "date")) %>%
  unnest(decomp)

nested_data %>%
  mutate(fcast_ets = map(fit_ets, forecast, h = 3)) %>% 
  mutate(sweep = map(fcast_ets, sweep::sw_sweep, fitted = FALSE, timetk_idx = TRUE)) %>%
  unnest(sweep) %>% 
  ggplot(aes(x = index, y = total_gmv, color = key, group = Category)) +
  geom_ribbon(aes(ymin = lo.95, ymax = hi.95), 
              fill = "#D5DBFF", color = NA, size = 0) +
  geom_ribbon(aes(ymin = lo.80, ymax = hi.80, fill = key), 
              fill = "#596DD5", color = NA, size = 0, alpha = 0.8) +
  geom_line() +
  labs(title = "Sold By Category",
       subtitle = "ETS Model Forecasts",
       x = "", y = "Units") +
  scale_x_date(date_breaks = "1 year", date_labels = "%Y") +
  scale_color_tq() +
  scale_fill_tq() +
  facet_wrap(~ Category, scales = "free_y", ncol = 3) +
  theme_tq() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

#_####
#_####
#_####
#_####
#_####
#_####
#_####
#_####
#_####
#_####
#_####
#_####
#_####
#_####
#_####
#_####
#_####
#_####
#_####
#_####
#_########playground###################################################################################################################################################################################################################################################################################################################

###################### calculate bachelor grade ####

tibble(
  "subject" = c(
    as.character("Soziologie"),
    as.character("Soziologie"),
    as.character("Soziologie"),
    as.character("Soziologie"),
    as.character("Soziologie"),
    as.character("Soziologie"),
    as.character("Soziologie"),
    as.character("Soziologie"),
    as.character("Soziologie in der Praxis"),
    as.character("Mathe"),
    as.character("Mathe"),
    as.character("Mathe"),
    as.character("Mathe"),
    as.character("Mathe"),
    as.character("Schlüsselkompetenzen")
  ),
  "modul" = c(
    as.character("Modul 1"),
    as.character("Modul 2"),
    as.character("Modul 3"),
    as.character("Modul 4"),
    as.character("Modul 5"),
    as.character("Modul 6"),
    as.character("Modul 8"),
    as.character("Bachelor-Arbeit"),
    as.character("Soziologie in der Praxis"),
    as.character("Einführung Analysis 1"),
    as.character("Elementare Lineare Algebra"),
    as.character("Grundlagen Mathematik"),
    as.character("Einführung in die Stochastik"),
    as.character("Angewandte Statistik"),
    as.character("Schlüsselkompetenzen")
  ),
  "weight_main" = c(
    as.numeric(0.05), #modul1
    as.numeric(0.10), #modul2
    as.numeric(0.05), #modul3
    as.numeric(0.05), #modul4
    as.numeric(0.20), #modul5
    as.numeric(0.15), #modul6
    as.numeric(0.20), #modul8
    as.numeric(0.20), #bachelor
    as.numeric(1), #soziologie in der praxis
    as.numeric(1), #ana
    as.numeric(1), #ela
    as.numeric(1), #gdm
    as.numeric(1), #stochastik
    as.numeric(1), #angewandte statistik
    as.numeric(1) #schlüsselkompetenzen 
  ),
  "weight_grade" = c(
    as.numeric(0.70),
    as.numeric(0.70),
    as.numeric(0.70),
    as.numeric(0.70),
    as.numeric(0.70),
    as.numeric(0.70),
    as.numeric(0.70),
    as.numeric(0.70),
    as.numeric(0.03),
    as.numeric(0.25),
    as.numeric(0.25),
    as.numeric(0.25),
    as.numeric(0.25),
    as.numeric(0.25),
    as.numeric(0.02)
  ),
  "grade" = c(
    as.numeric(2), #modul1
    as.numeric(NA), #modul2
    as.numeric(4), #modul3
    as.numeric(3), #modul4
    as.numeric(NA), #modul5
    as.numeric(NA), #modul6
    as.numeric(NA), #modul8
    as.numeric(NA), #bachelor
    as.numeric(NA), #soziologie in der praxis
    as.numeric(NA), #ana
    as.numeric(NA), #ela
    as.numeric(NA), #gdm
    as.numeric(NA), #stochastik
    as.numeric(NA), #angewandte statistik
    as.numeric(1) #schlüsselkompetenzen
  )
) %>% 
  group_by(subject, weight_grade) %>% 
  summarize(main_grade = weighted.mean(grade, weight_main, na.rm=T)) %>% 
  ungroup() %>% 
  summarize("bachelor grade" = weighted.mean(main_grade, weight_grade, na.rm=T))

# projekte #####
# einstellmess # sozialwissenschaftliche einstellungsmessungen #sowi ####

#datensatz einlesen###


all <- haven::read_sav("/Users/alexchaichan/Documents/Uni/Bachelor - Uni Kassel/1_Soziologie/Modul 8 Vertiefung -  Soziale Praktiken und kollektive Prozesse/Sozialwissenschaftlichen Einstellungsmessung/einstellmess/r_einstellmess/ZA3762_v2-0-0.sav") %>% 
  sjlabelled::remove_all_labels() %>% 
  janitor::clean_names() 







#deskriptive statistik###

# nas 

all %>%
  select(v161:v168) %>% 
  summarise_all(funs(sum(is.na(.))))

# boxplot

all %>% 
  ggplot(aes(x="", y=v161)) +
  geom_boxplot() +
  viridis::scale_fill_viridis(discrete = TRUE, alpha=0.6) +
  geom_jitter(color="black", size=0.4, alpha=0.9) +
  hrbrthemes::theme_ipsum() +
  theme(
    legend.position="none",
    plot.title = element_text(size=11)
  ) +
  ggtitle("ERFOLGSBED.,BRD: KLASSENZUGEHOERIGKEIT") +
  xlab("BEZEICHNUNG_X_ACHSE") +
  ylab("BEZEICHNUNG_y_ACHSE")



summarytools::freq(df$V161)
psych::describe(as.numeric(df$V161))
hist(as.numeric(df$V161))



##analyse###

#reliabilität bestimmen - cronbachs aplha berechnen###




all %>%
  select(v161:v168) %>%
  mutate_at(vars(v161,v162, v164, v165), ~as.numeric(recode(., "1" = 4, "2" =3, "3"= 2, "4" = 1))) %>% 
  sjPlot::sjt.itemanalysis() 





#skalen bilden###



all %>% 
  select(v161:v168) %>%
  mutate_at(vars(v161,v162, v164, v165), ~as.numeric(recode(., "1" = 4, "2" =3, "3"= 2, "4" = 1))) %>% 
  mutate(sozun_pos_mean = rowMeans(select(., v163, v166, v167, v168), na.rm = T),
         sozun_neg_mean = rowMeans(select(., v161, v162, v164, v165), na.rm = T), #mittelwert
         sozun_pos_sum = rowSums(select(., v163, v166, v167, v168), na.rm = T), #summensoce
         sozun_neg_sum = rowSums(select(., v161, v162, v164, v165), na.rm = T))



#explorative faktorenanalyse###
#korrelationsmatrix###


all %>% 
  select(v161:v168) %>%
  cor(use = "complete.obs", method = "spearman") %>% 
  corrplot::corrplot.mixed(upper ="pie", lower="number", lower.col ="black", sig.level = 0.95)



##vorbereitung  ##faktorenanalyse###
faktor <- all %>% 
  select(v161:v168) %>% 
  filter(complete.cases(.)) 


psych::cortest.bartlett(faktor)


#kmo und msa - alle werte müssen über 0.5 sein (wenn nicht -> ausschluss, außer es ist sehr wichtig)
psych::KMO(faktor)

#anzahl faktoren mit map test
#mit varimax-rotation und ml-faktorenanalyse


psych::nfactors(faktor, rotate = "varimax", fm="mle")
#velicer map -> gibt an wie viele faktoren da sein könnten



#anzahl faktoren mit parallel analyse



ev <- eigen(cor(faktor))
ap <- nFactors::parallel(subject=nrow(faktor), var=ncol(faktor), rep=100, cent=0.05)
nS <- nFactors::nScree(x=ev$values, aparallel = ap$eigen$qevpea)

nFactors::plotnScree(nS)




pca <- psych::principal(faktor, 2, rotate = "varimax")
plot(pca)


#faktorladungen ausgeben

pca$loadings

#kommunalitäten

pca$communality


#plot

plot(pca$values, type ="b") %>% 
  abline(h=1)





all %>% 
  select(v161, v162, v164, v165) %>% 
  mutate_at(vars(v161,v162, v164, v165), ~as.numeric(recode(., "1" = 4, "2" =3, "3"= 2, "4" = 1))) %>% 
  sjPlot::sjt.itemanalysis() 

all %>% 
  select(v163, v166, v167, v168) %>% 
  sjPlot::sjt.itemanalysis() 



all %>% 
  select(v161:v168) %>%
  mutate_at(vars(v161,v162, v164, v165), ~as.numeric(recode(., "1" = 4, "2" =3, "3"= 2, "4" = 1))) %>% 
  mutate(faktor2 = rowSums(select(., v163, v166, v167, v168), na.rm = T), #summensoce
         faktor1 = rowSums(select(., v161, v162, v164, v165), na.rm = T))





##############_v16x ###
###### netto einkommen ###

all %>% 
  select(v476) %>% 
  mutate(gehaltsklasse = ifelse(v476 >= 0 | v476 <= 8, "unter_1000",
                                ifelse(v476 >=9 | v476 <= 14, "unter_2000", "ueber_2000")))


# plot

all %>% 
  mutate_at(vars(v161,v162, v164, v165), ~as.numeric(recode(., "1" = 4, "2" =3, "3"= 2, "4" = 1))) %>%
  mutate(gehaltsklasse = ifelse(v476 >= 0 & v476 <= 8, 1,
                                ifelse(v476 >=9 & v476 <= 14, 2, 
                                       ifelse(v476 >= 15 & v476 <= 17, 3, 4)))) %>% 
  select(v163, gehaltsklasse) %>% 
  filter(!(is.na(v163)) & !(is.na(gehaltsklasse))) %>% 
  ggplot(aes(x=gehaltsklasse, y=v163, group=gehaltsklasse)) +
  geom_boxplot() +
  viridis::scale_fill_viridis(discrete = TRUE, alpha=0.6) +
  geom_jitter(color="black", size=0.4, alpha=0.9) +
  hrbrthemes::theme_ipsum() +
  theme(legend.position="none", plot.title = element_text(size=11)) +
  ggtitle("TITEL") +
  xlab("BEZEICHNUNG_X_ACHSE") +
  ylab("BEZEICHNUNG_y_ACHSE")




##### anova


all %>% 
  select(v161, v476) %>% 
  filter(!(is.na(v161)) & !(is.na(v476))) %>% 
  with(., aov(v161 ~ v476)) %>% 
  summary()  #p-wert <0.05 = signifikanter unterschied 
#plot(1,2,3,4)


all %>% 
  select(v161, v476) %>% 
  filter(!(is.na(v161)) & !(is.na(v476))) %>% 
  with(., pairwise.t.test(v161, v476, p.adjust="bonferroni"))


###levene-test auf homogenität

#desskriptiver test die sd muss sich stark unterscheiden # sd ist auch ein deskriptiver test auf gruppenunterschiede

all %>% 
  select(v161, v476) %>% 
  filter(!(is.na(v161)) & !(is.na(v476))) %>% 
  with(., psych::describeBy(v161, v476))


#analytischer test

all %>% 
  select(v161, v476) %>% 
  filter(!(is.na(v161)) & !(is.na(v476))) %>% 
  with(., car::leveneTest(v161, as.factor(v476))) #p-wert <0.05 = H0 (Varianzhomogenität) verwerfen!






####effektstärke berechnen
#wie groß ist der effekt, den wir mit der anova festgestellt haben?

all %>% 
  select(v161, v476) %>% 
  filter(!(is.na(v161)) & !(is.na(v476))) %>% 
  aov(v161 ~ v476, data = .) %>% 
  DescTools::EtaSq()

#eta quadrat (eta.sq) wert interessiert uns
#eta quadrat in einen f-wert umrechnen
sqrt(eta.sq/(1-eta.sq)) #werte für "eta.sq" einsetzen 

#ergebnis einordnen
#### A Power Primer (1992)
###Jacob Cohen
##s. 157 / punkt 7. (one-way analysis of variance)
#effektstärken

#   small     #medium     #large
#   0.10       0.25        0.40



###### schulabschluss ###


all %>% 
  mutate(abitur = ifelse(as.numeric(v60)==4 | as.numeric(v60)==5, "ja", "nein")) %>% 
  select(v161, abitur) %>% 
  filter(!(is.na(v161)) & !(is.na(abitur))) %>% 
  ggplot(aes(x=abitur, y=v161, fill=abitur)) +
  geom_boxplot() +
  viridis::scale_fill_viridis(discrete = TRUE, alpha=0.6) +
  geom_jitter(color="black", size=0.4, alpha=0.9) +
  hrbrthemes::theme_ipsum() +
  theme(
    legend.position="none",
    plot.title = element_text(size=11)
  ) +
  ggtitle("TITEL") +
  xlab("BEZEICHNUNG_X_ACHSE") +
  ylab("BEZEICHNUNG_y_ACHSE")


##### t-test


#beim t-test muss du dir ein neues subset erstellen und die gruppen einzeln auf normalverteilung prüfen 

#p <0.05 = Normalverteilt #AV ist die abjängige variable die getestet wird durch die gruppen


all %>% 
  mutate(abitur = ifelse(as.numeric(v60)==4 | as.numeric(v60)==5, "ja", "nein")) %>% 
  select(v161, abitur) %>% 
  filter(!(is.na(v161)) & !(is.na(abitur)) & abitur=="ja") %>% 
  with(., shapiro.test(v161))

all %>% 
  mutate(abitur = ifelse(as.numeric(v60)==4 | as.numeric(v60)==5, "ja", "nein")) %>% 
  select(v161, abitur) %>% 
  filter(!(is.na(v161)) & !(is.na(abitur)) & abitur=="nein") %>% 
  with(., shapiro.test(v161))


#visuelle überprüfung auf normalverteilung

all %>% 
  mutate(abitur = ifelse(as.numeric(v60)==4 | as.numeric(v60)==5, "ja", "nein")) %>% 
  select(v161, abitur) %>% 
  filter(!(is.na(v161)) & !(is.na(abitur)) & abitur=="ja") %>% 
  with(., hist(v161))

all %>% 
  mutate(abitur = ifelse(as.numeric(v60)==4 | as.numeric(v60)==5, "ja", "nein")) %>% 
  select(v161, abitur) %>% 
  filter(!(is.na(v161)) & !(is.na(abitur)) & abitur=="nein") %>% 
  with(., hist(v161))


#varianzen prüfen (gleiche / ungleiche varianzen) 
#bei ungleichen varianze (welch-test) #heißt nichts anders als var.equal = F
all %>% 
  mutate(abitur = ifelse(as.numeric(v60)==4 | as.numeric(v60)==5, "ja", "nein")) %>% 
  select(v161, abitur) %>% 
  with(., psych::describeBy(v161, abitur))

#gleiche varinzen (sd) var.equal = T, / #ungleiche varianzen (sd) var.equal = F, #zweiseitger test
#unabhängige sp
all %>% 
  mutate(abitur = ifelse(as.numeric(v60)==4 | as.numeric(v60)==5, "ja", "nein")) %>% 
  select(v161, abitur) %>%
  with(., t.test(v161 ~ abitur, var.equal = T, alternative = "two.sided"))


#wie groß ist der unterschied vom t-test
all %>% 
  mutate(abitur = ifelse(as.numeric(v60)==4 | as.numeric(v60)==5, "ja", "nein")) %>% 
  select(v161, abitur) %>%
  with(., lsr::cohensD(v161 ~ abitur))

#ergebnis einordnen
#### A Power Primer (1992)
###Jacob Cohen
##s. 157 / punkt 1. (cohens D)
#effektstärken

#   small     #medium     #large
#   0.20       0.50        0.80


###### gesundheitszustand###


all %>% 
  mutate_at(vars(v161,v162, v164, v165), ~as.numeric(recode(., "1" = 4, "2" =3, "3"= 2, "4" = 1))) %>% 
  select(v161, v251) %>% 
  filter(!(is.na(v161)) & !(is.na(v251))) %>% 
  ggplot(aes(x=v251, y=v161, group=as.factor(v251))) +
  geom_boxplot() +
  viridis::scale_fill_viridis(discrete = TRUE, alpha=0.6) +
  geom_jitter(color="black", size=0.4, alpha=0.9) +
  hrbrthemes::theme_ipsum() +
  theme(legend.position="none", plot.title = element_text(size=11)) +
  ggtitle("TITEL") +
  xlab("BEZEICHNUNG_X_ACHSE") +
  ylab("BEZEICHNUNG_y_ACHSE")


########  anova

all %>% 
  select(v161, v251) %>% 
  filter(!(is.na(v161)) & !(is.na(v251))) %>%
  with(., aov(v161 ~ v251)) %>% 
  summary()  #p-wert <0.05 = signifikanter unterschied 
#plot(1,2,3,4)


all %>% 
  select(v161, v251) %>% 
  filter(!(is.na(v161)) & !(is.na(v251))) %>% 
  with(., pairwise.t.test(v161, v251, p.adjust="bonferroni"))





###levene-test auf homogenität

#desskriptiver test die sd muss sich stark unterscheiden # sd ist auch ein deskriptiver test auf gruppenunterschiede

all %>% 
  select(v161, v251) %>% 
  filter(!(is.na(v161)) & !(is.na(v251))) %>% 
  with(., psych::describeBy(v161, v251))


#analytischer test

all %>% 
  select(v161, v251) %>% 
  filter(!(is.na(v161)) & !(is.na(v251))) %>% 
  with(., car::leveneTest(v161, as.factor(v251))) #p-wert <0.05 = H0 (Varianzhomogenität) verwerfen!

### NICHT SIGNIFIKANT ###



####effektstärke berechnen
#wie groß ist der effekt, den wir mit der anova festgestellt haben?

all %>% 
  select(v161, v251) %>% 
  filter(!(is.na(v161)) & !(is.na(v251))) %>% 
  aov(v161 ~ v251, data = .) %>% 
  DescTools::EtaSq()

#eta quadrat (eta.sq) wert interessiert uns
#eta quadrat in einen f-wert umrechnen
sqrt(eta.sq/(1-eta.sq)) #werte für "eta.sq" einsetzen 

#ergebnis einordnen
#### A Power Primer (1992)
###Jacob Cohen
##s. 157 / punkt 7. (one-way analysis of variance)
#effektstärken

#   small     #medium     #large
#   0.10       0.25        0.40
















# weiss ####
# datensatz einlesen ###

cyc <- readr::read_csv("/sensor_daten_cycle.csv")

cyc$id <- 1:nrow(cyc)

palette <- (c("#ccb9bc","#cb9ca1","#ba6b6c", "#af4448", "#b61827", "#002171", "#003c8f", "#004ba0", "#005cb2", "#0069c0"))

# eda ###

cyc %>% 
  dplyr::select(starts_with("Sensor_1")) %>%
  psych::describe() %>% 
  round(2) %>% 
  knitr::kable(caption="Sensor 1") %>% 
  kableExtra::kable_material(c("striped"))


# einteilung in 70 k cyclen ###

cyc <- cyc %>% 
  mutate(interval = ifelse(cycle >= 0 & cycle <= 70000, "70k",
                           ifelse(cycle >= 70001 & cycle <= 140000, "140k",
                                  ifelse(cycle >= 140001 & cycle <= 210000, "210k",
                                         ifelse(cycle >= 210001 & cycle <= 280000, "280k",
                                                ifelse(cycle >= 280001 & cycle <= 350000, "350k",
                                                       ifelse(cycle >= 350001 & cycle <= 420000, "420k",
                                                              ifelse(cycle >= 420001 & cycle <= 490000, "490k",
                                                                     ifelse(cycle >= 490001 & cycle <= 560000, "560k",
                                                                            ifelse(cycle >= 560001 & cycle <= 630000, "630k",
                                                                                   ifelse(cycle >= 630001 & cycle <= 800000, "700k", "700k")))))))))))
cyc$interval  <- factor(cyc$interval, ordered = TRUE, levels = c("70k", "140k", "210k", "280k", "350k", "420k", "490k", "560k", "630k", "700k"))









# verteilung nach 70k cyclen ###

measure <- cyc$sensor_1_peak_high_frequency

plot <- ggplot(cyc, aes(x=measure)) +
  geom_density(aes(group = interval, color = interval), position="identity") +
  scale_color_manual(values = palette)
ggplotly(plot)


# verteilug vor und nach cp ###

measure <- cyc$sensor_1_peak_high_frequency


cp <- cyc %>%
  timetk::plot_stl_diagnostics(
    meta_asc_timestamp, measure,
    .feature_set = c("trend"),
    .frequency   = "auto",
    .trend       = "auto",
    .message     = FALSE,
    .interactive = FALSE)
cp <- changepoint::cpt.mean(cp$data$.group_value, method = "AMOC")
cp <- cyc %>% 
  filter(id == cp@cpts[[1]]) %>% 
  select(cycle) %>% 
  as.double()
cyc %>% 
  mutate(changepoint = ifelse(cycle <= cp, "cp1", "cp2")) %>% 
  ggplot(aes(x=measure)) +
  geom_density(aes(group = changepoint, color = changepoint), position="identity")









# zeitreihen plots ###

measure <- cyc$sensor_1_peak_high_frequency
measure2 <- cyc$sensor_2_peak_high_frequency
measure3 <- cyc$sensor_3_peak_high_frequency
measure4 <- cyc$sensor_4_peak_high_frequency


cyc %>%
  plot_ly(x = ~cycle) %>% 
  add_trace(y = ~measure, type = "scatter", mode = "line", name = 'Sensor 1') %>% 
  add_trace(y = ~measure2, type = "scatter", mode = "line", name = 'Sensor 2') %>% 
  add_trace(y = ~measure3, type = "scatter", mode = "line", name = 'Sensor 3') %>%
  add_trace(y = ~measure4, type = "scatter", mode = "line", name = 'Sensor 4') %>% 
  layout(
    yaxis = list(title = "peak_high_frequency")
  )

# stl - trend ###



# cyc %>% 
#   select(meta_asc_timestamp, sensor_1_peak_high_frequency) %>% 
#   tsibble::as_tsibble() %>% 
#   column_to_rownames(var = "meta_asc_timestamp") %>%
#   unlist() %>% 
#   ts(data = as.vector(zoo::coredata(.)),
#                frequency = 360) %>% 
#   decompose() %>%
#   plot() -> trend

measure <- cyc$sensor_1_peak_high_frequency


cyc %>%
  timetk::plot_stl_diagnostics(
    meta_asc_timestamp, measure,
    .feature_set = c("trend"),
    .frequency   = "auto",
    .trend       = "auto",
    .message     = FALSE,
    .interactive = T)





# changepoints ###

library(changepoint)

measure <- cyc$sensor_1_peak_high_frequency



trend <- cyc %>%
  timetk::plot_stl_diagnostics(
    meta_asc_timestamp, measure,
    .feature_set = c("trend"),
    .frequency   = "auto",
    .trend       = "auto",
    .message     = FALSE,
    .interactive = FALSE)
trend <- changepoint::cpt.mean(trend$data$.group_value, method = "AMOC")
cyc %>% 
  filter(id == trend@cpts[[1]]) %>%
  select(measure, meta_asc_timestamp, cycle)  #funkioniert nicht wenn man "measure" einsetzt




# histo 2 intervalle : vor und nach changepoint
measure <- cyc$sensor_1_peak_high_frequency

cp <- cyc %>%
  timetk::plot_stl_diagnostics(
    meta_asc_timestamp, measure,
    .feature_set = c("trend"),
    .frequency   = "auto",
    .trend       = "auto",
    .message     = FALSE,
    .interactive = FALSE)
cp <- changepoint::cpt.mean(cp$data$.group_value, method = "AMOC")
cp <- cyc %>% 
  filter(id == cp@cpts[[1]]) %>% 
  select(cycle) %>% 
  as.double()
cyc %>% 
  mutate(changepoint = ifelse(cycle <= cp, "vor cp", "nach cp")) %>% 
  ggplot(aes(x=measure)) +
  geom_histogram(aes(group = changepoint, fill = changepoint), position="identity", alpha=0.7)







# tabelle ###

tab <- data.frame("", "", "", "", "") 

tab <- rbind(tab, list("sensor_1_peak_high_frequency", "2020-10-14 14:16:36",  415168, "schmalgipflig", "schmalgipflig")) 
tab <- rbind(tab, list("sensor_2_peak_high_frequency", "2020-09-27 23:12:44",  344812, "schmalgipflig", "schmalgipflig")) 
tab <- rbind(tab, list("sensor_3_peak_high_frequency", "2020-10-14 14:07:36",  415086, "schmalgipflig", "schmalgipflig")) 
tab <- rbind(tab, list("sensor_4_peak_high_frequency", "2020-10-14 22:02:44",  419432, "schmalgipflig", "schmalgipflig")) 
tab <- rbind(tab, list("sensor_1_peak_raw", "2020-09-27 17:59:47",  341942, "rechtsschief", "linksschief")) 
tab <- rbind(tab, list("sensor_2_peak_raw", "2020-09-27 22:54:22",  344648, "schmalgipflig", "breitgipflig")) 
tab <- rbind(tab, list("sensor_3_peak_raw", "2020-10-14 18:54:16",  417710, "bimodal", "bimodal")) 
tab <- rbind(tab, list("sensor_4_peak_raw", "2020-09-27 09:40:41",  337350, "bimodal", "bimodal")) 
tab <- rbind(tab, list("sensor_1_rms_acc_envelope", "2020-10-14 10:03:39",  412872, "bimodal", "bimodal")) 
tab <- rbind(tab, list("sensor_2_rms_acc_envelope", "2020-10-14 23:23:31", 420170, "bimodal", "bimodal"))
tab <- rbind(tab, list("sensor_3_rms_acc_envelope", "2020-09-27 21:52:32", 344074, "schmalgipflig", "bimodal"))  
tab <- rbind(tab, list("sensor_4_rms_acc_envelope", "2020-09-27 22:36:32", 344484, "bimodal", "bimodal"))  
tab <- rbind(tab, list("sensor_1_rms_acc_raw", "2020-10-14 10:11:33", 412954, "rechtsschief", "bimodal"))
tab <- rbind(tab, list("sensor_2_rms_acc_raw", "2020-10-14 23:24:06", 420170, "bimodal", "bimodal"))  
tab <- rbind(tab, list("sensor_3_rms_acc_raw", "2020-09-27 22:09:49", 344238, "schmalgipflig", "bimodal"))  
tab <- rbind(tab, list("sensor_4_rms_acc_raw", "2020-09-27 22:37:05", 344484, "bimodal", "bimodal"))  
tab <- rbind(tab, list("sensor_1_rms_high_frequency", "2020-09-27 16:57:53", 341368, "rechtsschief", "rechtsschief"))  
tab <- rbind(tab, list("sensor_2_rms_high_frequency", "2020-09-28 06:47:35", 348912, "rechtsschief", "rechtsschief"))  
tab <- rbind(tab, list("sensor_3_rms_high_frequency", "2020-09-28 04:15:25", 347600, "rechtsschief", "rechtsschief"))  
tab <- rbind(tab, list("sensor_4_rms_high_frequency", "2020-10-14 13:40:07", 414840, "rechtsschief", "rechtsschief"))  
tab <- rbind(tab, list("sensor_1_kurtosis_raw", "2020-09-24 16:36:22", 301434, "linksschief", "schmalgipflig"))  
tab <- rbind(tab, list("sensor_2_kurtosis_raw", "2020-11-28 11:11:09", 719387, "schmalgipflig", NA))  
tab <- rbind(tab, list("sensor_3_kurtosis_raw", "2020-11-28 11:11:09", 719387, "schmalgipflig", NA))  
tab <- rbind(tab, list("sensor_4_kurtosis_raw", "2020-09-26 00:42:29", 319146, "schmalgipflig", "schmalgipflig"))  
tab <- rbind(tab, list("sensor_1_skewness_raw", "2020-11-28 11:11:09", 719387, "linksschief", NA))  
tab <- rbind(tab, list("sensor_2_skewness_raw", "2020-11-28 11:11:09", 719387, "schmalgipflig", NA))  
tab <- rbind(tab, list("sensor_3_skewness_raw", "2020-11-28 11:11:09", 719387, "schmalgipflig", NA))  
tab <- rbind(tab, list("sensor_4_skewness_raw", "2020-11-28 11:11:09", 719387, "schmalgipflig", NA))  
tab <- rbind(tab, list("sensor_1_rms_acc_raw_zwischenwelle", "2020-11-20 14:42:31", 665104, "schmalgipflig", "rechtsschief"))  
tab <- rbind(tab, list("sensor_2_rms_acc_raw_zwischenwelle", "2020-10-15 05:33:02", 423532, "schmalgipflig", "rechtsschief"))  
tab <- rbind(tab, list("sensor_1_peak_acc_raw_zwischenwelle_2", "2020-10-15 04:56:58", 423204, "schmalgipflig", "rechtsschief")) 
tab <- rbind(tab, list("sensor_2_peak_acc_raw_zwischenwelle_2", "2020-10-15 04:02:16", 422712, "schmalgipflig", "rechtsschief")) 
tab <- rbind(tab, list("sensor_1_rms_acc_raw_zwischenwelle_2", "2020-10-15 06:00:05", 423778, "schmalgipflig", "rechtsschief"))  
tab <- rbind(tab, list("sensor_2_rms_acc_raw_zwischenwelle_2", "2020-10-15 03:35:49", 422466, "schmalgipflig", "rechtsschief"))  
tab <- rbind(tab, list("sensor_1_peak_acc_raw_zwischenwelle", "2020-11-21 14:00:33", 677814, "schmalgipflig", "rechtsschief"))  
tab <- rbind(tab, list("sensor_2_peak_acc_raw_zwischenwelle", "2020-10-15 04:29:20", 422958, "schmalgipflig", "rechtsschief"))  
tab <- rbind(tab, list("sensor_2_rms_acc_raw_kurve", "2020-10-15 04:20:18", 422876, "schmalgipflig", "schmalgipflig"))  
tab <- rbind(tab, list("sensor_3_rms_acc_raw_kurve", "2020-11-28 11:11:09", 719387, "schmalgipflig", NA))  
tab <- rbind(tab, list("sensor_2_rms_acc_raw_kurve_2", "2020-10-15 03:44:14", 422548, "schmalgipflig", "rechtsschief"))  
tab <- rbind(tab, list("sensor_3_rms_acc_raw_kurve_2", "2020-09-27 22:01:27", 344156, "multimodal", "multimodal"))  
tab <- rbind(tab, list("sensor_2_peak_acc_raw_kurve", "2020-10-15 03:17:47", 422302, "schmalgipflig", "schmalgipflig"))  
tab <- rbind(tab, list("sensor_3_peak_acc_raw_kurve", "2020-09-02 07:31:17", 220664, "rechtsschief", "schmalgipflig"))  
tab <- rbind(tab, list("sensor_2_peak_acc_raw_kurve_2", "2020-10-15 04:47:22", 423122, "schmalgipflig", "rechtsschief"))  
tab <- rbind(tab, list("sensor_3_peak_acc_raw_kurve_2", "2020-09-28 03:40:19", 347272, "multimodal", "rechtsschief"))  
tab <- rbind(tab, list("sensor_2_rms_acc_raw_teller", "2020-09-27 12:03:23", 338662, "rechtschief", "schmalgipflig"))  
tab <- rbind(tab, list("sensor_4_rms_acc_raw_teller", "2020-10-22 19:12:48", 482572, "schmalgipflig", "schmalgipflig"))  
tab <- rbind(tab, list("sensor_2_rms_acc_raw_teller_2", "2020-09-27 12:47:29", 339072, "rechtsschief", "schmalgipflig"))  
tab <- rbind(tab, list("sensor_4_rms_acc_raw_teller_2", "2020-10-21 15:53:40", 467648, "schmalgipflig", "schmalgipflig"))  
tab <- rbind(tab, list("sensor_2_peak_acc_raw_teller", "2020-09-27 14:16:41", 339892, "rechtsschief", "schmalgipflig"))  
tab <- rbind(tab, list("sensor_4_peak_acc_raw_teller", "2020-09-11 17:13:20", 292004, "rechtsschief", "rechtsschief"))  
tab <- rbind(tab, list("sensor_2_peak_acc_raw_teller_2", "2020-09-27 14:16:41", 339892, "rechtsschief", "schmalgipflig"))  
tab <- rbind(tab, list("sensor_4_peak_acc_raw_teller_2", "2020-09-27 14:16:41", 339892, "rechtsschief", "rechtsschief"))  
tab <- rbind(tab, list("sensor_2_rms_acc_raw_bolzen", "2020-10-15 06:53:28", 424270, "schmalgipflig", "breitgipflig"))  
tab <- rbind(tab, list("sensor_3_rms_acc_raw_bolzen", "2020-11-28 11:11:09", 719387, "schmalgipflig", NA))  
tab <- rbind(tab, list("sensor_2_rms_acc_raw_bolzen_2", "2020-10-15 03:53:15", 422630, "schmalgipflig", "breitgipflig"))  
tab <- rbind(tab, list("sensor_3_rms_acc_raw_bolzen_2", "2020-11-28 11:11:09", 719387, "rechtsschief", NA))  
tab <- rbind(tab, list("sensor_2_peak_acc_raw_bolzen", "2020-10-15 03:35:13", 422466, "schmalgipflig", "breitgipflig"))  
tab <- rbind(tab, list("sensor_3_peak_acc_raw_bolzen", "2020-11-28 11:11:09", 719387, "schmalgipflig", NA))  
tab <- rbind(tab, list("sensor_2_peak_acc_raw_bolzen_2", "2020-10-15 03:26:48", 422384, "schmalgipflig", "rechtsschief"))  
tab <- rbind(tab, list("sensor_3_peak_acc_raw_bolzen_2", "2020-10-14 11:25:38", 413610, "schmalgipflig", "breitgipflig"))  
tab <- rbind(tab, list("sensor_2_rms_acc_env_bereich_1", "2020-10-14 22:11:41", 419514, "bimodal", "bimodal"))  
tab <- rbind(tab, list("sensor_3_rms_acc_env_bereich_1", "2020-09-27 14:43:27", 340138, "bimodal", "bimodal"))  
tab <- rbind(tab, list("sensor_4_rms_acc_env_bereich_1", "2020-09-27 13:41:34", 339564, "bimodal", "bimodal"))  
tab <- rbind(tab, list("sensor_2_peak_acc_env_bereich_1", "2020-10-14 23:33:07", 420252, "bimodal", "bimodal"))  
tab <- rbind(tab, list("sensor_3_peak_acc_env_bereich_1", "2020-09-27 16:22:10", 341040, "bimodal", "bimodal"))  
tab <- rbind(tab, list("sensor_4_peak_acc_env_bereich_1", "2020-09-27 14:52:57", 340220, "bimodal", "bimodal"))  
tab <- rbind(tab, list("sensor_2_rms_acc_env_bereich_2", "2020-11-14 10:14:43", 623448, "rechtsschief", "breitgipflig"))  
tab <- rbind(tab, list("sensor_4_rms_acc_env_bereich_2", "2020-10-16 09:53:39", 439030, "schmalgipflig", "rechtsschief"))  
tab <- rbind(tab, list("sensor_2_peak_acc_env_bereich_2", "2020-10-16 07:02:35", 437472, "schmalgipflig", "rechtsschief"))  
tab <- rbind(tab, list("sensor_4_peak_acc_env_bereich_2", "2020-09-28 06:02:24", 348584, "schmalgipflig", "rechtsschief"))  
tab <- rbind(tab, list("sensor_1_rms_acc_env_bereich_3", "2020-10-15 01:29:49", 421318, "bimodal", "bimodal"))  
tab <- rbind(tab, list("sensor_2_rms_acc_env_bereich_3", "2020-10-16 00:17:44", 433782, "rechtsschief", "rechtsschief"))  
tab <- rbind(tab, list("sensor_3_rms_acc_env_bereich_3", "2020-09-26 05:01:18", 321524, "schmalgipflig", "breitgipflig"))  
tab <- rbind(tab, list("sensor_1_peak_acc_env_bereich_3", "2020-10-14 19:03:49", 417792, "rechtsschief", "rechtsschief"))  
tab <- rbind(tab, list("sensor_2_peak_acc_env_bereich_3", "2020-10-16 07:02:35", 437472, "schmalgipflig", "rechtsschief"))  
tab <- rbind(tab, list("sensor_3_peak_acc_env_bereich_3", "2020-09-25 14:43:53", 313652, "schmalgipflig", "rechtsschief"))  
tab <- rbind(tab, list("sensor_1_rms_acc_env_bereich_4", "2020-09-03 02:44:52", 231160, "rechtsschief", "bimodal"))  
tab <- rbind(tab, list("sensor_2_rms_acc_env_bereich_4", "2020-09-08 20:57:00", 254694, "rechtsschief", "rechtsschief"))  
tab <- rbind(tab, list("sensor_3_rms_acc_env_bereich_4", "2020-09-27 06:59:32", 335874, "rechtsschief", "rechtsschief"))  
tab <- rbind(tab, list("sensor_1_peak_acc_env_bereich_4", "2020-08-19 07:30:28", 118000, "glockenförmig", "glockenförmig"))  
tab <- rbind(tab, list("sensor_2_peak_acc_env_bereich_4", "2020-09-11 06:33:39", 286182, "schmalgipflig", "schmalgipflig"))  
tab <- rbind(tab, list("sensor_3_peak_acc_env_bereich_4", "2020-09-26 21:48:27", 330790, "rechtsschief", "rechtsschief"))  
tab <- rbind(tab, list("sensor_1_rms_acc_env_bereich_5", "2020-10-14 19:48:08", 418202, "bimodal", "bimodal"))  
tab <- rbind(tab, list("sensor_2_rms_acc_env_bereich_5", "2020-08-26 10:20:16", 171136, "bimodal", "bimodal"))  
tab <- rbind(tab, list("sensor_3_rms_acc_env_bereich_5", "2020-09-28 04:06:31", 347518, "bimodal", "rechtsschief"))  
tab <- rbind(tab, list("sensor_1_peak_acc_env_bereich_5", "2020-10-16 20:37:01", 444934, "schmalgipflig", "rechtsschief"))  
tab <- rbind(tab, list("sensor_2_peak_acc_env_bereich_5", "2020-09-08 23:21:07", 256006, "rechtsschief", "glockenförmig"))  
tab <- rbind(tab, list("sensor_3_peak_acc_env_bereich_5", "2020-09-28 01:08:39", 345878, "rechtsschief", "rechtsschief"))  
tab <- rbind(tab, list("sensor_1_rms_acc_env_bereich_6", "2020-09-12 00:57:48", 296268, "schmalgipflig", "rechtsschief"))  
tab <- rbind(tab, list("sensor_2_rms_acc_env_bereich_6", "2020-10-14 10:03:31", 412872, "bimodal", "bimodal"))  
tab <- rbind(tab, list("sensor_3_rms_acc_env_bereich_6", "2020-09-27 07:17:22", 336038, "glockenförmig", "linksschief"))  
tab <- rbind(tab, list("sensor_1_peak_acc_env_bereich_6", "2020-09-09 06:24:11", 259860, "rechtsschief", "rechtsschief"))  
tab <- rbind(tab, list("sensor_2_peak_acc_env_bereich_6", "2020-09-27 23:57:17", 345222, "bimodal", "rechtsschief"))  
tab <- rbind(tab, list("sensor_3_peak_acc_env_bereich_6", "2020-09-27 06:06:03", 335382, "schmalgipflig", "breitgipflig"))  
tab <- rbind(tab, list("sensor_1_rms_acc_env_bereich_7", "2020-10-14 10:58:01", 413364, "schmalgipflig", "breitgipflig"))  
tab <- rbind(tab, list("sensor_2_rms_acc_env_bereich_7", "2020-10-17 11:31:17", 453134, "rechtsschief", "rechtsschief"))  
tab <- rbind(tab, list("sensor_1_peak_acc_env_bereich_7", "2020-09-27 15:37:01", 340630, "schmalgipflig", "rechtsschief"))  
tab <- rbind(tab, list("sensor_2_peak_acc_env_bereich_7", "2020-08-26 21:09:28", 177040, "rechtsschief", "rechtsschief"))  
tab <- rbind(tab, list("sensor_1_cep_eingangswelle", "2020-11-28 11:11:09", 719387, "schmalgipflig", NA))  
tab <- rbind(tab, list("sensor_2_cep_eingangswelle", "2020-11-28 11:11:09", 719387, "schmalgipflig", NA))  
tab <- rbind(tab, list("sensor_1_cep_zwischenwelle", "2020-11-28 11:11:09", 719387, "schmalgipflig", NA))  
tab <- rbind(tab, list("sensor_2_cep_zwischenwelle", "2020-11-28 11:11:09", 719387, "linksschief", NA))  
tab <- rbind(tab, list("sensor_1_cep_kurve", "2020-11-28 11:11:09", 719387, "schmalgipflig", NA))  
tab <- rbind(tab, list("sensor_2_cep_kurve", "2020-11-28 11:11:09", 719387, "schmalgipflig", NA))  
tab <- rbind(tab, list("sensor_3_cep_kurve", "2020-11-28 11:11:09", 719387, "rechtsschief", NA))  
tab <- rbind(tab, list("sensor_1_rms_ritzel_1", "2020-10-15 05:15:00", 423368, "rechtsschief", "bimodal"))  
tab <- rbind(tab, list("sensor_2_rms_ritzel_1", "2020-09-24 14:49:44", 300450, "bimodal", "rechtsschief"))  
tab <- rbind(tab, list("sensor_1_peak_acc_raw_eingangswelle", "2020-10-15 01:47:44", 421482, "schmalgipflig", "rechtsschief"))  
tab <- rbind(tab, list("sensor_2_peak_acc_raw_eingangswelle", "2020-10-15 03:08:46", 422220, "schmalgipflig", "rechtsschief"))  
tab <- rbind(tab, list("sensor_1_peak_acc_raw_eingangswelle_2", "2020-10-14 23:14:32", 420088, "schmalgipflig", "rechtsschief")) 
tab <- rbind(tab, list("sensor_2_peak_acc_raw_eingangswelle_2", "2020-10-15 05:42:03", 423614, "schmalgipflig", "rechtsschief")) 
tab <- rbind(tab, list("sensor_1_rms_ritzel_2", "2020-10-14 22:38:38", 419760, "schmalgipflig", "rechtsschief"))  
tab <- rbind(tab, list("sensor_2_rms_ritzel_2", "2020-09-28 02:37:18", 346698, "schmalgipflig", "schmalgipflig"))  
tab <- rbind(tab, list("sensor_1_rms_zahnrad_1", "2020-10-14 13:22:11", 414676, "schmalgipflig", "schmalgipflig"))  
tab <- rbind(tab, list("sensor_2_rms_zahnrad_1", "2020-10-15 07:03:04", 424352, "schmalgipflig", "schmalgipflig"))  
tab <- rbind(tab, list("sensor_1_rms_zahnrad_2", "2020-10-29 05:32:35", 524720, "rechtsschief", "schmalgipflig"))  
tab <- rbind(tab, list("sensor_2_rms_zahnrad_2", "2020-11-28 11:11:09", 719387, "schmalgipflig", NA))  
tab <- rbind(tab, list("sensor_1_rms_zp1_3", "2020-11-13 02:32:46", 606146, "bimodal", "bimodal"))  
tab <- rbind(tab, list("sensor_2_rms_zp1_3", "2020-11-14 04:32:40", 620332, "glockenförmig", "breitgipflig"))  
tab <- rbind(tab, list("sensor_1_peak_zp1_3", "2020-11-13 01:56:05", 605818, "bimodal", "bimodal"))  
tab <- rbind(tab, list("sensor_2_peak_zp1_3", "2020-11-14 06:29:53", 621398, "glockenförmig", "glockenförmig"))  
tab <- rbind(tab, list("sensor_1_rms_zp1_2", "2020-10-17 01:39:39", 447722, "schmalgipflig", "breitgipflig"))  
tab <- rbind(tab, list("sensor_2_rms_zp1_2", "2020-10-15 07:12:04", 424434, "schmalgipflig", "schmalgipflig"))  
tab <- rbind(tab, list("sensor_1_peak_zp1_2", "2020-10-21 11:05:07", 465024, "glockenförmig", "rechtsschief"))  
tab <- rbind(tab, list("sensor_2_peak_zp1_2", "2020-10-15 23:05:46", 433126, "glockenförmig", "rechtsschief"))  
tab <- rbind(tab, list("sensor_1_rms_acc_raw_bereich_8", "2020-10-16 07:21:12", 437636, "schmalgipflig", "rechtsschief"))  
tab <- rbind(tab, list("sensor_2_rms_acc_raw_bereich_8", "2020-10-15 14:50:25", 428616, "schmalgipflig", "glockenförmig"))  
tab <- rbind(tab, list("sensor_1_peak_acc_raw_bereich_8", "2020-11-14 14:09:45", 625580, "rechtsschief", "rechtsschief"))  
tab <- rbind(tab, list("sensor_2_peak_acc_raw_bereich_8", "2020-10-24 09:22:58", 503400, "schmalgipflig", "rechtsschief"))  
tab <- rbind(tab, list("sensor_1_rms_acc_raw_bereich_11", "2020-09-26 10:40:31", 324640, "rechtsschief", "schmalgipflig"))  
tab <- rbind(tab, list("sensor_2_rms_acc_raw_bereich_11", "2020-09-26 08:53:21", 323656, "rechtsschief", "schmalgipflig"))  
tab <- rbind(tab, list("sensor_1_peak_acc_raw_bereich_11", "2020-10-29 16:28:04", 530706, "rechtsschief", "schmalgipflig"))  
tab <- rbind(tab, list("sensor_2_peak_acc_raw_bereich_11", "2020-09-25 21:35:06", 317424, "rechtsschief", "schmalgipflig"))  
tab <- rbind(tab, list("sensor_1_rms_acc_raw_bereich_12", "2020-10-16 14:23:37", 441490, "rechtsschief", "rechtsschief"))  
tab <- rbind(tab, list("sensor_2_rms_acc_raw_bereich_12", "2020-09-09 03:33:18", 258302, "bimodal", "bimodal"))  
tab <- rbind(tab, list("sensor_1_peak_acc_raw_bereich_12", "2020-10-16 10:48:04", 439522, "rechtsschief", "rechtsschief"))  
tab <- rbind(tab, list("sensor_2_peak_acc_raw_bereich_12", "2020-10-15 20:14:13", 431568, "rechtsschief", "bimodal"))  
tab <- rbind(tab, list("sensor_1_rms_zp2_3", "2020-10-14 23:42:06", 420334, "schmalgipflig", "rechtsschief"))  
tab <- rbind(tab, list("sensor_2_rms_zp2_3", "2020-10-15 03:17:11", 422302, "schmalgipflig", "rechtsschief"))  
tab <- rbind(tab, list("sensor_1_peak_zp2_3", "2020-10-14 20:50:56", 418776, "schmalgipflig", "rechtsschief"))  
tab <- rbind(tab, list("sensor_2_peak_zp2_3", "2020-10-15 01:02:18", 421072, "schmalgipflig", "rechtsschief"))  
tab <- rbind(tab, list("sensor_1_rms_zp2_2", "2020-10-15 06:08:30", 423860, "schmalgipflig", "rechtsschief"))  
tab <- rbind(tab, list("sensor_2_rms_zp2_2", "2020-10-15 04:20:54", 422876, "schmalgipflig", "rechtsschief"))  
tab <- rbind(tab, list("sensor_1_peak_zp2_2", "2020-10-15 01:56:09", 421564, "schmalgipflig", "rechtsschief"))  
tab <- rbind(tab, list("sensor_2_peak_zp2_2", "2020-10-15 01:02:18", 421072, "schmalgipflig", "rechtsschief"))  
tab <- rbind(tab, list("sensor_1_rms_acc_raw_bereich_7", "2020-10-31 22:55:04", 560554, "bimodal", "bimodal"))  
tab <- rbind(tab, list("sensor_2_rms_acc_raw_bereich_7", "2020-10-17 15:51:32", 455512, "rechtsschief", "schmalgipflig"))  
tab <- rbind(tab, list("sensor_1_peak_acc_raw_bereich_7", "2020-09-26 11:51:52", 325296, "bimodal", "bimodal"))  
tab <- rbind(tab, list("sensor_2_peak_acc_raw_bereich_7", "2020-09-27 04:36:53", 334562, "rechtsschief", "rechtsschief"))  
tab <- rbind(tab, list("sensor_1_rms_acc_raw_bereich_9", "2020-10-24 17:57:13", 508074, "bimodal", "bimodal"))  
tab <- rbind(tab, list("sensor_2_rms_acc_raw_bereich_9", "2020-11-28 11:11:09", 719387, "schmalgipflig", NA))  
tab <- rbind(tab, list("sensor_1_peak_acc_raw_bereich_9", "2020-10-22 23:33:47", 484950, "bimodal", "bimodal"))  
tab <- rbind(tab, list("sensor_2_peak_acc_raw_bereich_9", "2020-09-02 23:44:03", 229520, "schmalgipflig", "breitgipflig"))  
tab <- rbind(tab, list("sensor_1_rms_acc_raw_bereich_10", "2020-10-15 04:20:18", 422876, "schmalgipflig", "schmalgipflig"))  
tab <- rbind(tab, list("sensor_2_rms_acc_raw_bereich_10", "2020-10-17 07:29:05", 450920, "schmalgipflig", "schmalgipflig"))  
tab <- rbind(tab, list("sensor_1_peak_acc_raw_bereich_10", "2020-10-15 09:44:27", 425828, "rechtsschief", "rechtsschief"))  
tab <- rbind(tab, list("sensor_2_peak_acc_raw_bereich_10", "2020-10-16 21:21:31", 445344, "schmalgipflig", "rechtsschief"))  
tab <- rbind(tab, list("sensor_2_rms_stft_bereich_rb", "2020-10-16 05:05:20", 436406, "linksschief", "rechtsschief"))  
tab <- rbind(tab, list("sensor_3_rms_stft_bereich_rb", "2020-11-28 11:11:09", 719387, "schmalgipflig", NA))  
tab <- rbind(tab, list("sensor_1_rms_acc_env_zwischenwelle", "2020-08-28 01:15:11", 192374, "schmalgipflig", "schmalgipflig"))  
tab <- rbind(tab, list("sensor_1_peak_acc_env_zwischenwelle", "2020-08-27 15:37:50", 187126, "schmalgipflig", "schmalgipflig"))  
tab <- rbind(tab, list("sensor_1_rms_acc_env_eingangswelle", "2020-10-17 04:39:21", 449362, "rechtsschief", "rechtsschief"))  
tab <- rbind(tab, list("sensor_1_peak_acc_env_eingangswelle", "2020-10-16 15:43:20", 442228, "schmalgipflig", "rechtsschief"))  
tab <- rbind(tab, list("sensor_1_rms_acc_raw_eingangswelle", "2020-10-15 02:23:07", 421810, "schmalgipflig", "rechtsschief"))  
tab <- rbind(tab, list("sensor_2_rms_acc_raw_eingangswelle", "2020-10-15 03:35:13", 422466, "schmalgipflig", "rechtsschief"))  
tab <- rbind(tab, list("sensor_1_rms_acc_raw_eingangswelle_2", "2020-10-14 22:56:34", 419924, "schmalgipflig", "breitgipflig"))  
tab <- rbind(tab, list("sensor_2_rms_acc_raw_eingangswelle_2", "2020-10-15 05:33:02", 423532, "schmalgipflig", "rechtsschief"))  

tab <- tab[-c(1), ]
colnames(tab) <- c("messwert", "meta_asc_timestamp", "cycle", "vor_cp", "nach_cp") 
tab$cycle <- as.numeric(tab$cycle)
tab$meta_asc_timestamp <- as.POSIXct(tab$meta_asc_timestamp)

# häufigkeit der cps ---

tab %>% 
  filter(cycle <= 700000, grepl("sensor_1", Teil)) %>% 
  ggplot(aes(x=cycle)) +
  xlim(0, max(tab$cycle)) +
  geom_histogram(fill="#69b3a2", color="#e9ecef") +
  ggtitle("Häufigkeit der Changepoints für Sensor 1")

tab %>% 
  filter(cycle <= 700000, grepl("sensor_2", Teil)) %>% 
  ggplot(aes(x=cycle)) +
  xlim(0, max(tab$cycle)) +
  geom_histogram(fill="#69b3a2", color="#e9ecef") +
  ggtitle("Häufigkeit der Changepoints für Sensor 2")

max(tab$cycle)

tab %>% 
  filter(cycle <= 700000, grepl("sensor_3", Teil)) %>% 
  ggplot(aes(x=cycle)) +
  geom_histogram(fill="#69b3a2", color="#e9ecef") +
  xlim(0, max(tab$cycle)) +
  ggtitle("Häufigkeit der Changepoints für Sensor 3")


tab %>% 
  filter(cycle <= 700000, grepl("sensor_4", Teil)) %>% 
  ggplot(aes(x=cycle)) +
  xlim(0, max(tab$cycle)) +
  geom_histogram(fill="#69b3a2", color="#e9ecef") +
  ggtitle("Häufigkeit der Changepoints für Sensor 4")



# clustering timeseries auf raw data ---


dtw <- cyc %>%
  filter(cycle >= 0, cycle <=70000) %>% 
  dplyr::select(meta_asc_timestamp, matches("zwischenwelle")) %>% 
  gather(Bauteil, Messwert, -meta_asc_timestamp) %>% 
  spread(meta_asc_timestamp, Messwert) %>% 
  column_to_rownames(var = "Bauteil") %>%
  BBmisc::normalize(method="standardize") %>% 
  dtwclust::tsclust(type="hierarchical", k=3:9, distance="dtw_basic")

# clustering timeseries auf trend ---

# function 

fun1 <- function(x,y){
  
  stlplot  <- timetk::plot_stl_diagnostics(
    tibble(a=x, b=y), a, b,
    .feature_set = c("trend"),
    .frequency   = "auto",
    .trend       = "auto",
    .message     = FALSE,
    .interactive = FALSE) 
  
  return(stlplot$data$.group_value)
  
}

# create df

cyc_trend <- cyc %>% 
  mutate(across(starts_with("sensor"), ~ fun1(meta_asc_timestamp, .x)))



dtw <- cyc_trend %>%
  #filter(cycle >= 0, cycle <=70000) %>% 
  #dplyr::select(meta_asc_timestamp, matches("zwischenwelle")) %>% 
  gather(messung, wert, -meta_asc_timestamp) %>% 
  spread(meta_asc_timestamp, wert) %>% 
  column_to_rownames(var = "Bauteil") %>% 
  BBmisc::normalize(method="standardize") %>% 
  dtwclust::tsclust(type="hierarchical",distance="dtw_basic")


plot(dtw, type = "sc")

dtwclust::tsclust(dtw, type = "h", k = 6L, distance = "dtw_basic")



anom_vars <- tab %>% pull(messwert)

anom_tbl <- tibble(feature = character(), 
                   outlier = numeric(), 
                   outlier_norm = numeric(), 
                   outlier_failure = numeric(),
                   outlier_before_cp = numeric(),
                   outlier_after_cp = numeric(),
                   outlier_cp = numeric())

for (name in anom_vars) {
  
  print(name)
  
  cp_cycle <- tab %>% filter(messwert == name) %>% pull(cycle)
  
  anom_df <- mess_df %>% 
    select(cycle, {{name}}, meta_cw_ccw) %>% 
    mutate(across(cycle, ~(as.POSIXct("2021-01-01") + lubridate::seconds(.x) + (meta_cw_ccw == "CCW")))) %>% 
    time_decompose(target = {{name}}) %>% 
    anomalize(remainder) 
  
  anom_count <- anom_df %>% 
    summarize(sum(anomaly == "Yes")) %>% 
    pull()
  
  anom_count_early <- anom_df %>% 
    filter(cycle <= as.POSIXct("2021-01-01") + lubridate::seconds(cp_cycle)) %>% 
    summarize(sum(anomaly == "Yes")) %>% 
    pull()
  
  anom_count_late <- anom_df %>% 
    filter(cycle > as.POSIXct("2021-01-01") + lubridate::seconds(cp_cycle)) %>% 
    summarize(sum(anomaly == "Yes")) %>% 
    pull()
  
  anom_count_before <- anom_df %>% 
    filter(
      between(
        cycle, 
        as.POSIXct("2021-01-01") + lubridate::seconds(cp_cycle - 20000),
        as.POSIXct("2021-01-01") + lubridate::seconds(cp_cycle)
      )
    ) %>%
    summarize(sum(anomaly == "Yes")) %>% 
    pull()
  
  anom_count_after <- anom_df %>% 
    filter(
      between(
        cycle, 
        as.POSIXct("2021-01-01") + lubridate::seconds(cp_cycle),
        as.POSIXct("2021-01-01") + lubridate::seconds(cp_cycle + 20000)
      )
    ) %>%
    summarize(sum(anomaly == "Yes")) %>% 
    pull()
  
  anom_count_cp <- anom_df %>% 
    filter(
      between(
        cycle, 
        as.POSIXct("2021-01-01") + lubridate::seconds(cp_cycle - 50000),
        as.POSIXct("2021-01-01") + lubridate::seconds(cp_cycle + 50000)
      )
    ) %>% 
    summarize(sum(anomaly == "Yes")) %>% 
    pull()
  
  anom_tbl <- anom_tbl %>% 
    add_row(feature = name, outlier = anom_count, 
            outlier_norm = anom_count_early,
            outlier_failure = anom_count_late, 
            outlier_cp = anom_count_cp,
            outlier_before_cp = anom_count_before, 
            outlier_after_cp = anom_count_after)
}


# weiss/Analyse der Ergebnisse --------------------------------------------------

anom_tbl %>% 
  filter(str_starts(feature, "sensor_")) %>% 
  mutate(sensor = str_extract(feature, "sensor_[0-9]{1,}")) %>% 
  group_by(sensor) %>% summarize(mean(outlier))

anom_tbl %>% 
  rename(
    outlier_all = outlier,
    outlier_pre_cp = outlier_norm,
    outlier_post_cp = outlier_failure,
    outlier_pre_cp_near = outlier_before_cp,
    outlier_post_cp_near = outlier_after_cp,
  ) %>% 
  mutate(
    ratio_pre_cp = outlier_pre_cp_near / outlier_pre_cp,
    ratio_all = outlier_cp / outlier_all
  ) %>% 
  write_csv("anom_tbl_cp.csv") 

anom_tbl <- read_csv("anom_tbl_cp.csv")

anom_tbl %>% 
  filter(outlier_pre_cp != 0, str_detect(feature, pattern = "(kurtosis|skewness)", negate = TRUE)) %>% 
  View()

anom_tbl %>% 
  filter(outlier_pre_cp != 0, str_detect(feature, pattern = "(kurtosis|skewness)", negate = TRUE)) %>% 
  filter(ratio_pre_cp >= 0.3)
#mutate(special = ratio_pre_cp >= 0.3, welle = stringr::str_detect(feature, "bolzen")) %>% 
#count(special, welle)



anom_tbl %>% 
  filter(outlier_pre_cp != 0, str_detect(feature, pattern = "(kurtosis|skewness)", negate = TRUE)) %>% 
  ggplot(aes(x = outlier_pre_cp, y = ratio_pre_cp)) + geom_point() +
  xlab("Anzahl Anomalien vor CP") +
  ylab("Verhältnis Anomalien vor CP")

anom_tbl %>% 
  filter(outlier_pre_cp != 0, str_detect(feature, pattern = "(kurtosis|skewness)", negate = TRUE)) %>% 
  mutate(Bauteil = str_remove(str_extract(str_remove(feature, "_[0-9]{1,}$"), "_[a-z0-9]{1,}$"), "_")) %>% 
  ggplot(aes(x = ratio_pre_cp)) + geom_histogram(aes(fill = Bauteil)) +
  xlab("Verhältnis Anomalien vor CP") +
  ylab("Anzahl Messwerte")

anom_tbl %>% 
  filter(outlier_pre_cp != 0, str_detect(feature, pattern = "(kurtosis|skewness)", negate = TRUE)) %>% 
  mutate(Sensor = str_extract(feature, "sensor_[0-9]{1,}")) %>% 
  ggplot(aes(x = ratio_pre_cp)) + geom_histogram(aes(fill = Sensor)) +
  xlab("Verhältnis Anomalien vor CP") +
  ylab("Anzahl Messwerte")





# njn #########

# daten aufbereiten


# jäger in bundesland ###
# hier wird ein datensatz eingelesen, den jannis erzeugt hat mit einer übersicht über anzahl an jäger in deutschland nach bundesland
read_csv("jaegerindeutschland.csv")




# shape de ###
# hier wird ein datensatz eingelesen, der die daten für die map-plots beinhaltet und ifo über bundesländer, die an die kundendaten gejoint werden

de <- sf::st_read("/Users/alexchaichan/R/Postleitzahlengebiete_-_OSM/OSM_PLZ.shp")



export_kunden_daten <- read.csv2("Export_Kunden_Daten.csv",  fileEncoding = "Latin1") %>% 
  janitor::clean_names() %>% #clean names
  rename(name = vorname,
         vorname = nachname) %>% 
  left_join(., read_csv("namen_alter.csv"), by="vorname") %>% 
  mutate_if(is.character, list(~na_if(.,""))) %>% # empty strings as NA
  group_by(kundennummer, verkaufsdatum) %>% 
  mutate(bestell_id = cur_group_id()) %>%  #bestell id vergeben
  ungroup() %>% 
  mutate(geburtsdatum =  na_if(geburtsdatum, "  .  ."),
         geburtsdatum = lubridate::dmy(geburtsdatum)) %>% 
  mutate_at(vars(geburtsdatum), ~replace(geburtsdatum, geburtsdatum >="2005-01-01" | geburtsdatum <="1921-01-01", NA)) %>% #na_if yob >2005 | < 1921
  mutate(id  = row_number(),
         drop_kunden = ifelse(kundennummer %in% c("256313", "295356", "218082", "134177", "257382", "206304", "289096"), TRUE, FALSE),
         drop_artikel = ifelse(vk_netto <= 0.10, TRUE, FALSE),
         drop_warengruppe = ifelse(warengruppe %in% c("9999"), TRUE, FALSE),
         geburtsdatum = ,  # string make NA
         geschlecht = ifelse(str_detect(anrede, "He|Gra|Mr|Prinz$|Baron$|Fürst$|Erb"), "Mann",
                      ifelse(str_detect(anrede, "Fr|Grä|Ms.|Prinzessin|Baronin|Fürstin"), "Frau", NA)),
         geschlecht = coalesce(geschlecht, sex),
         alter = calc_age(geburtsdatum), 
         kundentyp = ifelse(is.na(anrede), "gewerblich",
                     ifelse(str_detect(anrede, "Firma|Praxis|handlung|amt$|Verlag|institut$|GmbH|verein"), "gewerblich", "privat")),
         verkaufsdatum = lubridate::dmy(verkaufsdatum),
         kundennummer = as.character(kundennummer),
         plz = str_pad(plz, width = 5, "left", pad="0"),
         plz_gebiet = str_extract(plz, "^[0-9]"),
         wochentag = weekdays(verkaufsdatum)) %>%
  group_by(kundennummer) %>% 
  mutate(vor_x_tagen_bestellt = as.numeric(as.Date("2021-07-23") - verkaufsdatum),
         tage_zwischen_bestellungen = verkaufsdatum - lag(verkaufsdatum)) %>% 
  ungroup() %>%
  separate(artikelnummer, "artikel_nr", sep="     ", remove=FALSE) %>% # artikelnummer separaten (größe abspalten)
  separate(verkaufsdatum, c("jahr", "monat", "tag"), sep="-", remove=FALSE) %>%
  relocate(id) %>% 
  relocate(bestell_id, .after = kundennummer) %>% 
  relocate(plz_gebiet, .after = plz) %>% 
  relocate(wochentag, .after = verkaufsdatum) 


export_kunden_daten <- export_kunden_daten %>% 
  distinct(bestell_id, .keep_all = TRUE) %>% 
  group_by(kundennummer) %>% 
  summarise(mean_tage_zwischen_bestellungen = as.numeric(mean(tage_zwischen_bestellungen, na.rm=TRUE))) %>%
  ungroup() %>% 
  right_join(y=export_kunden_daten, by="kundennummer") %>% 
  mutate_at(vars(mean_tage_zwischen_bestellungen), ~replace(., is.na(.), 0)) %>% 
  relocate(mean_tage_zwischen_bestellungen, .after = vor_x_tagen_bestellt)
  

export_kunden_daten <- export_kunden_daten %>% 
  distinct(kundennummer, .keep_all = TRUE) %>% 
  group_by(vorname) %>% 
  summarise(n=n(),
            true_age=mean(alter, na.rm=TRUE)) %>% 
  ungroup() %>% 
  right_join(y=export_kunden_daten, by="vorname") %>% 
  distinct(kundennummer, .keep_all = TRUE) %>% 
  mutate(true_age = ifelse(n<5 | n == max(n), NA, true_age)) %>% 
  group_by(kundennummer) %>% 
  mutate(mean_age = (testalter+true_age)/2,
         new_age = coalesce(alter, mean_age)) %>% 
  ungroup() %>% 
  mutate_all(~ifelse(is.nan(.), NA, .)) %>% 
  select(new_age, kundennummer) %>% 
  left_join(y=export_kunden_daten, by="kundennummer") 
  
  
export_kunden_daten <- export_kunden_daten %>% 
  rename(aprox_alter=new_age) %>% 
  mutate(altersgruppe = ifelse(aprox_alter >= 16 & aprox_alter <= 30, "16-30",
                        ifelse(aprox_alter >= 31 & aprox_alter <= 45, "31-45",
                        ifelse(aprox_alter >= 46 & aprox_alter <= 60, "46-60",
                        ifelse(aprox_alter >= 61 & aprox_alter <= 75, "61-75",
                        ifelse(aprox_alter >= 76 & aprox_alter <= 80, "76-80",
                        ifelse(aprox_alter >= 81 & aprox_alter <= 100, "81-100", "NA")))))),
         aprox_geschlecht = coalesce(geschlecht, sex)) %>% 
  relocate(aprox_alter, .after = alter) %>% 
  relocate(aprox_geschlecht, .after = geschlecht) %>% 
  relocate(id) %>% 
  select(-testalter, -sex)


  


export_artikeldaten.csv <- read.csv("Export_Artikeldaten.csv", fileEncoding = "UTF-8") %>% 
  janitor::clean_names() #clean names
  
  
# artikel daten ###

trans <- export_kunden_daten %>% 
  filter(drop_artikel==F, drop_kunden==F, drop_warengruppe==F) %>% 
  select(id, kundennummer, bestell_id, land, plz_gebiet, verkaufsdatum, wochentag, jahr, monat, tag, artikelnummer, artikel_nr, artikelbezeichnung, warengruppe, vk_netto, aprox_geschlecht, aprox_alter, kundentyp, vor_x_tagen_bestellt, tage_zwischen_bestellungen, altersgruppe)


# kunden daten  ###





kunden <- export_kunden_daten %>%
  distinct(bestell_id, .keep_all = TRUE) %>% 
  group_by(kundennummer) %>% 
  summarise(mean_tage_zwischen_bestellungen = as.numeric(mean(tage_zwischen_bestellungen, na.rm=TRUE))) %>%
  ungroup() %>% 
  right_join(y=export_kunden_daten, by="kundennummer") %>% 
  mutate_at(vars(mean_tage_zwischen_bestellungen), ~replace(., is.na(.), NA)) %>%
  filter(drop_artikel==F, drop_kunden==F, drop_warengruppe==F) %>% 
  group_by(kundennummer) %>% 
  mutate(gesamt_vk_netto = round(sum(vk_netto),2),
            gekaufte_artikel = n(),
            anzahl_bestellungen = n_distinct(bestell_id),
            artikel_pro_bestellung = round(gekaufte_artikel/anzahl_bestellungen, 2),
            mean_eur_pro_artikel=round(mean(vk_netto),2),
            tage_kunde = max(vor_x_tagen_bestellt),
            mean_eur_pro_bestellung = round(gesamt_vk_netto/anzahl_bestellungen, 2),
            letzte_bestellung_vor= min(vor_x_tagen_bestellt),
            mean_tage_zwischen_bestellungen = mean_tage_zwischen_bestellungen) %>% 
            #kauf_frequenz = (anzahl_bestellungen/tage_kunde))
  ungroup() %>% 
  distinct(kundennummer, .keep_all = T) %>% 
  select(kundennummer, anrede, name, vorname, strasse_nr, land, plz, plz_gebiet, ort, geburtsdatum, aprox_geschlecht, aprox_alter, altersgruppe, kundentyp, gesamt_vk_netto, gekaufte_artikel, anzahl_bestellungen, artikel_pro_bestellung, mean_eur_pro_artikel, mean_eur_pro_bestellung, tage_kunde, letzte_bestellung_vor, mean_tage_zwischen_bestellungen)



# 2009

kunden  <-trans %>% 
  filter(verkaufsdatum >="2009-01-01" & verkaufsdatum <="2009-12-31") %>% 
  group_by(kundennummer) %>% 
  mutate(umsatz_2009 = round(sum(vk_netto),2),
         bestellungen_2009 = n_distinct(bestell_id)) %>% 
  distinct(kundennummer, umsatz_2009, bestellungen_2009) %>% 
  left_join(kunden , ., by="kundennummer")

# 2010

kunden  <-trans %>% 
  filter(verkaufsdatum >="2010-01-01" & verkaufsdatum <="2010-12-31") %>% 
  group_by(kundennummer) %>% 
  mutate(umsatz_2010 = round(sum(vk_netto),2),
         bestellungen_2010 = n_distinct(bestell_id)) %>% 
  distinct(kundennummer, umsatz_2010, bestellungen_2010) %>% 
  left_join(kunden , ., by="kundennummer")

# 2011

kunden  <-trans %>% 
  filter(verkaufsdatum >="2011-01-01" & verkaufsdatum <="2011-12-31") %>% 
  group_by(kundennummer) %>% 
  mutate(umsatz_2011 = round(sum(vk_netto),2),
         bestellungen_2011 = n_distinct(bestell_id)) %>% 
  distinct(kundennummer, umsatz_2011, bestellungen_2011) %>% 
  left_join(kunden , ., by="kundennummer")

# 2012

kunden  <-trans %>% 
  filter(verkaufsdatum >="2012-01-01" & verkaufsdatum <="2012-12-31") %>% 
  group_by(kundennummer) %>% 
  mutate(umsatz_2012 = round(sum(vk_netto),2),
         bestellungen_2012 = n_distinct(bestell_id)) %>% 
  distinct(kundennummer, umsatz_2012, bestellungen_2012) %>% 
  left_join(kunden , ., by="kundennummer")

# 2013

kunden  <-trans %>% 
  filter(verkaufsdatum >="2013-01-01" & verkaufsdatum <="2013-12-31") %>% 
  group_by(kundennummer) %>% 
  mutate(umsatz_2013 = round(sum(vk_netto),2),
         bestellungen_2013 = n_distinct(bestell_id)) %>% 
  distinct(kundennummer, umsatz_2013, bestellungen_2013) %>% 
  left_join(kunden , ., by="kundennummer")

# 2014

kunden  <-trans %>% 
  filter(verkaufsdatum >="2014-01-01" & verkaufsdatum <="2014-12-31") %>% 
  group_by(kundennummer) %>% 
  mutate(umsatz_2014 = round(sum(vk_netto),2),
         bestellungen_2014 = n_distinct(bestell_id)) %>% 
  distinct(kundennummer, umsatz_2014, bestellungen_2014) %>% 
  left_join(kunden , ., by="kundennummer")

# 2015

kunden  <-trans %>% 
  filter(verkaufsdatum >="2015-01-01" & verkaufsdatum <="2015-12-31") %>% 
  group_by(kundennummer) %>% 
  mutate(umsatz_2015 = round(sum(vk_netto),2),
         bestellungen_2015 = n_distinct(bestell_id)) %>% 
  distinct(kundennummer, umsatz_2015, bestellungen_2015) %>% 
  left_join(kunden , ., by="kundennummer")

# 2016

kunden  <-trans %>% 
  filter(verkaufsdatum >="2016-01-01" & verkaufsdatum <="2016-12-31") %>% 
  group_by(kundennummer) %>% 
  mutate(umsatz_2016 = round(sum(vk_netto),2),
         bestellungen_2016 = n_distinct(bestell_id)) %>% 
  distinct(kundennummer, umsatz_2016, bestellungen_2016) %>% 
  left_join(kunden , ., by="kundennummer")

# 2017

kunden  <-trans %>% 
  filter(verkaufsdatum >="2017-01-01" & verkaufsdatum <="2017-12-31") %>% 
  group_by(kundennummer) %>% 
  mutate(umsatz_2017 = round(sum(vk_netto),2),
         bestellungen_2017 = n_distinct(bestell_id)) %>% 
  distinct(kundennummer, umsatz_2017, bestellungen_2017) %>% 
  left_join(kunden , ., by="kundennummer")

# 2018

kunden  <-trans %>% 
  filter(verkaufsdatum >="2018-01-01" & verkaufsdatum <="2018-12-31") %>% 
  group_by(kundennummer) %>% 
  mutate(umsatz_2018 = round(sum(vk_netto),2),
         bestellungen_2018 = n_distinct(bestell_id)) %>% 
  distinct(kundennummer, umsatz_2018, bestellungen_2018) %>% 
  left_join(kunden , ., by="kundennummer")

# 2019

kunden  <-trans %>% 
  filter(verkaufsdatum >="2019-01-01" & verkaufsdatum <="2019-12-31") %>% 
  group_by(kundennummer) %>% 
  mutate(umsatz_2019 = round(sum(vk_netto),2),
         bestellungen_2019 = n_distinct(bestell_id)) %>% 
  distinct(kundennummer, umsatz_2019, bestellungen_2019) %>% 
  left_join(kunden , ., by="kundennummer")

# 2020

kunden  <-trans %>% 
  filter(verkaufsdatum >="2020-01-01" & verkaufsdatum <="2020-12-31") %>% 
  group_by(kundennummer) %>% 
  mutate(umsatz_2020 = round(sum(vk_netto),2),
         bestellungen_2020 = n_distinct(bestell_id)) %>% 
  distinct(kundennummer, umsatz_2020, bestellungen_2020) %>%
  left_join(kunden , ., by="kundennummer")

# 2021

kunden  <-trans %>% 
  filter(verkaufsdatum >="2021-01-01" & verkaufsdatum <="2021-12-31") %>% 
  group_by(kundennummer) %>% 
  mutate(umsatz_2021 = round(sum(vk_netto),2),
         bestellungen_2021 = n_distinct(bestell_id)) %>% 
  distinct(kundennummer, umsatz_2021, bestellungen_2021) %>%
  left_join(kunden , ., by="kundennummer")



# cluster analyse

  # hierachische clusteranalyse ###


cluster <- kunden  %>% 
  slice_sample(n = 30000) %>% #sample 30k weil sonst zu groß #Error in hclust(.) : size cannot be NA nor exceed 65536
  select(kundennummer, gesamt_vk_netto, gekaufte_artikel, anzahl_bestellungen, artikel_pro_bestellung, mean_eur_pro_artikel, mean_eur_pro_bestellung) %>% 
  column_to_rownames(var="kundennummer") %>% 
  scale() %>% 
  as.data.frame()


hclust <- cluster %>% 
  dist() %>% 
  hclust(method = "ward.D")

plot(hclust, hang=-1, cex = 0.7)
rect.hclust(hclust, k = 6, border = "blue")


kunden  <- cluster %>% 
  mutate(cluster=cutree(hclust, 6)) %>% 
  rownames_to_column(var="kundennummer") %>% 
  select(kundennummer, cluster) %>% 
  left_join(kunden , ., by="kundennummer")


kunden <- cluster %>% 
  mutate(cluster=cutree(hclust, 6)) %>% 
  rownames_to_column(var="kundennummer") %>% 
  select(kundennummer, cluster) %>% 
  left_join(kunden, ., by="kundennummer")





kunden  %>%  
  filter(!(is.na(cluster))) %>% 
  group_by(cluster) %>%
  summarise(n_absolut = n(), na.rm=TRUE,
            n_relative =100/10000*n_absolut, na.rm=TRUE,
            mean_age = mean(age, na.rm=TRUE),
            mean_kauf_frequenz = mean(kauf_frequenz, na.rm=TRUE),
            mean_gesamt_vk_netto = mean(gesamt_vk_netto, na.rm=TRUE), 
            mean_gekaufte_artikel = mean(gekaufte_artikel, na.rm=TRUE),
            mean_anzahl_bestellungen = mean(anzahl_bestellungen, na.rm=TRUE),
            mean_artikel_pro_bestellung = mean(artikel_pro_bestellung, na.rm=TRUE),
            mean_eur_pro_artikel = mean(mean_eur_pro_artikel, na.rm=TRUE),
            mean_eur_pro_bestellung = mean(mean_eur_pro_bestellung, na.rm=TRUE),
            mean_tage_kunde = mean(tage_kunde, na.rm=TRUE),
            anzahl_mann = sum(sex == "Mann", na.rm=TRUE),
            anzahl_frau = sum(sex == "Frau", na.rm=TRUE),
            anzahl_gewerblich =sum(kundentyp =="gewerblich", na.rm=TRUE),
            anzahl_privat = sum(kundentyp=="privat", na.rm=TRUE)) %>% 
  round(2) %>% 
  t() %>% 
  janitor::row_to_names(row_number = 1) %>% 
  view()


kunden_slice <-orders %>%  # slice entfernen
  slice(1:10000) %>%  # slice entfernen
  left_join(., kunden _slice %>% select(kundennummer, cluster), by="kundennummer")

kunden %>% 
  filter(!(is.na(cluster))) %>% 
  group_by(cluster) %>% 
  summarise(wg_1200 = mean(warengruppe=="1200"), 
            wg_1500 = sum(warengruppe=="1500"),
            wg_1100 = sum(warengruppe=="1100"),
            wg_4500 = sum(warengruppe=="4500"),
            wg_1300 = sum(warengruppe=="1300"),
            wg_1400 = sum(warengruppe=="1400"),
            wg_1000 = sum(warengruppe=="1000"),
            wg_4000 = sum(warengruppe=="4000"),
            wg_3000 = sum(warengruppe=="3000"),
            wg_1050 = sum(warengruppe=="1050"),
            wg_5000 = sum(warengruppe=="5000"),
            wg_1350 = sum(warengruppe=="1350"),
            wg_6000 = sum(warengruppe=="6000"),
            wg_1900 = sum(warengruppe=="1900"),
            wg_2000 = sum(warengruppe=="2000"),
            wg_9999 = sum(warengruppe=="9999"),
            wg_7001 = sum(warengruppe=="7001"),
            wg_9000 = sum(warengruppe=="9000"),
            wg_7000 = sum(warengruppe=="7000"),
            wg_7002 = sum(warengruppe=="7002"),
            wg_7100 = sum(warengruppe=="7100"),
            wg_4700 = sum(warengruppe=="4700"),
            wg_1150 = sum(warengruppe=="1150"),
            wg_1025 = sum(warengruppe=="1025"),
            wg_1234 = sum(warengruppe=="1234"),
            wg_1625 = sum(warengruppe=="1625"),
            wg_1600 = sum(warengruppe=="1600"),
            wg_1650 = sum(warengruppe=="1650"),
            wg_4600 = sum(warengruppe=="4600"),
            wg_1700 = sum(warengruppe=="1700"),
            wg_1250 = sum(warengruppe=="1250"),
            wg_1626 = sum(warengruppe=="1626")) %>% 
  t() %>% 
  janitor::row_to_names(row_number = 1) %>% 
  view()

# k means clusteranalyse ###


kcluster <- kunden  %>% 
  select(kundennummer, gesamt_vk_netto, gekaufte_artikel, anzahl_bestellungen, artikel_pro_bestellung, mean_eur_pro_artikel, mean_eur_pro_bestellung) %>% #mean_tage_zwischen_bestellungen, letzte_bestellung_vor,
  column_to_rownames(var="kundennummer") %>% 
  scale() %>% 
  kmeans(9, iter.max=100)

# test elbow #
# load required packages
#library(factoextra)
#library(NbClust)


# Elbow method
#factoextra::fviz_nbclust(kcluster, kmeans, method = "wss") +
#  geom_vline(xintercept = 4, linetype = 2) + # add line for better visualisation
#  labs(subtitle = "Elbow method") # add subtitle


##### kmeans 

kunden  <- kunden  %>% 
  mutate(cluster = kcluster[[1]]) %>% 
  relocate(cluster, .after=kundentyp)

trans$cluster <- NULL


trans <- kunden  %>%
  select(kundennummer, cluster) %>% 
  left_join(trans, ., by="kundennummer") %>% 
  relocate(cluster, .after=kundentyp)




kunden  %>%  
  group_by(cluster) %>%
  summarise(n_absolut = n(),
            n_relative =100/nrow(kunden )*n_absolut, 
            mean_age = mean(aprox_alter, na.rm=TRUE),
            mean_gesamt_vk_netto = mean(gesamt_vk_netto), 
            mean_gekaufte_artikel = mean(gekaufte_artikel),
            mean_anzahl_bestellungen = mean(anzahl_bestellungen),
            mean_artikel_pro_bestellung = mean(artikel_pro_bestellung),
            mean_eur_pro_artikel = mean(mean_eur_pro_artikel),
            mean_eur_pro_bestellung = mean(mean_eur_pro_bestellung),
            mean_tage_kunde = mean(tage_kunde),
            mean_tage_zwischen_bestellungen = mean(mean_tage_zwischen_bestellungen),
            mean_letzte_bestellung_vor = mean(letzte_bestellung_vor)) %>% 
  round(2) %>% 
  t() %>% 
  janitor::row_to_names(row_number = 1)





kunden  %>% 
  group_by(cluster) %>% 
  select(sex) %>%
  table() %>% 
  prop.table(2) %>%
  as.data.frame() %>%
  rename(Cluster = cluster,
         Geschlecht = sex,
         Prozent = Freq) %>%
  ggplot() +
  aes(x=Cluster, y= Prozent, fill = Geschlecht) + 
  geom_bar(stat = "identity", position = "fill") + 
  scale_y_continuous(labels = scales::percent) +
  #geom_text(aes(label=paste0(sprintf("%1.1f", Freq*100), "%")), position = position_stack(vjust = 0.5), colour = "white") +
  #scale_fill_manual(values=randomcoloR::distinctColorPalette(2)) + #farbe festlegen
  #hrbrthemes::theme_ipsum() + #theme festlegen
  labs(title = "Verteilung Geschlecht auf Cluster")




trans %>% 
  group_by(cluster) %>% 
  summarise(n = n(),
            "Bücher Jagd" = 100/n*sum(warengruppe=="1000"),
            "Zeitschriften Jagd" = 100/n*sum(warengruppe=="1025"),
            "Bücher Waffen" = 100/n*sum(warengruppe=="1050"),
            "Videos/CD/Spiele Jagd" = 100/n*sum(warengruppe=="1100"),
            "Hörbücher" = 100/n*sum(warengruppe=="1150"),
            "Kalender" = 100/n*sum(warengruppe=="1200"),
            "Gutscheine" = 100/n*sum(warengruppe=="1234"),
            #"JANA Taler" = 100/n*sum(warengruppe=="1250"),
            "Zubehör Jagd" = 100/n*sum(warengruppe=="1300"),
            "Zubehör Sonstiges" = 100/n*sum(warengruppe=="1350"),
            "Bekleidung" = 100/n*sum(warengruppe=="1400"),
            "Messer" = 100/n*sum(warengruppe=="1500"),
            "Optik" = 100/n*sum(warengruppe=="1600"),
            "Waffen" = 100/n*sum(warengruppe=="1625"),
            "Gebrauchtwaffen" = 100/n*sum(warengruppe=="1626"),
            "Munition" = 100/n*sum(warengruppe=="1650"),
            "Karten, Postkarten allg." = 100/n*sum(warengruppe=="1700"),
            "JANASAN" = 100/n*sum(warengruppe=="1900"),
            "Bücher Angeln" = 100/n*sum(warengruppe=="2000"),
            "Bücher Kochen" = 100/n*sum(warengruppe=="3000"),
            "Bücher Hunde" = 100/n*sum(warengruppe=="4000"),
            "Tierfutter" = 100/n*sum(warengruppe=="4500"),
            "Nahrungsmittel" = 100/n*sum(warengruppe=="4600"),
            "Wein, Spirituosen" = 100/n*sum(warengruppe=="4700"),
            "Bücher Natur" = 100/n*sum(warengruppe=="5000"),
            "Bücher Preußen, Politik" = 100/n*sum(warengruppe=="6000"),
            "Bücher Pferde" = 100/n*sum(warengruppe=="7000"),
            "Bücher Kinder" = 100/n*sum(warengruppe=="7001"),
            "Nonbook Kinder" = 100/n*sum(warengruppe=="7002"),
            "Videos, CD etc Pferd" = 100/n*sum(warengruppe=="7100"),
            "Werbung" = 100/n*sum(warengruppe=="8000"),
            "Anzeigen/Werbebeteiligung" = 100/n*sum(warengruppe=="8000"),
            "Edition-Ratz-Fatz" = 100/n*sum(warengruppe=="9000"),
            "Gebühren ohne Steuer" = 100/n*sum(warengruppe=="9998"), 
            "Gebühren, Auslagen etc" = 100/n*sum(warengruppe=="9999")) %>% 
  t() %>% 
  janitor::row_to_names(row_number = 1) %>% 
  round(2) %>% 
  view()



  # market basket analysis warenkorb analyse 

  # artikeldateanalyse # basket market analysis ###


# prep data

trans %>% 
  filter(!(str_detect(artikelbezeichnung, "^[Jagdzeit international Band]"))) %>% 
  select(bestell_id, artikelbezeichnung) %>%
  arrange(bestell_id) %>% 
  as.data.frame() %>% 
  write.csv(., file = "basket.csv")

basket <- arules::read.transactions("basket.csv", format = "single", sep=",", cols=2:3)
arules::inspect(basket)


arules::apriori(basket)


# Absolute Item Frequency Plot

arules::itemFrequencyPlot(basket, topN=20, horiz=T, type="absolute", col="wheat2",xlab="Item name") 

trans %>% 
  group_by(artikelbezeichnung) %>% 
  count() %>% 
  arrange(desc(n)) %>% 
  slice(1:20) %>% 
  ggplot(aes(x=artikelbezeichnung, y=n))+
  geom_bar(x=artikelbezeichnung, y=n)




# Relative Item Frequency Plot
arules::itemFrequencyPlot(basket, topN=15, type="relative", col="lightcyan2", xlab="Item name", 
                          ylab="Frequency (relative)", main="Relative Item Frequency Plot")


# Visualization - Transactions per month
trans %>%    
  filter(drop_artikel == F & drop_kunden == F & drop_warengruppe == F) %>% 
  mutate(Monat=as.factor(monat)) %>%
  group_by(Monat) %>%
  summarise(Transaktionen=n_distinct(bestell_id)) %>%
  ggplot(aes(x=Monat, y=Transaktionen)) +
  geom_bar(stat="identity", fill="mistyrose2", 
           show.legend=FALSE, colour="black") +
  labs(title="Transaktionen pro Monat") +
  theme_bw() 

# Visualization - Transactions per weekday
trans %>% 
  filter(drop_artikel == F & drop_kunden == F & drop_warengruppe == F) %>% 
  group_by(bestell_id, day) %>%
  summarise(transaktionen=n_distinct(bestell_id)) %>%
  ggplot(aes(x=day, y=transaktionen)) +
  geom_bar(stat="identity", fill="peachpuff2", 
           show.legend=FALSE, colour="black") +
  labs(title="Transaktionen pro Wochentag") +
  scale_x_discrete(limits=c("Monday", "Tuesday", "Wednesday", "Thursday",
                            "Friday", "Saturday", "Sunday")) +
  theme_bw() 


trans %>%
  filter(drop_artikel == F & drop_kunden == F & drop_warengruppe == F) %>% 
  mutate(tag=as.factor(tag)) %>%
  group_by(tag) %>%
  summarise(transaktionen=n_distinct(bestell_id)) %>%
  ggplot(aes(x=tag, y=transaktionen)) +
  geom_bar(stat="identity", fill="mistyrose2", 
           show.legend=FALSE, colour="black") +
  labs(title="Transaktionen im Monat") +
  theme_bw() 


# rules

rules <- arules::apriori(basket, parameter =list(sup=0.00025))
arules::inspect(rules)

# Inspect association rules
arules::inspect(rules_sup1_conf50)

library(arulesViz)

# Scatter plot
plot(rules, measure=c("support", "lift"), shading="confidence")

# Graph (default layout)
plot(rules, method="graph")

# Graph (circular layout)
plot(rules, method="graph", control=list(layout=igraph::in_circle()))




# plots 


# zeitreihendecomposition ###



trans %>%
  group_by(jahr, lubridate::month(verkaufsdatum)) %>% 
  summarise(umsatz = sum(vk_netto),
            date = lubridate::floor_date(min(verkaufsdatum), "week")) %>%
  ungroup() %>% 
  timetk::plot_stl_diagnostics(
    date, umsatz,
    .feature_set = c("observed", "season", "trend", "remainder"),
    .frequency   = "auto",
    .trend       = "auto",
    .message     = FALSE,
    .interactive = F)



# sales history ##
trans %>%
  #filter(!is.na(sex)) %>% 
  group_by(jahr, lubridate::month(verkaufsdatum)) %>% #aprox_geschlecht
  summarise(umsatz = sum(vk_netto),
            date = lubridate::floor_date(min(verkaufsdatum), "week")) %>% 
  ggplot() +
  geom_line(aes(x = date, y = umsatz, group = aprox_geschlecht, color = aprox_geschlecht)) + #group = sex, color = sex
  labs(x = "Jahr",
       y = "Gesamt-Netto (€)",
       title = "Sales History",
       color = "Geschlecht") +
  theme(axis.text.x = element_text(angle = 45),plot.title = element_text(hjust = 0.5)) +
  scale_y_continuous(labels = scales::format_format(big.mark = ".", decimal.mark = ",", scientific = FALSE)) +
  scale_color_brewer(palette = "Paired") + theme(axis.text = element_text(size = 9),
                                                 axis.text.x = element_text(size = 7,
                                                                            vjust = 0, angle = 90)) 


# bar-chart ##





trans %>% 
  group_by(artikelbezeichnung) %>% 
  count() %>% 
  rename(Anzahl = n,
         Artikelbezeichnung = artikelbezeichnung) %>% 
  mutate(Artikelbezeichnung = as.character(Artikelbezeichnung)) %>% 
  arrange(desc(Anzahl)) %>% 
  as.data.frame() %>% 
  slice(1:10) %>% 
  ggplot(aes(y=reorder(Artikelbezeichnung, -Anzahl), x=Anzahl)) +
  geom_bar(stat = "identity", fill="#08306b") +
  theme(axis.text.x = element_text(angle = 90))+labs(x = "Artikelbezeichnung")



trans %>% 
  group_by(warengruppe) %>% 
  count() %>% 
  rename(Anzahl = n,
         Warengruppe = warengruppe) %>% 
  mutate(Warengruppe = as.character(Warengruppe)) %>% 
  arrange(desc(Anzahl)) %>% 
  as.data.frame() %>% 
  slice(1:10) %>% 
  ggplot(aes(y= reorder(Warengruppe, -Anzahl), x=Anzahl))+
  geom_bar(stat = "identity", fill="#08306b") + theme(axis.text.x = element_text(angle = 90))+labs(y = "Warengruppe")



kunden  %>% 
  group_by(cluster) %>% 
  select(altersgruppe) %>%
  filter(!is.na(altersgruppe)) %>% 
  table() %>% 
  prop.table(2) %>%
  as_tibble() %>%
  rename(Cluster = cluster,
         Altersgruppe = altersgruppe,
         Prozent = n) %>%
  ggplot() +
  aes(x=Cluster, y=Prozent, fill = Altersgruppe) + 
  geom_bar(stat = "identity", position = "fill") + 
  scale_y_continuous(labels = scales::percent) +
  #geom_text(aes(label=paste0(sprintf("%1.1f", Freq*100), "%")), position = position_stack(vjust = 0.5), colour = "white") +
  scale_fill_manual(values=(c("#ccb9bc","#cb9ca1","#ba6b6c", "#af4448", "#b61827", "#8B0000"))) + #farbe festlegen
  #hrbrthemes::theme_ipsum() + #theme festlegen
  labs(title = "Verteilung Altersgruppen auf Cluster")





# pyramid plot ###

kunden  %>%
  select(aprox_alter, aprox_geschlecht) %>% 
  table() %>% 
  as.data.frame.matrix() %>% 
  rownames_to_column("Alter") %>% 
  mutate(Frau = as.numeric(Frau),
         Mann = as.numeric(Mann),
         Alter = as.numeric(Alter)) %>% 
  mutate(Alter = ifelse(Alter >=15 & Alter <= 22 , "15-22", 
                        ifelse(Alter >= 23 & Alter <= 27, "23-27",
                               ifelse(Alter >= 28 & Alter <= 32, "28-32",
                                      ifelse(Alter >= 33 & Alter <= 37, "33-37",
                                             ifelse(Alter >= 38 & Alter <= 42, "38-42",
                                                    ifelse(Alter >= 43 & Alter <= 47, "43-47",
                                                           ifelse(Alter >= 48 & Alter <= 52, "48-52",
                                                                  ifelse(Alter >= 53 & Alter <= 57, "53-57",
                                                                         ifelse(Alter >= 58 & Alter <= 62, "58-62",
                                                                                ifelse(Alter >= 63 & Alter <= 67, "63-67",
                                                                                       ifelse(Alter >= 68 & Alter <= 72, "68-72",
                                                                                              ifelse(Alter >= 73 & Alter <= 77, "73-77",
                                                                                                     ifelse(Alter >= 78 & Alter <= 82, "78-82", "83 und älter")))))))))))))) %>% 
  group_by(Alter) %>% 
  mutate(Frau = sum(Frau),
         Mann = sum(Mann)) %>% 
  ungroup() %>% 
  distinct(Alter, .keep_all = TRUE) %>% 
  pivot_longer(names_to = 'Geschlecht', values_to = 'Population', cols = 2:3) %>% 
  mutate(PopPerc=case_when(Geschlecht=='Mann'~round(Population/sum(Population)*100,2),
                           TRUE~-round(Population/sum(Population)*100,2)),
         signal=case_when(Geschlecht=='Mann'~1,
                          TRUE~-1)) %>% 
  ggplot() +
  geom_bar(aes(x=Alter,y=PopPerc,fill=Geschlecht),stat='identity')+
  geom_text(aes(x=Alter,y=PopPerc+signal*.3,label=abs(PopPerc)))+
  coord_flip() +
  scale_fill_manual(name='',values=c('darkred','steelblue'))+
  scale_y_continuous(breaks=seq(-10,10,1),
                     labels=function(x){paste(abs(x),'%')})+
  labs(x='',y='Kunden (%)',
       title='Altersverteilung nach Geschlecht') +
  cowplot::theme_cowplot()+
  theme(axis.text.x=element_text(vjust=.5),
        panel.grid.major.y = element_line(color='lightgray',linetype='dashed'),
        legend.position = 'top',
        legend.justification = 'center')


# maps ###

agg_map <- kunden  %>% 
  filter(land=="D") %>%
  left_join(., sf::st_read("/Users/alexchaichan/R/Postleitzahlengebiete_-_OSM/OSM_PLZ.shp") %>% select(plz), by="plz") %>% 
  group_by(plz_gebiet) %>% 
  summarise(anzahl_bestellungen = sum(anzahl_bestellungen, na.rm=T),
            anzahl_kunden = n(),
            gesamt_vk_netto = sum(gesamt_vk_netto, na.rm=T),
            ratio = round(gesamt_vk_netto/anzahl_kunden,2)) %>% 
  slice(-11) %>% 
  nngeo::st_remove_holes()
#save(agg_map, file = "agg_map.RData")

agg_map %>% 
  ggplot() +
  geom_sf(aes(fill=gesamt_vk_netto))


# bestellugnen männer frauen ##
kunden %>%
  group_by(kundentyp) %>%
  summarise(n_invoices = n(),
            invoice_amount = sum(gesamt_vk_netto, na.rm = TRUE)) %>%
  ggplot() +
  geom_bar(aes(x = kundentyp, y = invoice_amount), stat = "identity", fill = "#08306b") +
  scale_y_continuous(labels = scales::format_format(big.mark = ".", decimal.mark = ",", scientific = FALSE)) +
  labs(x = "Kundentyp", 
       y = "Summe Bestellungen (€)",
       title = "Gesamt-Netto nach Kundentyp") +
  theme(plot.title = element_text(hjust = 0.5))


# wann wird gekauft ###
# Visualization - Transactions per month
trans %>%
  mutate(Monat=as.factor(monat)) %>%
  group_by(Monat) %>%
  summarise(Transaktionen=n_distinct(bestell_id)) %>%
  ggplot(aes(x=Monat, y=Transaktionen)) +
  geom_bar(stat="identity", fill="#08306b", 
           show.legend=FALSE) +
  labs(title="Transaktionen pro Monat") +
  theme_bw() 

trans %>%
  filter(drop_artikel == F & drop_kunden == F & drop_warengruppe == F) %>% 
  mutate(Tag=ifelse(day=="Monday", "Montag",
                    ifelse(day=="Tuesday", "Dienstag",
                           ifelse(day=="Wednesday", "Mittwoch",
                                  ifelse(day=="Thursday", "Donnerstag",
                                         ifelse(day=="Friday", "Freitag",
                                                ifelse(day=="Saturday", "Samstag",
                                                       ifelse(day=="Sunday", "Sonntag",0)))))))) %>% 
  group_by(Tag) %>%
  summarise(Transaktionen=n_distinct(bestell_id)) %>%
  ggplot(aes(x=Tag, y=Transaktionen)) +
  geom_bar(stat="identity", fill="#08306b", 
           show.legend=FALSE) +
  labs(title="Transaktionen pro Wochentag") +
  scale_x_discrete(limits=c("Montag", "Dienstag", "Mittwoch", "Donnerstag",
                            "Freitag", "Samstag", "Sonntag")) +
  theme_bw() 



kunden  %>% 
  group_by(cluster) %>% 
  select(aprox_geschlecht) %>%
  table() %>% 
  prop.table(2) %>%
  as.data.frame() %>%
  rename(Cluster = cluster,
         Geschlecht = aprox_geschlecht,
         Prozent = Freq) %>%
  ggplot() +
  aes(x=Cluster, y=Prozent, fill = Geschlecht) + 
  geom_bar(stat = "identity", position = "fill") + 
  scale_y_continuous(labels = scales::percent) +
  #geom_text(aes(label=paste0(sprintf("%1.1f", Freq*100), "%")), position = position_stack(vjust = 0.5), colour = "white") +
  #scale_fill_manual(values=randomcoloR::distinctColorPalette(2)) + #farbe festlegen
  #hrbrthemes::theme_ipsum() + #theme festlegen
  labs(title = "Verteilung Geschlecht auf Cluster")

























# ebay training #######

# data sim ###

library(tidyverse)
library(forecast)

categories <- c("Electronics", "Home&Garden", "Automotive", "Toys")

countries <- c("DE", "US", "UK", "FR")

old_loc <- Sys.getlocale("LC_TIME")
Sys.setlocale(category = "LC_TIME", locale = "English")

revenue_df <- expand.grid(
  Category = categories, 
  Country = countries, 
  Year = c("2020", "2021"),
  Month = month.name
) %>% 
  as_tibble() %>% 
  mutate(
    GMV = round(100000 * runif(n(), 1, 50)),
    Quantity = round(
      GMV / case_when(
        Category == "Electronics" ~ 200,
        Category == "Home&Garden" ~ 150,
        Category == "Automotive" ~ 1000,
        Category == "Toys" ~ 80
      )
    ),
    Sellers = round(1000 * runif(n(), 1, 10)),
    date = as.Date(strptime(str_c("01-", Month, "-", Year), format = "%d-%B-%Y"))
  ) %>% 
  filter(Year == "2020" | Month %in% month.name[1:10])

Sys.setlocale(category = "LC_TIME", locale = old_loc)

revenue_df %>% 
  write_csv("revenue.csv")


old_loc <- Sys.getlocale("LC_TIME")
Sys.setlocale(category = "LC_TIME", locale = "English")

gmv_df <- read_csv(here::here("revenue.csv")) %>% 
  mutate(date = as.Date(strptime(str_c("01-", Month, "-", Year), format = "%d-%B-%Y")))

Sys.setlocale(category = "LC_TIME", locale = old_loc)

yoy_data <- read_csv("revenue.csv") %>% 
  mutate(date = as.Date(strptime(str_c("01-", Month, "-", Year), format = "%d-%B-%Y"))) %>% 
  filter(Category == "Toys", Country == "DE") %>% 
  select(Year, GMV, date) %>% 
  mutate(date = if_else(Year == "2020", date %m+% years(1), date)) %>% 
  pivot_wider(id_cols = date, names_from = Year, values_from = GMV, names_prefix = "GMV_") %>% 
  filter(between(date, max(date) %m-% months(7), max(date))) %>% 
  drop_na() %>% 
  mutate(YoY = GMV_2021 / GMV_2020 - 1)

max_y <- yoy_data %>% select(-YoY) %>% pivot_longer(-date, names_to = "Figure") %>% pull(value) %>% max()
min_y <- yoy_data %>% select(-YoY) %>% pivot_longer(-date, names_to = "Figure") %>% pull(value) %>% min()

min_yoy <- yoy_data %>% pull(YoY) %>% min()
max_yoy <- yoy_data %>% pull(YoY) %>% max()

plot_data <- yoy_data %>% 
  mutate(YoY = (YoY - min(YoY)) * (max_y - min_y) / diff(range(YoY)) + min_y) %>% 
  pivot_longer(-date, names_to = "Figure") 

ggplot(plot_data %>% filter(Figure != "YoY")) + 
  geom_bar(data = plot_data %>% 
             filter(Figure == "YoY") %>% 
             rename(YoY = value), 
           aes(x = date, y = YoY),
           stat = "identity", alpha = 0.6) +
  geom_point(aes(x = date, y = value, color = as.factor(Figure))) +
  geom_line(aes(x = date, y = value, color = as.factor(Figure))) +
  scale_y_continuous(
    "GMV", 
    sec.axis = sec_axis(~ . - min_y * (max_yoy - min_yoy) / (max_y - min_y), name = "YoY")
  )

highchart() %>% 
  hc_add_series(data = gmv_df %>% group_by(date) %>% summarize(GMV = sum(GMV)), hcaes(x = date, y = GMV), type = "spline")

highchart() %>% 
  hc_add_series(data = gmv_df %>% filter(Year == 2021, Month == "October"),
                hcaes(x = Category, y = GMV, group = Country), type = "column")


hchart(
  gmv_df %>% filter(Year == 2021, Month == "October"), 
  "column", 
  hcaes(x = Category, y = GMV, group = Country),
  stacking = "normal"
)


plot_data <- gmv_df %>% 
  filter(Category == "Automotive", Country == "DE") %>% 
  select(Year, GMV, date) %>% 
  mutate(date = if_else(Year == "2020", date %m+% years(1), date)) %>% 
  pivot_wider(id_cols = date, names_from = Year, values_from = GMV, names_prefix = "GMV_") %>% 
  #filter(between(date, max(date) %m-% months(7), max(date))) %>% 
  #drop_na() %>% 
  mutate(YoY = GMV_2021 / GMV_2020 - 1)#,
         #across(-YoY, replace_na, replace = 0))

highchart() %>% 
  hc_yAxis_multiples(
    list(lineWidth = 3, title = list(text = "$")),
    list(showLastLabel = FALSE, opposite = TRUE, title = list(text = "%"))
  ) %>% 
  hc_add_series(name = "GMV - Current Year", data = plot_data, hcaes(date, GMV_2021), type = "spline", zIndex = 5) %>% 
  hc_add_series(name = "GMV - Previous Year", data = plot_data, hcaes(date, GMV_2020), type = "spline", zIndex = 5) %>% 
  hc_add_series(name = "YoY", data = plot_data, hcaes(date, YoY), type = "column", yAxis = 1, color = "lightgray",
                zIndex = 1) %>% 
  hc_xAxis(labels = list(format = '{value:%b %d}'))


hchart(arima_data, type = "spline", hcaes(x = date, y = GMV, group = type))


# casestudy ###

# https://www.tidymodels.org/start/case-study/
  
# import libraries
library(tidyverse)
library(tidymodels)

# import data 
data("mlc_churn")



#### explorative data analysis ###

# view data
glimpse(mlc_churn)


# count churns
mlc_churn %>% 
  count(churn) %>% 
  mutate(prop = n/sum(n))

# visualization churns
ggplot(data=mlc_churn, aes(x=churn)) +
  geom_histogram(stat =  "count", fill=c("darkred", "steelblue")) +
  labs(title = "Mobile Contract Churn", c="Churn", y="Frequency")


#### spliting data & resampling ###
splits <- initial_split(mlc_churn, strata = churn)

churn_training <- training(splits)
churn_test  <- testing(splits)


#### training set proportions by churns ###
churn_training %>% 
  count(churn) %>% 
  mutate(prop = n/sum(n))


# test set proportions by churns
churn_test  %>% 
  count(churn) %>% 
  mutate(prop = n/sum(n))


# split training into training & validation
val_set <- validation_split(churn_training, strata = churn, prop = 0.80)


#### 1st model ###


# logistic regression model
lr_mod <- 
  logistic_reg(penalty = tune(), mixture = 1) %>%
  set_engine("glmnet")

# create recipe
lr_recipe <- 
  recipe(churn ~ ., data = churn_training) %>% 
  step_dummy(all_nominal(), -all_outcomes()) %>% 
  step_zv(all_predictors()) %>% 
  step_normalize(all_predictors())


# create workflow
lr_workflow <- 
  workflow() %>% 
  add_model(lr_mod) %>% 
  add_recipe(lr_recipe)

# tune model 
lr_reg_grid <- tibble(penalty = 10^seq(-4, -1, length.out = 30))

lr_reg_grid %>% top_n(-5) # lowest penalty values
lr_reg_grid %>% top_n(5)  # highest penalty values

# train model
lr_res <- 
  lr_workflow %>% 
  tune_grid(val_set,
            grid = lr_reg_grid,
            control = control_grid(save_pred = TRUE),
            metrics = metric_set(roc_auc))

# roc curve
lr_res %>% 
  collect_metrics() %>% 
  ggplot(aes(x = penalty, y = mean)) + 
  geom_point() + 
  geom_line() + 
  ylab("Area under the ROC Curve") +
  scale_x_log10(labels = scales::label_number())


# best model
lr_res %>% 
  show_best("roc_auc", n = 15) %>% 
  arrange(penalty)
  
 
lr_best <- 
  lr_res %>% 
  collect_metrics() %>% 
  arrange(penalty) %>% 
  slice(24)
lr_best

lr_auc <- 
  lr_res %>% 
  collect_predictions(parameters = lr_best) %>% 
  roc_curve(churn, .pred_yes) %>% 
  mutate(model = "Logistic Regression")

autoplot(lr_auc)

#### random forest model ###

rf_mod <- 
  rand_forest(mtry = tune(), min_n = tune(), trees = 1000) %>% 
  set_engine("ranger", num.threads = 4) %>% 
  set_mode("classification")

#reciept & workflow
rf_recipe <- 
  recipe(churn ~ ., data = churn_training)

rf_workflow <- 
  workflow() %>% 
  add_model(rf_mod) %>% 
  add_recipe(rf_recipe)

rf_res <- 
  rf_workflow %>% 
  tune_grid(val_set,
            grid = 25,
            control = control_grid(save_pred = TRUE),
            metrics = metric_set(roc_auc))


rf_res %>% 
  show_best(metric = "roc_auc")

autoplot(rf_res)


rf_best <- 
  rf_res %>% 
  select_best(metric = "roc_auc")
rf_best

rf_res %>% 
  collect_predictions()


rf_auc <- 
  rf_res %>% 
  collect_predictions(parameters = rf_best) %>% 
  roc_curve(churn, .pred_yes) %>% 
  mutate(model = "Random Forest")


bind_rows(rf_auc, lr_auc) %>% 
  ggplot(aes(x = 1 - specificity, y = sensitivity, col = model)) + 
  geom_path(lwd = 1.5, alpha = 0.8) +
  geom_abline(lty = 3) + 
  coord_equal() + 
  scale_color_viridis_d(option = "plasma", end = .6)

#### last model ###

# the last model
last_rf_mod <- 
  rand_forest(mtry = 8, min_n = 7, trees = 1000) %>% 
  set_engine("ranger", num.threads = 4, importance = "impurity") %>% 
  set_mode("classification")

# the last workflow
last_rf_workflow <- 
  rf_workflow %>% 
  update_model(last_rf_mod)

# the last fit
last_rf_fit <- 
  last_rf_workflow %>% 
  last_fit(splits)


last_rf_fit %>% 
  collect_metrics()


last_rf_fit %>% 
  pluck(".workflow", 1) %>%   
  extract_fit_parsnip() %>% 
  vip::vip(num_features = 20)


last_rf_fit %>% 
  collect_predictions() %>% 
  roc_curve(churn, .pred_yes) %>% 
  autoplot()



# business report pdf ##

---
title: "Business Report 10/2021"
author: "Florian Schmoll"
date: "`r format(Sys.Date(), '%d %b %Y')`"
output: 
  pdf_document:
    latex_engine: pdflatex
header-includes:
  - \usepackage[scaled]{helvet}
  - \renewcommand\familydefault{\sfdefault} 
  - \usepackage{floatrow} 
  - \floatsetup[figure]{capposition=top}
  - \floatsetup[table]{capposition=top}
  - \usepackage{booktabs}
  - \usepackage{longtable}
  - \usepackage{float}
  - \floatplacement{figure}{H}
  - \floatplacement{table}{H}
lang: en
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(tidyverse)
library(knitr)
library(lubridate)
library(tidyquant)
library(timetk)
library(sweep)
library(forecast)


gmv_df <- read_csv(here::here("revenue.csv"))

```

This will be a table showing the numbers for the current month in *Germany*:

```{r echo=FALSE, message=FALSE, warning=FALSE}
gmv_df %>% 
  select(-date) %>% 
  filter(Country == "DE", Year == 2021, Month == "October") %>% 
  knitr::kable(caption="\\label{tab:LABEL}GMV Germany current Month/Year", longtable = T) %>% 
  kableExtra::kable_styling(latex_options = "striped", position = "center")
```

This will be a table showing the numbers for the previous year:

```{r echo=FALSE, message=FALSE, warning=FALSE}
gmv_df %>% 
  select(-date) %>% 
  filter(Country == "DE" & Year == "2020" & Month == "October") %>% 
  knitr::kable(caption="\\label{tab:LABEL1}GMV Germany previous Month/Year") %>% 
  kableExtra::kable_styling(latex_options = "striped", position = "center")
```


## Revenue per Category/Country

```{r echo=FALSE, message=FALSE, warning=FALSE, fig.align="center"}
gmv_df %>% 
  filter(Year == "2021", Month == "October") %>% 
  ggplot(aes(x=Category, y=GMV, fill = Country)) + 
  geom_bar(stat = "identity") +
  scale_y_continuous(labels=scales::dollar_format(suffix="$", prefix="")) +
  labs(title= "GMV") 
```



## Forecast


```{r echo=FALSE, message=FALSE, warning=FALSE, fig.align="center"}


hist_gmv <- arima.sim(n = 46, list(ar = 0.8, ma = 1))

arima_data <- hist_gmv %>% 
  as_tibble() %>% 
  rename(GMV = x) %>% 
  mutate(date = seq.Date(from = as.Date("2018-01-01"), to = as.Date("2021-10-01"), by = "months"),
         type = "historical",
         GMV = (GMV + 2 * (min(GMV) < 0) * abs(min(GMV))) * 10000) %>% 
  bind_rows(
    .,
    tibble(
      GMV = as.numeric(forecast(auto.arima(.$GMV), h = 6)$mean),
      date = seq.Date(from = as.Date("2021-11-01"), to = as.Date("2022-04-01"), by = "months"),
      type = "forecast"
    )
  ) 

arima_data %>% 
  filter(date >= "2019-01-01") %>% 
  ggplot(aes(x = date, y = GMV)) +
  geom_line(aes(color = type)) +
  geom_point(aes(color = type)) +
  theme(legend.title = element_blank()) +
  ggtitle("6-months GMV Forecast - Electronics") +
  xlab("Date")
```


# business report html ###

---
title: "Business Report 10/2021"
author: "Florian Schmoll"
date: "`r format(Sys.Date(), '%d %b %Y')`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(tidyverse)
library(knitr)
library(lubridate)
library(tidyquant)
library(timetk)
library(sweep)
library(forecast)
library(highcharter)


gmv_df <- read_csv(here::here("revenue.csv")) 

```

## Business figures

# Current Year

```{r echo=FALSE, message=FALSE, warning=FALSE}
gmv_df %>% 
  select(-date) %>% 
  filter(Country == "DE", Year == 2021) %>% 
  arrange(Year) %>% 
  DT::datatable()
```


# Previous Year

```{r echo=FALSE, message=FALSE, warning=FALSE}
gmv_df %>% 
  select(-date) %>% 
  filter(Country == "DE", Year == 2020) %>% 
    DT::datatable()
```


## Revenue per Category - Current Year/Month

```{r echo=FALSE, message=FALSE, warning=FALSE, fig.align="center"}
hchart(
  gmv_df %>% filter(Year == 2021, Month == "October"), 
  "column", 
  hcaes(x = Category, y = GMV, group = Country),
  stacking = "normal"
)
```

## Revenue per Category - Previous Year/Month

```{r echo=FALSE, message=FALSE, warning=FALSE, fig.align="center"}
hchart(
  gmv_df %>% filter(Year == 2020, Month == "October"), 
  "column", 
  hcaes(x = Category, y = GMV, group = Country),
  stacking = "normal"
)
```


## Year-over-Year


```{r echo=FALSE, message=FALSE, warning=FALSE, fig.align="center"}
plot_data <- gmv_df %>% 
  filter(Category == "Automotive", Country == "DE") %>% 
  select(Year, GMV, date) %>% 
  mutate(date = if_else(Year == "2020", date %m+% years(1), date)) %>% 
  pivot_wider(id_cols = date, names_from = Year, values_from = GMV, names_prefix = "GMV_") %>% 
  filter(between(date, max(date) %m-% months(7), max(date))) %>% 
  mutate(YoY = GMV_2021 / GMV_2020 - 1)

highchart() %>% 
  hc_yAxis_multiples(
    list(lineWidth = 3, title = list(text = "$")),
    list(showLastLabel = FALSE, opposite = TRUE, title = list(text = "%"))
  ) %>% 
  hc_add_series(name = "GMV - Current Year", data = plot_data, hcaes(date, GMV_2021), type = "spline", zIndex = 5) %>% 
  hc_add_series(name = "GMV - Previous Year", data = plot_data, hcaes(date, GMV_2020), type = "spline", zIndex = 5) %>% 
  hc_add_series(name = "YoY", data = plot_data, hcaes(date, YoY), type = "column", yAxis = 1, color = "lightgray",
                zIndex = 1) %>% 
  hc_xAxis(labels = list(format = '{value:%b %d}'))

```

## Revenue Forecast

```{r echo=FALSE, message=FALSE, warning=FALSE}

hist_gmv <- arima.sim(n = 46, list(ar = 0.8, ma = 1))

arima_data <- hist_gmv %>% 
  as_tibble() %>% 
  rename(GMV = x) %>% 
  mutate(date = seq.Date(from = as.Date("2018-01-01"), to = as.Date("2021-10-01"), by = "months"),
         type = "historical",
         GMV = (GMV + 2 * (min(GMV) < 0) * abs(min(GMV))) * 10000) %>% 
  bind_rows(
    .,
    tibble(
      GMV = as.numeric(forecast(auto.arima(.$GMV), h = 6)$mean),
      date = seq.Date(from = as.Date("2021-11-01"), to = as.Date("2022-04-01"), by = "months"),
      type = "forecast"
    )
  ) 

arima_plot <- arima_data %>% 
  filter(date >= "2019-01-01") %>% 
  ggplot(aes(x = date, y = GMV)) +
  geom_line(aes(color = type)) +
  geom_point(aes(color = type)) +
  theme(legend.title = element_blank()) +
  ggtitle("6-months GMV Forecast - Electronics") +
  xlab("Date")


plotly::ggplotly(arima_plot)

```


